HloModule SyncTensorsGraph.10, is_scheduled=true, entry_computation_layout={(f32[], f32[3]{0}, f32[3]{0})->(f32[3]{0})}, frontend_attributes={fingerprint_before_lhs="555ea9fcca75ac81e5aeec9d3d5c02ff"}

%fused_divide (param_0: f32[3], param_1.1: f32[]) -> f32[3] {
  %param_0 = f32[3]{0} parameter(0)
  %param_1.1 = f32[] parameter(1)
  %broadcast.2.1 = f32[3]{0} broadcast(f32[] %param_1.1), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul.3/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %divide.2.1 = f32[3]{0} divide(f32[3]{0} %param_0, f32[3]{0} %broadcast.2.1), metadata={op_type="aten__div" op_name="aten__div" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_multiply (param_0.1: f32[3], param_1.3: f32[]) -> f32[3] {
  %param_0.1 = f32[3]{0} parameter(0)
  %param_1.3 = f32[] parameter(1)
  %broadcast.2.2 = f32[3]{0} broadcast(f32[] %param_1.3), dimensions={}, metadata={op_type="aten__mul" op_name="aten__mul.3/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %multiply.2.1 = f32[3]{0} multiply(f32[3]{0} %param_0.1, f32[3]{0} %broadcast.2.2), metadata={op_type="aten__mul" op_name="aten__mul.3/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

ENTRY %SyncTensorsGraph.10 (p0.1.0: f32[], p1.2.0: f32[3], p2.3.0: f32[3]) -> (f32[3]) {
  %p0.1.0 = f32[] parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %p2.3.0 = f32[3]{0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=728}
  %p1.2.0 = f32[3]{0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=728}
  %loop_multiply_fusion = f32[3]{0} fusion(f32[3]{0} %p2.3.0, f32[] %p0.1.0), kind=kLoop, calls=%fused_multiply, metadata={op_type="aten__mul" op_name="aten__mul.3/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %custom-call.6.0 = f32[3]{0} custom-call(f32[3]{0} %loop_multiply_fusion, f32[3]{0} %p1.2.0), custom_call_target="XlaGpuSimpleAdd", metadata={op_type="xla__custom_call" op_name="xla__custom_call" source_file="/code/github_code/xla_vllm/vllm/playground/custom_call/naive_ctypes/simple_add/test_torch_compile_custom_call.py" source_line=154}
  %loop_divide_fusion = f32[3]{0} fusion(f32[3]{0} %custom-call.6.0, f32[] %p0.1.0), kind=kLoop, calls=%fused_divide, metadata={op_type="aten__div" op_name="aten__div" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %tuple.9.0 = (f32[3]{0}) tuple(f32[3]{0} %loop_divide_fusion)
}

