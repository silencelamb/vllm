HloModule SyncTensorsGraph.9, is_scheduled=true, entry_computation_layout={()->(f32[2,128,4096]{2,1,0})}, replica_count=8, frontend_attributes={fingerprint_before_lhs="56edd664717b8832b42a7c788a127639"}

%fused_broadcast () -> f32[2,128,4096] {
  %constant_2_1 = f32[] constant(0), metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %broadcast.2.1 = f32[2,128,4096]{2,1,0} broadcast(f32[] %constant_2_1), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

ENTRY %SyncTensorsGraph.9 () -> (f32[2,128,4096]) {
  %loop_broadcast_fusion = f32[2,128,4096]{2,1,0} fusion(), kind=kLoop, calls=%fused_broadcast, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %tuple.8.0 = (f32[2,128,4096]{2,1,0}) tuple(f32[2,128,4096]{2,1,0} %loop_broadcast_fusion)
}

