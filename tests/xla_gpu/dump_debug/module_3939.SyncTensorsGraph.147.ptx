//
// Generated by LLVM NVPTX Back-End
//

.version 8.5
.target sm_80
.address_size 64

	// .globl	loop_gather_fusion
.extern .shared .align 16 .b8 global_smem[];

.visible .entry loop_gather_fusion(
	.param .u64 .ptr .align 16 loop_gather_fusion_param_0,
	.param .u64 .ptr .align 16 loop_gather_fusion_param_1,
	.param .u64 .ptr .align 128 loop_gather_fusion_param_2
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd1, [loop_gather_fusion_param_0];
	ld.param.u64 	%rd2, [loop_gather_fusion_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [loop_gather_fusion_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shr.u32 	%r3, %r2, 3;
	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd5, %rd7;
	ld.global.nc.u32 	%r4, [%rd8];
	min.u32 	%r5, %r4, 151935;
	shl.b32 	%r6, %r2, 7;
	and.b32  	%r7, %r6, 896;
	shl.b32 	%r8, %r5, 10;
	or.b32  	%r9, %r8, %r7;
	or.b32  	%r10, %r9, %r1;
	mul.wide.u32 	%rd9, %r10, 2;
	add.s64 	%rd10, %rd6, %rd9;
	ld.global.nc.u16 	%rs1, [%rd10];
	or.b32  	%r11, %r6, %r1;
	mul.wide.u32 	%rd11, %r11, 2;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.b16 	[%rd12], %rs1;
	ret;

}
	// .globl	fusion_9
.visible .entry fusion_9(
	.param .u64 .ptr .align 16 fusion_9_param_0,
	.param .u64 .ptr .align 128 fusion_9_param_1,
	.param .u64 .ptr .align 16 fusion_9_param_2,
	.param .u64 .ptr .align 128 fusion_9_param_3
)
.reqntid 64, 1, 1
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<49>;
	.reg .b32 	%r<101>;
	.reg .f32 	%f<114>;
	.reg .b64 	%rd<25>;

	ld.param.u64 	%rd8, [fusion_9_param_0];
	ld.param.u64 	%rd9, [fusion_9_param_3];
	cvta.to.global.u64 	%rd10, %rd9;
	ld.param.u64 	%rd11, [fusion_9_param_1];
	ld.param.u64 	%rd12, [fusion_9_param_2];
	cvta.to.global.u64 	%rd13, %rd12;
	cvta.to.global.u64 	%rd14, %rd11;
	cvta.to.global.u64 	%rd3, %rd8;
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	mul.wide.s32 	%rd15, %r1, 1024;
	shl.b64 	%rd16, %rd15, 1;
	add.s64 	%rd17, %rd14, %rd16;
	mov.u32 	%r81, %tid.x;
	and.b32  	%r82, %r81, 31;
	shl.b32 	%r83, %r81, 3;
	and.b32  	%r84, %r83, 248;
	setp.lt.u32 	%p4, %r81, 32;
	selp.b32 	%r85, 0, 256, %p4;
	or.b32  	%r86, %r84, %r85;
	mul.wide.u32 	%rd18, %r86, 2;
	add.s64 	%rd1, %rd17, %rd18;
	add.s64 	%rd2, %rd1, 1024;
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	mov.u32 %r5, 0x0;
	ld.global.v4.b32 { %r2, %r3, %r4, %r5 }, [ %rd1 + 0 ];
	// end inline asm
	mov.b32 	{%rs1, %rs2}, %r2;
	mov.b32 	{%rs3, %rs4}, %r3;
	mov.b32 	{%rs5, %rs6}, %r4;
	mov.b32 	{%rs7, %rs8}, %r5;
	// begin inline asm
	mov.u32 %r6, 0x0;
	mov.u32 %r7, 0x0;
	mov.u32 %r8, 0x0;
	mov.u32 %r9, 0x0;
	ld.global.v4.b32 { %r6, %r7, %r8, %r9 }, [ %rd2 + 0 ];
	// end inline asm
	mov.b32 	{%rs9, %rs10}, %r6;
	mov.b32 	{%rs11, %rs12}, %r7;
	mov.b32 	{%rs13, %rs14}, %r8;
	mov.b32 	{%rs15, %rs16}, %r9;
	// begin inline asm
	cvt.f32.bf16 %r10, %rs1;
	// end inline asm
	mov.b32 	%f1, %r10;
	// begin inline asm
	cvt.f32.bf16 %r11, %rs2;
	// end inline asm
	mov.b32 	%f2, %r11;
	// begin inline asm
	cvt.f32.bf16 %r12, %rs3;
	// end inline asm
	mov.b32 	%f3, %r12;
	// begin inline asm
	cvt.f32.bf16 %r13, %rs4;
	// end inline asm
	mov.b32 	%f4, %r13;
	// begin inline asm
	cvt.f32.bf16 %r14, %rs5;
	// end inline asm
	mov.b32 	%f5, %r14;
	// begin inline asm
	cvt.f32.bf16 %r15, %rs6;
	// end inline asm
	mov.b32 	%f6, %r15;
	// begin inline asm
	cvt.f32.bf16 %r16, %rs7;
	// end inline asm
	mov.b32 	%f7, %r16;
	// begin inline asm
	cvt.f32.bf16 %r17, %rs8;
	// end inline asm
	mov.b32 	%f8, %r17;
	// begin inline asm
	cvt.f32.bf16 %r18, %rs9;
	// end inline asm
	mov.b32 	%f9, %r18;
	// begin inline asm
	cvt.f32.bf16 %r19, %rs10;
	// end inline asm
	mov.b32 	%f10, %r19;
	// begin inline asm
	cvt.f32.bf16 %r20, %rs11;
	// end inline asm
	mov.b32 	%f11, %r20;
	// begin inline asm
	cvt.f32.bf16 %r21, %rs12;
	// end inline asm
	mov.b32 	%f12, %r21;
	// begin inline asm
	cvt.f32.bf16 %r22, %rs13;
	// end inline asm
	mov.b32 	%f13, %r22;
	// begin inline asm
	cvt.f32.bf16 %r23, %rs14;
	// end inline asm
	mov.b32 	%f14, %r23;
	// begin inline asm
	cvt.f32.bf16 %r24, %rs15;
	// end inline asm
	mov.b32 	%f15, %r24;
	// begin inline asm
	cvt.f32.bf16 %r25, %rs16;
	// end inline asm
	mov.b32 	%f16, %r25;
	mul.rn.f32 	%f17, %f1, %f1;
	mul.rn.f32 	%f18, %f2, %f2;
	mul.rn.f32 	%f19, %f3, %f3;
	mul.rn.f32 	%f20, %f4, %f4;
	mul.rn.f32 	%f21, %f5, %f5;
	mul.rn.f32 	%f22, %f6, %f6;
	mul.rn.f32 	%f23, %f7, %f7;
	mul.rn.f32 	%f24, %f8, %f8;
	mul.rn.f32 	%f25, %f9, %f9;
	mul.rn.f32 	%f26, %f10, %f10;
	mul.rn.f32 	%f27, %f11, %f11;
	mul.rn.f32 	%f28, %f12, %f12;
	mul.rn.f32 	%f29, %f13, %f13;
	mul.rn.f32 	%f30, %f14, %f14;
	mul.rn.f32 	%f31, %f15, %f15;
	mul.rn.f32 	%f32, %f16, %f16;
	add.rn.f32 	%f33, %f17, %f18;
	add.rn.f32 	%f34, %f33, %f19;
	add.rn.f32 	%f35, %f34, %f20;
	add.rn.f32 	%f36, %f35, %f21;
	add.rn.f32 	%f37, %f36, %f22;
	add.rn.f32 	%f38, %f37, %f23;
	add.rn.f32 	%f39, %f38, %f24;
	add.rn.f32 	%f40, %f39, %f25;
	add.rn.f32 	%f41, %f40, %f26;
	add.rn.f32 	%f42, %f41, %f27;
	add.rn.f32 	%f43, %f42, %f28;
	add.rn.f32 	%f44, %f43, %f29;
	add.rn.f32 	%f45, %f44, %f30;
	add.rn.f32 	%f46, %f45, %f31;
	add.rn.f32 	%f47, %f46, %f32;
	mov.b32 	%r87, %f47;
	shfl.sync.bfly.b32	%r88, %r87, 16, 31, -1;
	mov.b32 	%f48, %r88;
	add.rn.f32 	%f49, %f47, %f48;
	mov.b32 	%r89, %f49;
	shfl.sync.bfly.b32	%r90, %r89, 8, 31, -1;
	mov.b32 	%f50, %r90;
	add.rn.f32 	%f51, %f49, %f50;
	mov.b32 	%r91, %f51;
	shfl.sync.bfly.b32	%r92, %r91, 4, 31, -1;
	mov.b32 	%f52, %r92;
	add.rn.f32 	%f53, %f51, %f52;
	mov.b32 	%r93, %f53;
	shfl.sync.bfly.b32	%r94, %r93, 2, 31, -1;
	mov.b32 	%f54, %r94;
	add.rn.f32 	%f55, %f53, %f54;
	mov.b32 	%r95, %f55;
	shfl.sync.bfly.b32	%r96, %r95, 1, 31, -1;
	mov.b32 	%f56, %r96;
	add.rn.f32 	%f57, %f55, %f56;
	setp.eq.s32 	%p1, %r82, 0;
	shr.u32 	%r97, %r81, 3;
	and.b32  	%r98, %r97, 4;
	cvt.u64.u32 	%rd19, %r98;
	mov.u64 	%rd20, global_smem;
	add.s64 	%rd21, %rd20, %rd19;
	mov.b32 	%r27, %f57;
	cvt.u32.u64 	%r26, %rd21;
	// begin inline asm
	@%p1 st.shared.b32 [ %r26 + 0 ], %r27;
	// end inline asm
	bar.sync 	0;
	setp.lt.u32 	%p2, %r81, 2;
	shl.b32 	%r99, %r81, 2;
	cvt.u64.u32 	%rd22, %r99;
	add.s64 	%rd23, %rd20, %rd22;
	cvt.u32.u64 	%r29, %rd23;
	// begin inline asm
	@%p2 ld.shared.b32 %r28, [ %r29 + 0 ];
	// end inline asm
	mov.b32 	%f58, %r28;
	shfl.sync.bfly.b32	%r100, %r28, 1, 31, -1;
	mov.b32 	%f59, %r100;
	add.rn.f32 	%f60, %f58, %f59;
	setp.eq.s32 	%p3, %r81, 0;
	mov.b32 	%r31, %f60;
	// begin inline asm
	@%p3 st.shared.b32 [ %r29 + 0 ], %r31;
	// end inline asm
	bar.sync 	0;
	ld.shared.f32 	%f61, [global_smem];
	mul.rn.f32 	%f62, %f61, 0f3A800000;
	// begin inline asm
	mov.u32 %r32, 0x0;
	ld.global.b32 { %r32 }, [ %rd3 + 0 ];
	// end inline asm
	mov.b32 	%f63, %r32;
	add.rn.f32 	%f64, %f62, %f63;
	rsqrt.approx.f32 	%f65, %f64;
	mul.rn.f32 	%f66, %f1, %f65;
	mul.rn.f32 	%f67, %f2, %f65;
	mul.rn.f32 	%f68, %f3, %f65;
	mul.rn.f32 	%f69, %f4, %f65;
	mul.rn.f32 	%f70, %f5, %f65;
	mul.rn.f32 	%f71, %f6, %f65;
	mul.rn.f32 	%f72, %f7, %f65;
	mul.rn.f32 	%f73, %f8, %f65;
	mul.rn.f32 	%f74, %f9, %f65;
	mul.rn.f32 	%f75, %f10, %f65;
	mul.rn.f32 	%f76, %f11, %f65;
	mul.rn.f32 	%f77, %f12, %f65;
	mul.rn.f32 	%f78, %f13, %f65;
	mul.rn.f32 	%f79, %f14, %f65;
	mul.rn.f32 	%f80, %f15, %f65;
	mul.rn.f32 	%f81, %f16, %f65;
	add.s64 	%rd4, %rd13, %rd18;
	add.s64 	%rd5, %rd4, 1024;
	// begin inline asm
	mov.u32 %r33, 0x0;
	mov.u32 %r34, 0x0;
	mov.u32 %r35, 0x0;
	mov.u32 %r36, 0x0;
	ld.global.v4.b32 { %r33, %r34, %r35, %r36 }, [ %rd4 + 0 ];
	// end inline asm
	mov.b32 	{%rs17, %rs18}, %r33;
	mov.b32 	{%rs19, %rs20}, %r34;
	mov.b32 	{%rs21, %rs22}, %r35;
	mov.b32 	{%rs23, %rs24}, %r36;
	// begin inline asm
	mov.u32 %r37, 0x0;
	mov.u32 %r38, 0x0;
	mov.u32 %r39, 0x0;
	mov.u32 %r40, 0x0;
	ld.global.v4.b32 { %r37, %r38, %r39, %r40 }, [ %rd5 + 0 ];
	// end inline asm
	mov.b32 	{%rs25, %rs26}, %r37;
	mov.b32 	{%rs27, %rs28}, %r38;
	mov.b32 	{%rs29, %rs30}, %r39;
	mov.b32 	{%rs31, %rs32}, %r40;
	// begin inline asm
	cvt.f32.bf16 %r41, %rs17;
	// end inline asm
	mov.b32 	%f82, %r41;
	// begin inline asm
	cvt.f32.bf16 %r42, %rs18;
	// end inline asm
	mov.b32 	%f83, %r42;
	// begin inline asm
	cvt.f32.bf16 %r43, %rs19;
	// end inline asm
	mov.b32 	%f84, %r43;
	// begin inline asm
	cvt.f32.bf16 %r44, %rs20;
	// end inline asm
	mov.b32 	%f85, %r44;
	// begin inline asm
	cvt.f32.bf16 %r45, %rs21;
	// end inline asm
	mov.b32 	%f86, %r45;
	// begin inline asm
	cvt.f32.bf16 %r46, %rs22;
	// end inline asm
	mov.b32 	%f87, %r46;
	// begin inline asm
	cvt.f32.bf16 %r47, %rs23;
	// end inline asm
	mov.b32 	%f88, %r47;
	// begin inline asm
	cvt.f32.bf16 %r48, %rs24;
	// end inline asm
	mov.b32 	%f89, %r48;
	// begin inline asm
	cvt.f32.bf16 %r49, %rs25;
	// end inline asm
	mov.b32 	%f90, %r49;
	// begin inline asm
	cvt.f32.bf16 %r50, %rs26;
	// end inline asm
	mov.b32 	%f91, %r50;
	// begin inline asm
	cvt.f32.bf16 %r51, %rs27;
	// end inline asm
	mov.b32 	%f92, %r51;
	// begin inline asm
	cvt.f32.bf16 %r52, %rs28;
	// end inline asm
	mov.b32 	%f93, %r52;
	// begin inline asm
	cvt.f32.bf16 %r53, %rs29;
	// end inline asm
	mov.b32 	%f94, %r53;
	// begin inline asm
	cvt.f32.bf16 %r54, %rs30;
	// end inline asm
	mov.b32 	%f95, %r54;
	// begin inline asm
	cvt.f32.bf16 %r55, %rs31;
	// end inline asm
	mov.b32 	%f96, %r55;
	// begin inline asm
	cvt.f32.bf16 %r56, %rs32;
	// end inline asm
	mov.b32 	%f97, %r56;
	mul.rn.f32 	%f98, %f66, %f82;
	mul.rn.f32 	%f99, %f67, %f83;
	mul.rn.f32 	%f100, %f68, %f84;
	mul.rn.f32 	%f101, %f69, %f85;
	mul.rn.f32 	%f102, %f70, %f86;
	mul.rn.f32 	%f103, %f71, %f87;
	mul.rn.f32 	%f104, %f72, %f88;
	mul.rn.f32 	%f105, %f73, %f89;
	mul.rn.f32 	%f106, %f74, %f90;
	mul.rn.f32 	%f107, %f75, %f91;
	mul.rn.f32 	%f108, %f76, %f92;
	mul.rn.f32 	%f109, %f77, %f93;
	mul.rn.f32 	%f110, %f78, %f94;
	mul.rn.f32 	%f111, %f79, %f95;
	mul.rn.f32 	%f112, %f80, %f96;
	mul.rn.f32 	%f113, %f81, %f97;
	mov.b32 	%r57, %f98;
	// begin inline asm
	cvt.rn.bf16.f32 %rs33, %r57;
	// end inline asm
	mov.b32 	%r58, %f99;
	// begin inline asm
	cvt.rn.bf16.f32 %rs34, %r58;
	// end inline asm
	mov.b32 	%r59, %f100;
	// begin inline asm
	cvt.rn.bf16.f32 %rs35, %r59;
	// end inline asm
	mov.b32 	%r60, %f101;
	// begin inline asm
	cvt.rn.bf16.f32 %rs36, %r60;
	// end inline asm
	mov.b32 	%r61, %f102;
	// begin inline asm
	cvt.rn.bf16.f32 %rs37, %r61;
	// end inline asm
	mov.b32 	%r62, %f103;
	// begin inline asm
	cvt.rn.bf16.f32 %rs38, %r62;
	// end inline asm
	mov.b32 	%r63, %f104;
	// begin inline asm
	cvt.rn.bf16.f32 %rs39, %r63;
	// end inline asm
	mov.b32 	%r64, %f105;
	// begin inline asm
	cvt.rn.bf16.f32 %rs40, %r64;
	// end inline asm
	mov.b32 	%r65, %f106;
	// begin inline asm
	cvt.rn.bf16.f32 %rs41, %r65;
	// end inline asm
	mov.b32 	%r66, %f107;
	// begin inline asm
	cvt.rn.bf16.f32 %rs42, %r66;
	// end inline asm
	mov.b32 	%r67, %f108;
	// begin inline asm
	cvt.rn.bf16.f32 %rs43, %r67;
	// end inline asm
	mov.b32 	%r68, %f109;
	// begin inline asm
	cvt.rn.bf16.f32 %rs44, %r68;
	// end inline asm
	mov.b32 	%r69, %f110;
	// begin inline asm
	cvt.rn.bf16.f32 %rs45, %r69;
	// end inline asm
	mov.b32 	%r70, %f111;
	// begin inline asm
	cvt.rn.bf16.f32 %rs46, %r70;
	// end inline asm
	mov.b32 	%r71, %f112;
	// begin inline asm
	cvt.rn.bf16.f32 %rs47, %r71;
	// end inline asm
	mov.b32 	%r72, %f113;
	// begin inline asm
	cvt.rn.bf16.f32 %rs48, %r72;
	// end inline asm
	add.s64 	%rd24, %rd10, %rd16;
	add.s64 	%rd6, %rd24, %rd18;
	add.s64 	%rd7, %rd6, 1024;
	mov.b32 	%r73, {%rs33, %rs34};
	mov.b32 	%r74, {%rs35, %rs36};
	mov.b32 	%r75, {%rs37, %rs38};
	mov.b32 	%r76, {%rs39, %rs40};
	// begin inline asm
	st.global.v4.b32 [ %rd6 + 0 ], { %r73, %r74, %r75, %r76 };
	// end inline asm
	mov.b32 	%r77, {%rs41, %rs42};
	mov.b32 	%r78, {%rs43, %rs44};
	mov.b32 	%r79, {%rs45, %rs46};
	mov.b32 	%r80, {%rs47, %rs48};
	// begin inline asm
	st.global.v4.b32 [ %rd7 + 0 ], { %r77, %r78, %r79, %r80 };
	// end inline asm
	ret;

}
	// .globl	wrapped_slice
.visible .entry wrapped_slice(
	.param .u64 .ptr .align 128 wrapped_slice_param_0,
	.param .u64 .ptr .align 128 wrapped_slice_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd1, [wrapped_slice_param_0];
	ld.param.u64 	%rd2, [wrapped_slice_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r2, 7;
	and.b32  	%r4, %r3, 896;
	shl.b32 	%r5, %r2, 9;
	and.b32  	%r6, %r5, 520192;
	or.b32  	%r7, %r6, %r4;
	or.b32  	%r8, %r7, %r1;
	mul.wide.u32 	%rd5, %r8, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6+6144];
	or.b32  	%r9, %r3, %r1;
	mul.wide.u32 	%rd7, %r9, 2;
	add.s64 	%rd8, %rd3, %rd7;
	st.global.b16 	[%rd8], %rs1;
	ret;

}
	// .globl	triton_softmax_2_0
.visible .entry triton_softmax_2_0(
	.param .u64 .ptr .align 16 triton_softmax_2_0_param_0,
	.param .u64 .ptr .align 128 triton_softmax_2_0_param_1,
	.param .u64 .ptr .align 128 triton_softmax_2_0_param_2
)
.reqntid 64, 1, 1
{
	.reg .pred 	%p<4>;
	.reg .b16 	%rs<9>;
	.reg .b32 	%r<59>;
	.reg .f32 	%f<44>;
	.reg .b64 	%rd<36>;

	ld.param.u64 	%rd5, [triton_softmax_2_0_param_0];
	ld.param.u64 	%rd6, [triton_softmax_2_0_param_2];
	cvta.to.global.u64 	%rd7, %rd6;
	ld.param.u64 	%rd8, [triton_softmax_2_0_param_1];
	cvta.to.global.u64 	%rd9, %rd8;
	cvta.to.global.u64 	%rd2, %rd5;
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	shl.b32 	%r33, %r1, 9;
	cvt.u64.u32 	%rd10, %r33;
	and.b64  	%rd11, %rd10, 512;
	mul.wide.s32 	%rd12, %r1, 2048;
	or.b64  	%rd13, %rd12, %rd11;
	shl.b64 	%rd14, %rd13, 1;
	or.b64  	%rd15, %rd14, 4096;
	add.s64 	%rd16, %rd9, %rd15;
	mov.u32 	%r34, %tid.x;
	shl.b32 	%r35, %r34, 3;
	and.b32  	%r36, %r35, 248;
	setp.gt.u32 	%p3, %r34, 31;
	selp.b32 	%r37, 256, 0, %p3;
	or.b32  	%r38, %r36, %r37;
	mul.wide.u32 	%rd17, %r38, 2;
	add.s64 	%rd1, %rd16, %rd17;
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	mov.u32 %r5, 0x0;
	ld.global.v4.b32 { %r2, %r3, %r4, %r5 }, [ %rd1 + 0 ];
	// end inline asm
	mov.b32 	{%rs1, %rs2}, %r2;
	mov.b32 	{%rs3, %rs4}, %r3;
	mov.b32 	{%rs5, %rs6}, %r4;
	mov.b32 	{%rs7, %rs8}, %r5;
	// begin inline asm
	cvt.f32.bf16 %r6, %rs1;
	// end inline asm
	mov.b32 	%f1, %r6;
	// begin inline asm
	cvt.f32.bf16 %r7, %rs2;
	// end inline asm
	mov.b32 	%f2, %r7;
	// begin inline asm
	cvt.f32.bf16 %r8, %rs3;
	// end inline asm
	mov.b32 	%f3, %r8;
	// begin inline asm
	cvt.f32.bf16 %r9, %rs4;
	// end inline asm
	mov.b32 	%f4, %r9;
	// begin inline asm
	cvt.f32.bf16 %r10, %rs5;
	// end inline asm
	mov.b32 	%f5, %r10;
	// begin inline asm
	cvt.f32.bf16 %r11, %rs6;
	// end inline asm
	mov.b32 	%f6, %r11;
	// begin inline asm
	cvt.f32.bf16 %r12, %rs7;
	// end inline asm
	mov.b32 	%f7, %r12;
	// begin inline asm
	cvt.f32.bf16 %r13, %rs8;
	// end inline asm
	mov.b32 	%f8, %r13;
	mul.rn.f32 	%f9, %f1, %f1;
	mul.rn.f32 	%f10, %f2, %f2;
	mul.rn.f32 	%f11, %f3, %f3;
	mul.rn.f32 	%f12, %f4, %f4;
	mul.rn.f32 	%f13, %f5, %f5;
	mul.rn.f32 	%f14, %f6, %f6;
	mul.rn.f32 	%f15, %f7, %f7;
	mul.rn.f32 	%f16, %f8, %f8;
	add.rn.f32 	%f17, %f9, %f10;
	add.rn.f32 	%f18, %f17, %f11;
	add.rn.f32 	%f19, %f18, %f12;
	add.rn.f32 	%f20, %f19, %f13;
	add.rn.f32 	%f21, %f20, %f14;
	add.rn.f32 	%f22, %f21, %f15;
	add.rn.f32 	%f23, %f22, %f16;
	mov.b32 	%r39, %f23;
	shfl.sync.bfly.b32	%r40, %r39, 8, 31, -1;
	mov.b32 	%f24, %r40;
	add.rn.f32 	%f25, %f23, %f24;
	mov.b32 	%r41, %f25;
	shfl.sync.bfly.b32	%r42, %r41, 4, 31, -1;
	mov.b32 	%f26, %r42;
	add.rn.f32 	%f27, %f25, %f26;
	mov.b32 	%r43, %f27;
	shfl.sync.bfly.b32	%r44, %r43, 2, 31, -1;
	mov.b32 	%f28, %r44;
	add.rn.f32 	%f29, %f27, %f28;
	mov.b32 	%r45, %f29;
	shfl.sync.bfly.b32	%r46, %r45, 1, 31, -1;
	mov.b32 	%f30, %r46;
	add.rn.f32 	%f31, %f29, %f30;
	mul.rn.f32 	%f32, %f31, 0f3C000000;
	// begin inline asm
	mov.u32 %r14, 0x0;
	ld.global.b32 { %r14 }, [ %rd2 + 0 ];
	// end inline asm
	mov.b32 	%f33, %r14;
	add.rn.f32 	%f34, %f32, %f33;
	rsqrt.approx.f32 	%f35, %f34;
	mul.rn.f32 	%f36, %f1, %f35;
	mul.rn.f32 	%f37, %f2, %f35;
	mul.rn.f32 	%f38, %f3, %f35;
	mul.rn.f32 	%f39, %f4, %f35;
	mul.rn.f32 	%f40, %f5, %f35;
	mul.rn.f32 	%f41, %f6, %f35;
	mul.rn.f32 	%f42, %f7, %f35;
	mul.rn.f32 	%f43, %f8, %f35;
	add.s64 	%rd18, %rd7, %rd12;
	selp.b64 	%rd19, 128, 0, %p3;
	selp.b64 	%rd20, 384, 256, %p3;
	shl.b32 	%r47, %r34, 2;
	and.b32  	%r48, %r47, 124;
	cvt.u64.u32 	%rd21, %r48;
	or.b64  	%rd22, %rd19, %rd21;
	or.b64  	%rd23, %rd20, %rd21;
	shl.b64 	%rd24, %rd22, 2;
	add.s64 	%rd3, %rd18, %rd24;
	shl.b64 	%rd25, %rd23, 2;
	add.s64 	%rd4, %rd18, %rd25;
	selp.b32 	%r49, 128, 0, %p3;
	or.b32  	%r50, %r48, %r49;
	shr.u32 	%r51, %r38, 4;
	and.b32  	%r52, %r51, 24;
	add.s32 	%r53, %r52, %r38;
	shl.b32 	%r54, %r53, 2;
	cvt.u64.u32 	%rd26, %r54;
	mov.u64 	%rd27, global_smem;
	add.s64 	%rd28, %rd27, %rd26;
	mov.b32 	%r16, %f36;
	mov.b32 	%r17, %f37;
	mov.b32 	%r18, %f38;
	mov.b32 	%r19, %f39;
	mov.b32 	%r21, %f40;
	mov.b32 	%r22, %f41;
	mov.b32 	%r23, %f42;
	mov.b32 	%r24, %f43;
	cvt.u32.u64 	%r15, %rd28;
	mov.pred 	%p1, -1;
	// begin inline asm
	@%p1 st.shared.v4.b32 [ %r15 + 0 ], { %r16, %r17, %r18, %r19 };
	// end inline asm
	add.s32 	%r20, %r15, 16;
	// begin inline asm
	@%p1 st.shared.v4.b32 [ %r20 + 0 ], { %r21, %r22, %r23, %r24 };
	// end inline asm
	bar.sync 	0;
	shr.u32 	%r55, %r49, 4;
	add.s32 	%r56, %r55, %r50;
	mul.wide.u32 	%rd29, %r56, 4;
	add.s64 	%rd30, %rd27, %rd29;
	or.b32  	%r57, %r49, 256;
	shr.u32 	%r58, %r57, 4;
	cvt.u64.u32 	%rd31, %r58;
	cvt.u64.u32 	%rd32, %r50;
	add.s64 	%rd33, %rd31, %rd32;
	shl.b64 	%rd34, %rd33, 2;
	add.s64 	%rd35, %rd27, %rd34;
	ld.shared.v4.u32 	{%r29, %r30, %r31, %r32}, [%rd35+1024];
	ld.shared.v4.u32 	{%r25, %r26, %r27, %r28}, [%rd30];
	// begin inline asm
	st.global.v4.b32 [ %rd3 + 0 ], { %r25, %r26, %r27, %r28 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd4 + 0 ], { %r29, %r30, %r31, %r32 };
	// end inline asm
	ret;

}
	// .globl	input_concatenate_fusion
.visible .entry input_concatenate_fusion(
	.param .u64 .ptr .align 128 input_concatenate_fusion_param_0,
	.param .u64 .ptr .align 16 input_concatenate_fusion_param_1,
	.param .u64 .ptr .align 16 input_concatenate_fusion_param_2,
	.param .u64 .ptr .align 16 input_concatenate_fusion_param_3,
	.param .u64 .ptr .align 128 input_concatenate_fusion_param_4
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<17>;
	.reg .f32 	%f<15>;
	.reg .b64 	%rd<25>;

	ld.param.u64 	%rd1, [input_concatenate_fusion_param_0];
	ld.param.u64 	%rd2, [input_concatenate_fusion_param_4];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [input_concatenate_fusion_param_1];
	ld.param.u64 	%rd5, [input_concatenate_fusion_param_3];
	cvta.to.global.u64 	%rd6, %rd5;
	ld.param.u64 	%rd7, [input_concatenate_fusion_param_2];
	cvta.to.global.u64 	%rd8, %rd7;
	cvta.to.global.u64 	%rd9, %rd4;
	cvta.to.global.u64 	%rd10, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	and.b32  	%r3, %r1, 63;
	shl.b32 	%r4, %r1, 1;
	and.b32  	%r5, %r4, 128;
	shl.b32 	%r6, %r2, 8;
	or.b32  	%r7, %r5, %r6;
	or.b32  	%r8, %r7, %r3;
	mul.wide.u32 	%rd11, %r8, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.f32 	%f1, [%rd12];
	mul.wide.u32 	%rd13, %r3, 2;
	add.s64 	%rd14, %rd9, %rd13;
	ld.global.nc.u16 	%rs1, [%rd14];
	cvt.f32.bf16 	%f2, %rs1;
	mul.rn.f32 	%f3, %f1, %f2;
	and.b32  	%r9, %r2, -4;
	cvt.u64.u32 	%rd15, %r9;
	add.s64 	%rd16, %rd6, %rd15;
	ld.global.nc.u32 	%r10, [%rd16];
	min.s32 	%r11, %r10, 40959;
	max.s32 	%r12, %r11, 0;
	shl.b32 	%r13, %r12, 7;
	or.b32  	%r14, %r13, %r3;
	mul.wide.u32 	%rd17, %r14, 2;
	add.s64 	%rd18, %rd8, %rd17;
	ld.global.nc.u16 	%rs2, [%rd18];
	cvt.f32.bf16 	%f4, %rs2;
	mul.rn.f32 	%f5, %f3, %f4;
	or.b32  	%r15, %r1, 64;
	ld.global.nc.f32 	%f6, [%rd12+256];
	mul.wide.u32 	%rd19, %r15, 2;
	add.s64 	%rd20, %rd9, %rd19;
	ld.global.nc.u16 	%rs3, [%rd20];
	cvt.f32.bf16 	%f7, %rs3;
	mul.rn.f32 	%f8, %f6, %f7;
	or.b32  	%r16, %r13, %r15;
	mul.wide.u32 	%rd21, %r16, 2;
	add.s64 	%rd22, %rd8, %rd21;
	ld.global.nc.u16 	%rs4, [%rd22];
	cvt.f32.bf16 	%f9, %rs4;
	mul.rn.f32 	%f10, %f8, %f9;
	sub.rn.f32 	%f11, %f5, %f10;
	cvt.rn.bf16.f32 	%rs5, %f11;
	mul.wide.u32 	%rd23, %r8, 2;
	add.s64 	%rd24, %rd3, %rd23;
	st.global.b16 	[%rd24], %rs5;
	mul.rn.f32 	%f12, %f8, %f4;
	mul.rn.f32 	%f13, %f3, %f9;
	add.rn.f32 	%f14, %f12, %f13;
	cvt.rn.bf16.f32 	%rs6, %f14;
	st.global.b16 	[%rd24+128], %rs6;
	ret;

}
	// .globl	loop_slice_fusion
.visible .entry loop_slice_fusion(
	.param .u64 .ptr .align 16 loop_slice_fusion_param_0,
	.param .u64 .ptr .align 128 loop_slice_fusion_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	ld.param.u64 	%rd1, [loop_slice_fusion_param_0];
	ld.param.u64 	%rd2, [loop_slice_fusion_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r1, 2;
	shl.b32 	%r4, %r2, 9;
	or.b32  	%r5, %r3, %r4;
	mul.wide.u32 	%rd5, %r5, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd6+138706944];
	add.s64 	%rd7, %rd3, %rd5;
	st.global.v4.b16 	[%rd7], {%rs1, %rs2, %rs3, %rs4};
	ret;

}
	// .globl	wrapped_slice_1
.visible .entry wrapped_slice_1(
	.param .u64 .ptr .align 16 wrapped_slice_1_param_0,
	.param .u64 .ptr .align 128 wrapped_slice_1_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	ld.param.u64 	%rd1, [wrapped_slice_1_param_0];
	ld.param.u64 	%rd2, [wrapped_slice_1_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r1, 2;
	shl.b32 	%r4, %r2, 9;
	or.b32  	%r5, %r3, %r4;
	mul.wide.u32 	%rd5, %r5, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd6];
	add.s64 	%rd7, %rd3, %rd5;
	st.global.v4.b16 	[%rd7], {%rs1, %rs2, %rs3, %rs4};
	ret;

}
