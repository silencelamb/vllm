BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <1688 p5.68.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 116, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1804 p116.2133.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <1454 gemm_fusion_dot.108.0 @0> (size=3538944,offset=0): bf16[64,27648]{1,0}
 value: <1680 custom-call.109.0{0} @0> (size=524288,offset=0): bf16[64,4096]{1,0}
 value: <1685 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <1453 wrapped_concatenate @0> (size=113246208,offset=0): bf16[27648,2048]{1,0}
 value: <1459 custom-call.55.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1463 custom-call.56.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1467 custom-call.57.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1471 custom-call.58.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1475 custom-call.59.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1479 custom-call.60.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1483 custom-call.61.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1487 custom-call.62.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1492 custom-call.63.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1496 custom-call.64.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1500 custom-call.65.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1504 custom-call.66.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1508 custom-call.67.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1512 custom-call.68.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1516 custom-call.69.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1520 custom-call.70.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1525 custom-call.71.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1529 custom-call.72.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1533 custom-call.73.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1537 custom-call.74.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1541 custom-call.75.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1545 custom-call.76.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1549 custom-call.77.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1553 custom-call.78.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1558 custom-call.79.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1562 custom-call.80.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1566 custom-call.81.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1570 custom-call.82.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1574 custom-call.83.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1578 custom-call.84.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1582 custom-call.85.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1586 custom-call.86.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1591 custom-call.87.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1595 custom-call.88.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1599 custom-call.89.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1603 custom-call.90.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1607 custom-call.91.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1611 custom-call.92.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1615 custom-call.93.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1619 custom-call.94.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1624 custom-call.95.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1628 custom-call.96.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1632 custom-call.97.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1636 custom-call.98.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1640 custom-call.99.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1644 custom-call.100.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1648 custom-call.101.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1652 custom-call.102.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1657 custom-call.103.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1661 custom-call.104.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1665 custom-call.105.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1669 custom-call.106.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1673 custom-call.107.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1677 custom-call.108.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1681 custom-call.109.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1682 triton_softmax.29.0 @0> (size=262144,offset=0): f32[64,8,128]{2,1,0}
 value: <1686 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1719 p8.90.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1722 p12.162.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1725 p16.234.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 12582912, parameter 20, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1728 p20.306.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 8: size 12582912, parameter 24, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1731 p24.378.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 9: size 12582912, parameter 28, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1734 p28.450.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 10: size 12582912, parameter 32, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1737 p32.522.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 11: size 12582912, parameter 36, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1740 p36.594.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 12: size 12582912, parameter 40, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1743 p40.666.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 13: size 12582912, parameter 44, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1746 p44.738.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 14: size 12582912, parameter 48, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1749 p48.810.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 15: size 12582912, parameter 52, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1752 p52.882.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 16: size 12582912, parameter 56, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1755 p56.954.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 17: size 12582912, parameter 60, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1758 p60.1026.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 18: size 12582912, parameter 64, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1761 p64.1098.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 19: size 12582912, parameter 68, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1764 p68.1170.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 20: size 12582912, parameter 72, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1767 p72.1242.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 21: size 12582912, parameter 76, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1770 p76.1314.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 22: size 12582912, parameter 80, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1773 p80.1386.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 23: size 12582912, parameter 84, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1776 p84.1458.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 24: size 12582912, parameter 88, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1779 p88.1530.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 25: size 12582912, parameter 92, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1782 p92.1602.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 26: size 12582912, parameter 96, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1785 p96.1674.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 27: size 12582912, parameter 100, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1788 p100.1746.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 28: size 12582912, parameter 104, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1791 p104.1818.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 29: size 12582912, parameter 108, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1794 p108.1890.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 30: size 12582912, parameter 112, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1797 p112.1962.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 31: size 10485760, parameter 115, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <1802 p115.2089.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 32: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <1800 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 33: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1720 p7.88.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 34: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1723 p11.160.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 35: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1726 p15.232.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 36: size 6291456, parameter 19, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1729 p19.304.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 37: size 6291456, parameter 23, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1732 p23.376.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 38: size 6291456, parameter 27, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1735 p27.448.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 39: size 6291456, parameter 31, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1738 p31.520.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 40: size 6291456, parameter 35, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1741 p35.592.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 41: size 6291456, parameter 39, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1744 p39.664.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 42: size 6291456, parameter 43, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1747 p43.736.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 43: size 6291456, parameter 47, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1750 p47.808.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 44: size 6291456, parameter 51, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1753 p51.880.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 45: size 6291456, parameter 55, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1756 p55.952.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 46: size 6291456, parameter 59, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1759 p59.1024.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 47: size 6291456, parameter 63, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1762 p63.1096.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 48: size 6291456, parameter 67, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1765 p67.1168.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 49: size 6291456, parameter 71, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1768 p71.1240.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 50: size 6291456, parameter 75, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1771 p75.1312.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 51: size 6291456, parameter 79, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1774 p79.1384.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 52: size 6291456, parameter 83, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1777 p83.1456.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 53: size 6291456, parameter 87, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1780 p87.1528.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 54: size 6291456, parameter 91, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1783 p91.1600.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 55: size 6291456, parameter 95, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1786 p95.1672.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 56: size 6291456, parameter 99, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1789 p99.1744.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 57: size 6291456, parameter 103, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1792 p103.1816.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 58: size 6291456, parameter 107, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1795 p107.1888.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 59: size 6291456, parameter 111, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1798 p111.1960.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 60: size 4194304, parameter 110, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1690 p110.1944.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 61: size 4194304, parameter 106, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1691 p106.1872.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 62: size 4194304, parameter 102, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1692 p102.1800.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 63: size 4194304, parameter 98, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1693 p98.1728.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 64: size 4194304, parameter 94, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1694 p94.1656.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 65: size 4194304, parameter 90, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1695 p90.1584.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 66: size 4194304, parameter 86, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1696 p86.1512.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 67: size 4194304, parameter 82, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1697 p82.1440.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 68: size 4194304, parameter 78, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1698 p78.1368.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 69: size 4194304, parameter 74, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1699 p74.1296.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 70: size 4194304, parameter 70, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1700 p70.1224.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 71: size 4194304, parameter 66, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1701 p66.1152.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 72: size 4194304, parameter 62, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1702 p62.1080.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 73: size 4194304, parameter 58, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1703 p58.1008.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 74: size 4194304, parameter 54, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1704 p54.936.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 75: size 4194304, parameter 50, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1705 p50.864.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 76: size 4194304, parameter 46, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1706 p46.792.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 77: size 4194304, parameter 42, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1707 p42.720.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 78: size 4194304, parameter 38, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1708 p38.648.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 79: size 4194304, parameter 34, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1709 p34.576.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 80: size 4194304, parameter 30, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1710 p30.504.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 81: size 4194304, parameter 26, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1711 p26.432.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 82: size 4194304, parameter 22, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1712 p22.360.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 83: size 4194304, parameter 18, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1713 p18.288.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 84: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1714 p14.216.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 85: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1715 p10.144.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 86: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1716 p6.72.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 87: size 131072, maybe-live-out:
 value: <1456 fusion.339 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1462 custom-call.56.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1497 fusion.334 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1503 custom-call.66.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1530 fusion.330 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1536 custom-call.74.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1563 fusion.326 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1569 custom-call.82.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1596 fusion.322 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1602 custom-call.90.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1629 fusion.318 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1635 custom-call.98.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1662 fusion.314 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1668 custom-call.106.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1683 input_concatenate_fusion @0> (size=131072,offset=0): bf16[64,8,128]{2,1,0}
allocation 88: size 131072, maybe-live-out:
 value: <1455 loop_gather_fusion @0> (size=131072,offset=0): bf16[64,1,1024]{2,0,1}
 value: <1489 fusion.335 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1495 custom-call.64.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1522 fusion.331 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1528 custom-call.72.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1555 fusion.327 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1561 custom-call.80.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1588 fusion.323 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1594 custom-call.88.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1621 fusion.319 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1627 custom-call.96.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1654 fusion.315 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1660 custom-call.104.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1684 wrapped_slice @0> (size=131072,offset=0): bf16[64,1024]{1,0}
allocation 89: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <1718 p9.92.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 90: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <1721 p13.164.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 91: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <1724 p17.236.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 92: size 2048, parameter 21, shape |bf16[1024]| at ShapeIndex {}:
 value: <1727 p21.308.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 93: size 2048, parameter 25, shape |bf16[1024]| at ShapeIndex {}:
 value: <1730 p25.380.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 94: size 2048, parameter 29, shape |bf16[1024]| at ShapeIndex {}:
 value: <1733 p29.452.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 95: size 2048, parameter 33, shape |bf16[1024]| at ShapeIndex {}:
 value: <1736 p33.524.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 96: size 2048, parameter 37, shape |bf16[1024]| at ShapeIndex {}:
 value: <1739 p37.596.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 97: size 2048, parameter 41, shape |bf16[1024]| at ShapeIndex {}:
 value: <1742 p41.668.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 98: size 2048, parameter 45, shape |bf16[1024]| at ShapeIndex {}:
 value: <1745 p45.740.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 99: size 2048, parameter 49, shape |bf16[1024]| at ShapeIndex {}:
 value: <1748 p49.812.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 100: size 2048, parameter 53, shape |bf16[1024]| at ShapeIndex {}:
 value: <1751 p53.884.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 101: size 2048, parameter 57, shape |bf16[1024]| at ShapeIndex {}:
 value: <1754 p57.956.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 102: size 2048, parameter 61, shape |bf16[1024]| at ShapeIndex {}:
 value: <1757 p61.1028.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 103: size 2048, parameter 65, shape |bf16[1024]| at ShapeIndex {}:
 value: <1760 p65.1100.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 104: size 2048, parameter 69, shape |bf16[1024]| at ShapeIndex {}:
 value: <1763 p69.1172.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 105: size 2048, parameter 73, shape |bf16[1024]| at ShapeIndex {}:
 value: <1766 p73.1244.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 106: size 2048, parameter 77, shape |bf16[1024]| at ShapeIndex {}:
 value: <1769 p77.1316.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 107: size 2048, parameter 81, shape |bf16[1024]| at ShapeIndex {}:
 value: <1772 p81.1388.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 108: size 2048, parameter 85, shape |bf16[1024]| at ShapeIndex {}:
 value: <1775 p85.1460.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 109: size 2048, parameter 89, shape |bf16[1024]| at ShapeIndex {}:
 value: <1778 p89.1532.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 110: size 2048, parameter 93, shape |bf16[1024]| at ShapeIndex {}:
 value: <1781 p93.1604.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 111: size 2048, parameter 97, shape |bf16[1024]| at ShapeIndex {}:
 value: <1784 p97.1676.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 112: size 2048, parameter 101, shape |bf16[1024]| at ShapeIndex {}:
 value: <1787 p101.1748.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 113: size 2048, parameter 105, shape |bf16[1024]| at ShapeIndex {}:
 value: <1790 p105.1820.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 114: size 2048, parameter 109, shape |bf16[1024]| at ShapeIndex {}:
 value: <1793 p109.1892.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 115: size 2048, parameter 113, shape |bf16[1024]| at ShapeIndex {}:
 value: <1796 p113.1964.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 116: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <1799 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 117: size 256, parameter 4, shape |s32[64]| at ShapeIndex {}:
 value: <1689 p4.66.0 @0> (size=256,offset=0): s32[64]{0}
allocation 118: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <1801 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 119: size 256, parameter 114, shape |s32[64]| at ShapeIndex {}:
 value: <1803 p114.2088.0 @0> (size=256,offset=0): s32[64]{0}
allocation 120: size 32, output shape is |(bf16[64,8,128], bf16[64,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <1805 tuple.2142.0{} @0> (size=32,offset=0): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 121: size 4, thread-local:
 value: <39 add.353 @0> (size=4,offset=0): f32[]
allocation 122: size 4, thread-local:
 value: <38 y.102 @0> (size=4,offset=0): f32[]
allocation 123: size 4, thread-local:
 value: <37 x.101 @0> (size=4,offset=0): f32[]
allocation 124: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <1717 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 125: size 1711008, preallocated-temp:
 value: <1457 custom-call.55.0{} @0> (size=16,offset=1710848): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1458 custom-call.55.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1460 loop_convert_fusion.26 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1461 custom-call.56.0{} @0> (size=16,offset=1710720): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1464 fusion.338 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1465 custom-call.57.0{} @0> (size=16,offset=6656): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1466 custom-call.57.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1468 loop_convert_fusion.25 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1469 custom-call.58.0{} @0> (size=16,offset=0): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1470 custom-call.58.0{0} @0> (size=131072,offset=1448576): bf16[64,1024]{1,0}
 value: <1472 fusion.337 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1473 custom-call.59.0{} @0> (size=16,offset=128): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1474 custom-call.59.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1476 loop_convert_fusion.24 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1477 custom-call.60.0{} @0> (size=16,offset=256): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1478 custom-call.60.0{0} @0> (size=131072,offset=1579648): bf16[64,1024]{1,0}
 value: <1480 fusion.336 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1481 custom-call.61.0{} @0> (size=16,offset=384): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1482 custom-call.61.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1484 loop_convert_fusion.23 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1485 custom-call.62.0{} @0> (size=16,offset=512): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1486 custom-call.62.0{0} @0> (size=131072,offset=6784): bf16[64,1024]{1,0}
 value: <1488 loop_add_fusion @0> (size=262144,offset=1186432): f32[64,1024]{1,0}
 value: <1490 custom-call.63.0{} @0> (size=16,offset=640): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1491 custom-call.63.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1493 loop_convert_fusion.22 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1494 custom-call.64.0{} @0> (size=16,offset=768): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1498 custom-call.65.0{} @0> (size=16,offset=896): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1499 custom-call.65.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1501 loop_convert_fusion.21 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1502 custom-call.66.0{} @0> (size=16,offset=1024): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1505 fusion.333 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1506 custom-call.67.0{} @0> (size=16,offset=1152): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1507 custom-call.67.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1509 loop_convert_fusion.20 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1510 custom-call.68.0{} @0> (size=16,offset=1280): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1511 custom-call.68.0{0} @0> (size=131072,offset=1448576): bf16[64,1024]{1,0}
 value: <1513 fusion.332 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1514 custom-call.69.0{} @0> (size=16,offset=1408): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1515 custom-call.69.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1517 loop_convert_fusion.19 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1518 custom-call.70.0{} @0> (size=16,offset=1536): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1519 custom-call.70.0{0} @0> (size=131072,offset=6784): bf16[64,1024]{1,0}
 value: <1521 loop_add_fusion.1 @0> (size=262144,offset=1186432): f32[64,1024]{1,0}
 value: <1523 custom-call.71.0{} @0> (size=16,offset=1664): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1524 custom-call.71.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1526 loop_convert_fusion.18 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1527 custom-call.72.0{} @0> (size=16,offset=1792): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1531 custom-call.73.0{} @0> (size=16,offset=1920): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1532 custom-call.73.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1534 loop_convert_fusion.17 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1535 custom-call.74.0{} @0> (size=16,offset=2048): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1538 fusion.329 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1539 custom-call.75.0{} @0> (size=16,offset=2176): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1540 custom-call.75.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1542 loop_convert_fusion.16 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1543 custom-call.76.0{} @0> (size=16,offset=2304): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1544 custom-call.76.0{0} @0> (size=131072,offset=1448576): bf16[64,1024]{1,0}
 value: <1546 fusion.328 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1547 custom-call.77.0{} @0> (size=16,offset=2432): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1548 custom-call.77.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1550 loop_convert_fusion.15 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1551 custom-call.78.0{} @0> (size=16,offset=2560): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1552 custom-call.78.0{0} @0> (size=131072,offset=6784): bf16[64,1024]{1,0}
 value: <1554 loop_add_fusion.2 @0> (size=262144,offset=1186432): f32[64,1024]{1,0}
 value: <1556 custom-call.79.0{} @0> (size=16,offset=2688): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1557 custom-call.79.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1559 loop_convert_fusion.14 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1560 custom-call.80.0{} @0> (size=16,offset=2816): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1564 custom-call.81.0{} @0> (size=16,offset=2944): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1565 custom-call.81.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1567 loop_convert_fusion.13 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1568 custom-call.82.0{} @0> (size=16,offset=3072): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1571 fusion.325 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1572 custom-call.83.0{} @0> (size=16,offset=3200): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1573 custom-call.83.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1575 loop_convert_fusion.12 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1576 custom-call.84.0{} @0> (size=16,offset=3328): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1577 custom-call.84.0{0} @0> (size=131072,offset=1448576): bf16[64,1024]{1,0}
 value: <1579 fusion.324 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1580 custom-call.85.0{} @0> (size=16,offset=3456): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1581 custom-call.85.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1583 loop_convert_fusion.11 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1584 custom-call.86.0{} @0> (size=16,offset=3584): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1585 custom-call.86.0{0} @0> (size=131072,offset=6784): bf16[64,1024]{1,0}
 value: <1587 loop_add_fusion.3 @0> (size=262144,offset=1186432): f32[64,1024]{1,0}
 value: <1589 custom-call.87.0{} @0> (size=16,offset=3712): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1590 custom-call.87.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1592 loop_convert_fusion.10 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1593 custom-call.88.0{} @0> (size=16,offset=3840): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1597 custom-call.89.0{} @0> (size=16,offset=3968): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1598 custom-call.89.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1600 loop_convert_fusion.9 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1601 custom-call.90.0{} @0> (size=16,offset=4096): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1604 fusion.321 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1605 custom-call.91.0{} @0> (size=16,offset=4224): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1606 custom-call.91.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1608 loop_convert_fusion.8 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1609 custom-call.92.0{} @0> (size=16,offset=4352): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1610 custom-call.92.0{0} @0> (size=131072,offset=1448576): bf16[64,1024]{1,0}
 value: <1612 fusion.320 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1613 custom-call.93.0{} @0> (size=16,offset=4480): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1614 custom-call.93.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1616 loop_convert_fusion.7 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1617 custom-call.94.0{} @0> (size=16,offset=4608): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1618 custom-call.94.0{0} @0> (size=131072,offset=6784): bf16[64,1024]{1,0}
 value: <1620 loop_add_fusion.4 @0> (size=262144,offset=1186432): f32[64,1024]{1,0}
 value: <1622 custom-call.95.0{} @0> (size=16,offset=4736): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1623 custom-call.95.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1625 loop_convert_fusion.6 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1626 custom-call.96.0{} @0> (size=16,offset=4864): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1630 custom-call.97.0{} @0> (size=16,offset=4992): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1631 custom-call.97.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1633 loop_convert_fusion.5 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1634 custom-call.98.0{} @0> (size=16,offset=5120): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1637 fusion.317 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1638 custom-call.99.0{} @0> (size=16,offset=5248): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1639 custom-call.99.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1641 loop_convert_fusion.4 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1642 custom-call.100.0{} @0> (size=16,offset=5376): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1643 custom-call.100.0{0} @0> (size=131072,offset=1448576): bf16[64,1024]{1,0}
 value: <1645 fusion.316 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1646 custom-call.101.0{} @0> (size=16,offset=5504): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1647 custom-call.101.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1649 loop_convert_fusion.3 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1650 custom-call.102.0{} @0> (size=16,offset=5632): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1651 custom-call.102.0{0} @0> (size=131072,offset=6784): bf16[64,1024]{1,0}
 value: <1653 loop_add_fusion.5 @0> (size=262144,offset=1186432): f32[64,1024]{1,0}
 value: <1655 custom-call.103.0{} @0> (size=16,offset=5760): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1656 custom-call.103.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1658 loop_convert_fusion.2 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1659 custom-call.104.0{} @0> (size=16,offset=5888): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1663 custom-call.105.0{} @0> (size=16,offset=6016): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1664 custom-call.105.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1666 loop_convert_fusion.1 @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1667 custom-call.106.0{} @0> (size=16,offset=6144): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1670 fusion.313 @0> (size=131072,offset=793216): bf16[64,1024]{1,0}
 value: <1671 custom-call.107.0{} @0> (size=16,offset=6272): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1672 custom-call.107.0{0} @0> (size=786432,offset=6784): bf16[64,6144]{1,0}
 value: <1674 loop_convert_fusion @0> (size=393216,offset=793216): bf16[64,3072]{1,0}
 value: <1675 custom-call.108.0{} @0> (size=16,offset=6400): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1676 custom-call.108.0{0} @0> (size=131072,offset=6784): bf16[64,1024]{1,0}
 value: <1678 fusion.311 @0> (size=131072,offset=137856): bf16[64,1024]{1,0}
 value: <1679 custom-call.109.0{} @0> (size=16,offset=6528): (bf16[64,4096]{1,0}, s8[4194304]{0})
 value: <1687 tuple{} @0> (size=32,offset=1710976): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 1509752528 (1.41GiB)

Used values:
<37 x.101 @0>
 positions:
  x.101
 uses:
  add.353, operand 0
 from instruction: %x.101 = f32[] parameter(0)
<38 y.102 @0>
 positions:
  y.102
 uses:
  add.353, operand 1
 from instruction: %y.102 = f32[] parameter(1)
<39 add.353 @0>
 positions:
  add.353
 uses:
 from instruction: %add.353 = f32[] add(f32[] %x.101, f32[] %y.102)
<1453 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.108.0, operand 0
 from instruction: %wrapped_concatenate = bf16[27648,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9, bf16[1024,2048]{1,0} %p.10, bf16[1024,2048]{1,0} %p.11, /*index=10*/bf16[1024,2048]{1,0} %p.12, bf16[1024,2048]{1,0} %p.13, bf16[1024,2048]{1,0} %p.14, bf16[1024,2048]{1,0} %p.15, bf16[1024,2048]{1,0} %p.16, /*index=15*/bf16[1024,2048]{1,0} %p.17, bf16[1024,2048]{1,0} %p.18, bf16[1024,2048]{1,0} %p.19, bf16[1024,2048]{1,0} %p.20, bf16[1024,2048]{1,0} %p.21, /*index=20*/bf16[1024,2048]{1,0} %p.22, bf16[1024,2048]{1,0} %p.23, bf16[1024,2048]{1,0} %p.24, bf16[1024,2048]{1,0} %p.25, bf16[1024,2048]{1,0} %p.26, /*index=25*/bf16[1024,2048]{1,0} %p.27, bf16[1024,2048]{1,0} %p.28), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1454 gemm_fusion_dot.108.0 @0>
 positions:
  gemm_fusion_dot.108.0
 uses:
  fusion.339, operand 2
  fusion.338, operand 2
  fusion.337, operand 3
  fusion.336, operand 2
  loop_add_fusion, operand 0
  fusion.334, operand 3
  fusion.333, operand 4
  fusion.332, operand 2
  loop_add_fusion.1, operand 0
  fusion.330, operand 3
  fusion.329, operand 4
  fusion.328, operand 2
  loop_add_fusion.2, operand 0
  fusion.326, operand 3
  fusion.325, operand 4
  fusion.324, operand 2
  loop_add_fusion.3, operand 0
  fusion.322, operand 3
  fusion.321, operand 4
  fusion.320, operand 2
  loop_add_fusion.4, operand 0
  fusion.318, operand 3
  fusion.317, operand 4
  fusion.316, operand 2
  loop_add_fusion.5, operand 0
  fusion.314, operand 3
  fusion.313, operand 4
  fusion.311, operand 5
 from instruction: %gemm_fusion_dot.108.0 = bf16[64,27648]{1,0} fusion(bf16[27648,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.108_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"64","block_n":"128","block_k":"128","split_k":"1","num_stages":"3","num_warps":"8","num_ctas":"1"}},"force_earliest_schedule":false}
<1455 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.339, operand 1
  fusion.338, operand 1
  fusion.337, operand 2
  fusion.336, operand 4
  loop_add_fusion, operand 3
 from instruction: %loop_gather_fusion = bf16[64,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[64]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1456 fusion.339 @0>
 positions:
  fusion.339
 uses:
  custom-call.55.0, operand 0
 from instruction: %fusion.339 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.30), kind=kCustom, calls=%fused_computation.280, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1457 custom-call.55.0{} @0>
 positions:
  custom-call.55.0 {}
 uses:
  get-tuple-element.55, operand 0 {}
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.339, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1458 custom-call.55.0{0} @0>
 positions:
  custom-call.55.0 {0}
  get-tuple-element.55
 uses:
  loop_convert_fusion.26, operand 0
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.339, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1459 custom-call.55.0{1} @0>
 positions:
  custom-call.55.0 {1}
 uses:
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.339, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1460 loop_convert_fusion.26 @0>
 positions:
  loop_convert_fusion.26
 uses:
  custom-call.56.0, operand 0
 from instruction: %loop_convert_fusion.26 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.55), kind=kLoop, calls=%fused_convert.26, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1461 custom-call.56.0{} @0>
 positions:
  custom-call.56.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.26, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1462 custom-call.56.0{0} @0>
 positions:
  custom-call.56.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.338, operand 3
  fusion.337, operand 4
  fusion.336, operand 5
  loop_add_fusion, operand 4
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.26, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1463 custom-call.56.0{1} @0>
 positions:
  custom-call.56.0 {1}
 uses:
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.26, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1464 fusion.338 @0>
 positions:
  fusion.338
 uses:
  custom-call.57.0, operand 0
 from instruction: %fusion.338 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.1.0, bf16[1024]{0} %p.33), kind=kCustom, calls=%fused_computation.279, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1465 custom-call.57.0{} @0>
 positions:
  custom-call.57.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.338, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1466 custom-call.57.0{0} @0>
 positions:
  custom-call.57.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.25, operand 0
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.338, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1467 custom-call.57.0{1} @0>
 positions:
  custom-call.57.0 {1}
 uses:
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.338, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1468 loop_convert_fusion.25 @0>
 positions:
  loop_convert_fusion.25
 uses:
  custom-call.58.0, operand 0
 from instruction: %loop_convert_fusion.25 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.25, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1469 custom-call.58.0{} @0>
 positions:
  custom-call.58.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.25, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1470 custom-call.58.0{0} @0>
 positions:
  custom-call.58.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.337, operand 5
  fusion.336, operand 6
  loop_add_fusion, operand 5
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.25, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1471 custom-call.58.0{1} @0>
 positions:
  custom-call.58.0 {1}
 uses:
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.25, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1472 fusion.337 @0>
 positions:
  fusion.337
 uses:
  custom-call.59.0, operand 0
 from instruction: %fusion.337 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[1024]{0} %p.36, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.278, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1473 custom-call.59.0{} @0>
 positions:
  custom-call.59.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.337, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1474 custom-call.59.0{0} @0>
 positions:
  custom-call.59.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.24, operand 0
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.337, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1475 custom-call.59.0{1} @0>
 positions:
  custom-call.59.0 {1}
 uses:
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.337, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1476 loop_convert_fusion.24 @0>
 positions:
  loop_convert_fusion.24
 uses:
  custom-call.60.0, operand 0
 from instruction: %loop_convert_fusion.24 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.24, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1477 custom-call.60.0{} @0>
 positions:
  custom-call.60.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.24, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1478 custom-call.60.0{0} @0>
 positions:
  custom-call.60.0 {0}
  get-tuple-element.5.0
 uses:
  fusion.336, operand 1
  loop_add_fusion, operand 2
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.24, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1479 custom-call.60.0{1} @0>
 positions:
  custom-call.60.0 {1}
 uses:
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.24, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1480 fusion.336 @0>
 positions:
  fusion.336
 uses:
  custom-call.61.0, operand 0
 from instruction: %fusion.336 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1024]{1,0} %get-tuple-element.5.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.39, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.1.0, bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.277, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1481 custom-call.61.0{} @0>
 positions:
  custom-call.61.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.61.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.336, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1482 custom-call.61.0{0} @0>
 positions:
  custom-call.61.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.23, operand 0
 from instruction: %custom-call.61.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.336, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1483 custom-call.61.0{1} @0>
 positions:
  custom-call.61.0 {1}
 uses:
 from instruction: %custom-call.61.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.336, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1484 loop_convert_fusion.23 @0>
 positions:
  loop_convert_fusion.23
 uses:
  custom-call.62.0, operand 0
 from instruction: %loop_convert_fusion.23 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.23, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1485 custom-call.62.0{} @0>
 positions:
  custom-call.62.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.62.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.23, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1486 custom-call.62.0{0} @0>
 positions:
  custom-call.62.0 {0}
  get-tuple-element.7.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.62.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.23, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1487 custom-call.62.0{1} @0>
 positions:
  custom-call.62.0 {1}
 uses:
 from instruction: %custom-call.62.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.23, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1488 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.335, operand 0
  fusion.334, operand 1
  fusion.333, operand 2
  fusion.332, operand 4
  loop_add_fusion.1, operand 3
 from instruction: %loop_add_fusion = f32[64,1024]{1,0} fusion(bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.7.0, bf16[64,1024]{1,0} %get-tuple-element.5.0, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.1099/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1489 fusion.335 @0>
 positions:
  fusion.335
 uses:
  custom-call.63.0, operand 0
 from instruction: %fusion.335 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion, f32[] %p.29, bf16[1024]{0} %p.42), kind=kCustom, calls=%fused_computation.276, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.315"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1490 custom-call.63.0{} @0>
 positions:
  custom-call.63.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.63.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.335, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1491 custom-call.63.0{0} @0>
 positions:
  custom-call.63.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.22, operand 0
 from instruction: %custom-call.63.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.335, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1492 custom-call.63.0{1} @0>
 positions:
  custom-call.63.0 {1}
 uses:
 from instruction: %custom-call.63.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.335, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1493 loop_convert_fusion.22 @0>
 positions:
  loop_convert_fusion.22
 uses:
  custom-call.64.0, operand 0
 from instruction: %loop_convert_fusion.22 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.22, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1494 custom-call.64.0{} @0>
 positions:
  custom-call.64.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.64.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1495 custom-call.64.0{0} @0>
 positions:
  custom-call.64.0 {0}
  get-tuple-element.9.0
 uses:
  fusion.334, operand 2
  fusion.333, operand 3
  fusion.332, operand 5
  loop_add_fusion.1, operand 4
 from instruction: %custom-call.64.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1496 custom-call.64.0{1} @0>
 positions:
  custom-call.64.0 {1}
 uses:
 from instruction: %custom-call.64.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1497 fusion.334 @0>
 positions:
  fusion.334
 uses:
  custom-call.65.0, operand 0
 from instruction: %fusion.334 = bf16[64,1024]{1,0} fusion(f32[] %p.29, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.45), kind=kCustom, calls=%fused_computation.275, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1498 custom-call.65.0{} @0>
 positions:
  custom-call.65.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.65.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.334, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1499 custom-call.65.0{0} @0>
 positions:
  custom-call.65.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.21, operand 0
 from instruction: %custom-call.65.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.334, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1500 custom-call.65.0{1} @0>
 positions:
  custom-call.65.0 {1}
 uses:
 from instruction: %custom-call.65.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.334, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1501 loop_convert_fusion.21 @0>
 positions:
  loop_convert_fusion.21
 uses:
  custom-call.66.0, operand 0
 from instruction: %loop_convert_fusion.21 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.21, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1502 custom-call.66.0{} @0>
 positions:
  custom-call.66.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.66.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1503 custom-call.66.0{0} @0>
 positions:
  custom-call.66.0 {0}
  get-tuple-element.11.0
 uses:
  fusion.333, operand 5
  fusion.332, operand 6
  loop_add_fusion.1, operand 5
 from instruction: %custom-call.66.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1504 custom-call.66.0{1} @0>
 positions:
  custom-call.66.0 {1}
 uses:
 from instruction: %custom-call.66.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1505 fusion.333 @0>
 positions:
  fusion.333
 uses:
  custom-call.67.0, operand 0
 from instruction: %fusion.333 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[1024]{0} %p.48, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.274, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1506 custom-call.67.0{} @0>
 positions:
  custom-call.67.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.67.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.333, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1507 custom-call.67.0{0} @0>
 positions:
  custom-call.67.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.20, operand 0
 from instruction: %custom-call.67.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.333, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1508 custom-call.67.0{1} @0>
 positions:
  custom-call.67.0 {1}
 uses:
 from instruction: %custom-call.67.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.333, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1509 loop_convert_fusion.20 @0>
 positions:
  loop_convert_fusion.20
 uses:
  custom-call.68.0, operand 0
 from instruction: %loop_convert_fusion.20 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.20, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1510 custom-call.68.0{} @0>
 positions:
  custom-call.68.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.68.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1511 custom-call.68.0{0} @0>
 positions:
  custom-call.68.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.332, operand 1
  loop_add_fusion.1, operand 2
 from instruction: %custom-call.68.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1512 custom-call.68.0{1} @0>
 positions:
  custom-call.68.0 {1}
 uses:
 from instruction: %custom-call.68.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1513 fusion.332 @0>
 positions:
  fusion.332
 uses:
  custom-call.69.0, operand 0
 from instruction: %fusion.332 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1024]{1,0} %get-tuple-element.13.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.51, f32[64,1024]{1,0} %loop_add_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.273, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1514 custom-call.69.0{} @0>
 positions:
  custom-call.69.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.69.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.332, bf16[6144,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1515 custom-call.69.0{0} @0>
 positions:
  custom-call.69.0 {0}
  get-tuple-element.14.0
 uses:
  loop_convert_fusion.19, operand 0
 from instruction: %custom-call.69.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.332, bf16[6144,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1516 custom-call.69.0{1} @0>
 positions:
  custom-call.69.0 {1}
 uses:
 from instruction: %custom-call.69.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.332, bf16[6144,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1517 loop_convert_fusion.19 @0>
 positions:
  loop_convert_fusion.19
 uses:
  custom-call.70.0, operand 0
 from instruction: %loop_convert_fusion.19 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert.19, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1518 custom-call.70.0{} @0>
 positions:
  custom-call.70.0 {}
 uses:
  get-tuple-element.15.0, operand 0 {}
 from instruction: %custom-call.70.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1519 custom-call.70.0{0} @0>
 positions:
  custom-call.70.0 {0}
  get-tuple-element.15.0
 uses:
  loop_add_fusion.1, operand 1
 from instruction: %custom-call.70.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1520 custom-call.70.0{1} @0>
 positions:
  custom-call.70.0 {1}
 uses:
 from instruction: %custom-call.70.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1521 loop_add_fusion.1 @0>
 positions:
  loop_add_fusion.1
 uses:
  fusion.331, operand 0
  fusion.330, operand 1
  fusion.329, operand 2
  fusion.328, operand 4
  loop_add_fusion.2, operand 3
 from instruction: %loop_add_fusion.1 = f32[64,1024]{1,0} fusion(bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.15.0, bf16[64,1024]{1,0} %get-tuple-element.13.0, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kLoop, calls=%fused_add.1, metadata={op_type="aten__add" op_name="aten__add.1171/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1522 fusion.331 @0>
 positions:
  fusion.331
 uses:
  custom-call.71.0, operand 0
 from instruction: %fusion.331 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.1, f32[] %p.29, bf16[1024]{0} %p.54), kind=kCustom, calls=%fused_computation.272, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.315"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1523 custom-call.71.0{} @0>
 positions:
  custom-call.71.0 {}
 uses:
  get-tuple-element.16.0, operand 0 {}
 from instruction: %custom-call.71.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.331, bf16[6144,1024]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1524 custom-call.71.0{0} @0>
 positions:
  custom-call.71.0 {0}
  get-tuple-element.16.0
 uses:
  loop_convert_fusion.18, operand 0
 from instruction: %custom-call.71.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.331, bf16[6144,1024]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1525 custom-call.71.0{1} @0>
 positions:
  custom-call.71.0 {1}
 uses:
 from instruction: %custom-call.71.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.331, bf16[6144,1024]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1526 loop_convert_fusion.18 @0>
 positions:
  loop_convert_fusion.18
 uses:
  custom-call.72.0, operand 0
 from instruction: %loop_convert_fusion.18 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%fused_convert.18, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1527 custom-call.72.0{} @0>
 positions:
  custom-call.72.0 {}
 uses:
  get-tuple-element.17.0, operand 0 {}
 from instruction: %custom-call.72.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1528 custom-call.72.0{0} @0>
 positions:
  custom-call.72.0 {0}
  get-tuple-element.17.0
 uses:
  fusion.330, operand 2
  fusion.329, operand 3
  fusion.328, operand 5
  loop_add_fusion.2, operand 4
 from instruction: %custom-call.72.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1529 custom-call.72.0{1} @0>
 positions:
  custom-call.72.0 {1}
 uses:
 from instruction: %custom-call.72.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1530 fusion.330 @0>
 positions:
  fusion.330
 uses:
  custom-call.73.0, operand 0
 from instruction: %fusion.330 = bf16[64,1024]{1,0} fusion(f32[] %p.29, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.57), kind=kCustom, calls=%fused_computation.271, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1531 custom-call.73.0{} @0>
 positions:
  custom-call.73.0 {}
 uses:
  get-tuple-element.18.0, operand 0 {}
 from instruction: %custom-call.73.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.330, bf16[6144,1024]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1532 custom-call.73.0{0} @0>
 positions:
  custom-call.73.0 {0}
  get-tuple-element.18.0
 uses:
  loop_convert_fusion.17, operand 0
 from instruction: %custom-call.73.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.330, bf16[6144,1024]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1533 custom-call.73.0{1} @0>
 positions:
  custom-call.73.0 {1}
 uses:
 from instruction: %custom-call.73.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.330, bf16[6144,1024]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1534 loop_convert_fusion.17 @0>
 positions:
  loop_convert_fusion.17
 uses:
  custom-call.74.0, operand 0
 from instruction: %loop_convert_fusion.17 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.18.0), kind=kLoop, calls=%fused_convert.17, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1535 custom-call.74.0{} @0>
 positions:
  custom-call.74.0 {}
 uses:
  get-tuple-element.19.0, operand 0 {}
 from instruction: %custom-call.74.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1536 custom-call.74.0{0} @0>
 positions:
  custom-call.74.0 {0}
  get-tuple-element.19.0
 uses:
  fusion.329, operand 5
  fusion.328, operand 6
  loop_add_fusion.2, operand 5
 from instruction: %custom-call.74.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1537 custom-call.74.0{1} @0>
 positions:
  custom-call.74.0 {1}
 uses:
 from instruction: %custom-call.74.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1538 fusion.329 @0>
 positions:
  fusion.329
 uses:
  custom-call.75.0, operand 0
 from instruction: %fusion.329 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[1024]{0} %p.60, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kCustom, calls=%fused_computation.270, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1539 custom-call.75.0{} @0>
 positions:
  custom-call.75.0 {}
 uses:
  get-tuple-element.20.0, operand 0 {}
 from instruction: %custom-call.75.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.329, bf16[6144,1024]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1540 custom-call.75.0{0} @0>
 positions:
  custom-call.75.0 {0}
  get-tuple-element.20.0
 uses:
  loop_convert_fusion.16, operand 0
 from instruction: %custom-call.75.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.329, bf16[6144,1024]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1541 custom-call.75.0{1} @0>
 positions:
  custom-call.75.0 {1}
 uses:
 from instruction: %custom-call.75.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.329, bf16[6144,1024]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1542 loop_convert_fusion.16 @0>
 positions:
  loop_convert_fusion.16
 uses:
  custom-call.76.0, operand 0
 from instruction: %loop_convert_fusion.16 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.20.0), kind=kLoop, calls=%fused_convert.16, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1543 custom-call.76.0{} @0>
 positions:
  custom-call.76.0 {}
 uses:
  get-tuple-element.21.0, operand 0 {}
 from instruction: %custom-call.76.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1544 custom-call.76.0{0} @0>
 positions:
  custom-call.76.0 {0}
  get-tuple-element.21.0
 uses:
  fusion.328, operand 1
  loop_add_fusion.2, operand 2
 from instruction: %custom-call.76.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1545 custom-call.76.0{1} @0>
 positions:
  custom-call.76.0 {1}
 uses:
 from instruction: %custom-call.76.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1546 fusion.328 @0>
 positions:
  fusion.328
 uses:
  custom-call.77.0, operand 0
 from instruction: %fusion.328 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1024]{1,0} %get-tuple-element.21.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.63, f32[64,1024]{1,0} %loop_add_fusion.1, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kCustom, calls=%fused_computation.269, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1547 custom-call.77.0{} @0>
 positions:
  custom-call.77.0 {}
 uses:
  get-tuple-element.22.0, operand 0 {}
 from instruction: %custom-call.77.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.328, bf16[6144,1024]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1548 custom-call.77.0{0} @0>
 positions:
  custom-call.77.0 {0}
  get-tuple-element.22.0
 uses:
  loop_convert_fusion.15, operand 0
 from instruction: %custom-call.77.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.328, bf16[6144,1024]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1549 custom-call.77.0{1} @0>
 positions:
  custom-call.77.0 {1}
 uses:
 from instruction: %custom-call.77.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.328, bf16[6144,1024]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1550 loop_convert_fusion.15 @0>
 positions:
  loop_convert_fusion.15
 uses:
  custom-call.78.0, operand 0
 from instruction: %loop_convert_fusion.15 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.22.0), kind=kLoop, calls=%fused_convert.15, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1551 custom-call.78.0{} @0>
 positions:
  custom-call.78.0 {}
 uses:
  get-tuple-element.23.0, operand 0 {}
 from instruction: %custom-call.78.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.65), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1552 custom-call.78.0{0} @0>
 positions:
  custom-call.78.0 {0}
  get-tuple-element.23.0
 uses:
  loop_add_fusion.2, operand 1
 from instruction: %custom-call.78.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.65), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1553 custom-call.78.0{1} @0>
 positions:
  custom-call.78.0 {1}
 uses:
 from instruction: %custom-call.78.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.65), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1554 loop_add_fusion.2 @0>
 positions:
  loop_add_fusion.2
 uses:
  fusion.327, operand 0
  fusion.326, operand 1
  fusion.325, operand 2
  fusion.324, operand 4
  loop_add_fusion.3, operand 3
 from instruction: %loop_add_fusion.2 = f32[64,1024]{1,0} fusion(bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.23.0, bf16[64,1024]{1,0} %get-tuple-element.21.0, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kLoop, calls=%fused_add.2, metadata={op_type="aten__add" op_name="aten__add.1243/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1555 fusion.327 @0>
 positions:
  fusion.327
 uses:
  custom-call.79.0, operand 0
 from instruction: %fusion.327 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.2, f32[] %p.29, bf16[1024]{0} %p.66), kind=kCustom, calls=%fused_computation.268, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.315"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1556 custom-call.79.0{} @0>
 positions:
  custom-call.79.0 {}
 uses:
  get-tuple-element.24.0, operand 0 {}
 from instruction: %custom-call.79.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.327, bf16[6144,1024]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1557 custom-call.79.0{0} @0>
 positions:
  custom-call.79.0 {0}
  get-tuple-element.24.0
 uses:
  loop_convert_fusion.14, operand 0
 from instruction: %custom-call.79.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.327, bf16[6144,1024]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1558 custom-call.79.0{1} @0>
 positions:
  custom-call.79.0 {1}
 uses:
 from instruction: %custom-call.79.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.327, bf16[6144,1024]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1559 loop_convert_fusion.14 @0>
 positions:
  loop_convert_fusion.14
 uses:
  custom-call.80.0, operand 0
 from instruction: %loop_convert_fusion.14 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.24.0), kind=kLoop, calls=%fused_convert.14, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1560 custom-call.80.0{} @0>
 positions:
  custom-call.80.0 {}
 uses:
  get-tuple-element.25.0, operand 0 {}
 from instruction: %custom-call.80.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.68), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1561 custom-call.80.0{0} @0>
 positions:
  custom-call.80.0 {0}
  get-tuple-element.25.0
 uses:
  fusion.326, operand 2
  fusion.325, operand 3
  fusion.324, operand 5
  loop_add_fusion.3, operand 4
 from instruction: %custom-call.80.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.68), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1562 custom-call.80.0{1} @0>
 positions:
  custom-call.80.0 {1}
 uses:
 from instruction: %custom-call.80.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.68), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1563 fusion.326 @0>
 positions:
  fusion.326
 uses:
  custom-call.81.0, operand 0
 from instruction: %fusion.326 = bf16[64,1024]{1,0} fusion(f32[] %p.29, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.69), kind=kCustom, calls=%fused_computation.267, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1564 custom-call.81.0{} @0>
 positions:
  custom-call.81.0 {}
 uses:
  get-tuple-element.26.0, operand 0 {}
 from instruction: %custom-call.81.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.326, bf16[6144,1024]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1565 custom-call.81.0{0} @0>
 positions:
  custom-call.81.0 {0}
  get-tuple-element.26.0
 uses:
  loop_convert_fusion.13, operand 0
 from instruction: %custom-call.81.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.326, bf16[6144,1024]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1566 custom-call.81.0{1} @0>
 positions:
  custom-call.81.0 {1}
 uses:
 from instruction: %custom-call.81.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.326, bf16[6144,1024]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1567 loop_convert_fusion.13 @0>
 positions:
  loop_convert_fusion.13
 uses:
  custom-call.82.0, operand 0
 from instruction: %loop_convert_fusion.13 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.26.0), kind=kLoop, calls=%fused_convert.13, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1568 custom-call.82.0{} @0>
 positions:
  custom-call.82.0 {}
 uses:
  get-tuple-element.27.0, operand 0 {}
 from instruction: %custom-call.82.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.71), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1569 custom-call.82.0{0} @0>
 positions:
  custom-call.82.0 {0}
  get-tuple-element.27.0
 uses:
  fusion.325, operand 5
  fusion.324, operand 6
  loop_add_fusion.3, operand 5
 from instruction: %custom-call.82.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.71), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1570 custom-call.82.0{1} @0>
 positions:
  custom-call.82.0 {1}
 uses:
 from instruction: %custom-call.82.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.71), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1571 fusion.325 @0>
 positions:
  fusion.325
 uses:
  custom-call.83.0, operand 0
 from instruction: %fusion.325 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[1024]{0} %p.72, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.27.0), kind=kCustom, calls=%fused_computation.266, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1572 custom-call.83.0{} @0>
 positions:
  custom-call.83.0 {}
 uses:
  get-tuple-element.28.0, operand 0 {}
 from instruction: %custom-call.83.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.325, bf16[6144,1024]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1573 custom-call.83.0{0} @0>
 positions:
  custom-call.83.0 {0}
  get-tuple-element.28.0
 uses:
  loop_convert_fusion.12, operand 0
 from instruction: %custom-call.83.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.325, bf16[6144,1024]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1574 custom-call.83.0{1} @0>
 positions:
  custom-call.83.0 {1}
 uses:
 from instruction: %custom-call.83.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.325, bf16[6144,1024]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1575 loop_convert_fusion.12 @0>
 positions:
  loop_convert_fusion.12
 uses:
  custom-call.84.0, operand 0
 from instruction: %loop_convert_fusion.12 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.28.0), kind=kLoop, calls=%fused_convert.12, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1576 custom-call.84.0{} @0>
 positions:
  custom-call.84.0 {}
 uses:
  get-tuple-element.29.0, operand 0 {}
 from instruction: %custom-call.84.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.74), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1577 custom-call.84.0{0} @0>
 positions:
  custom-call.84.0 {0}
  get-tuple-element.29.0
 uses:
  fusion.324, operand 1
  loop_add_fusion.3, operand 2
 from instruction: %custom-call.84.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.74), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1578 custom-call.84.0{1} @0>
 positions:
  custom-call.84.0 {1}
 uses:
 from instruction: %custom-call.84.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.74), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1579 fusion.324 @0>
 positions:
  fusion.324
 uses:
  custom-call.85.0, operand 0
 from instruction: %fusion.324 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1024]{1,0} %get-tuple-element.29.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.75, f32[64,1024]{1,0} %loop_add_fusion.2, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.25.0, bf16[64,1024]{1,0} %get-tuple-element.27.0), kind=kCustom, calls=%fused_computation.265, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1580 custom-call.85.0{} @0>
 positions:
  custom-call.85.0 {}
 uses:
  get-tuple-element.30.0, operand 0 {}
 from instruction: %custom-call.85.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.324, bf16[6144,1024]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1581 custom-call.85.0{0} @0>
 positions:
  custom-call.85.0 {0}
  get-tuple-element.30.0
 uses:
  loop_convert_fusion.11, operand 0
 from instruction: %custom-call.85.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.324, bf16[6144,1024]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1582 custom-call.85.0{1} @0>
 positions:
  custom-call.85.0 {1}
 uses:
 from instruction: %custom-call.85.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.324, bf16[6144,1024]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1583 loop_convert_fusion.11 @0>
 positions:
  loop_convert_fusion.11
 uses:
  custom-call.86.0, operand 0
 from instruction: %loop_convert_fusion.11 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.30.0), kind=kLoop, calls=%fused_convert.11, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1584 custom-call.86.0{} @0>
 positions:
  custom-call.86.0 {}
 uses:
  get-tuple-element.31.0, operand 0 {}
 from instruction: %custom-call.86.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.77), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1585 custom-call.86.0{0} @0>
 positions:
  custom-call.86.0 {0}
  get-tuple-element.31.0
 uses:
  loop_add_fusion.3, operand 1
 from instruction: %custom-call.86.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.77), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1586 custom-call.86.0{1} @0>
 positions:
  custom-call.86.0 {1}
 uses:
 from instruction: %custom-call.86.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.77), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1587 loop_add_fusion.3 @0>
 positions:
  loop_add_fusion.3
 uses:
  fusion.323, operand 0
  fusion.322, operand 1
  fusion.321, operand 2
  fusion.320, operand 4
  loop_add_fusion.4, operand 3
 from instruction: %loop_add_fusion.3 = f32[64,1024]{1,0} fusion(bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.31.0, bf16[64,1024]{1,0} %get-tuple-element.29.0, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.27.0), kind=kLoop, calls=%fused_add.3, metadata={op_type="aten__add" op_name="aten__add.1315/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1588 fusion.323 @0>
 positions:
  fusion.323
 uses:
  custom-call.87.0, operand 0
 from instruction: %fusion.323 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.3, f32[] %p.29, bf16[1024]{0} %p.78), kind=kCustom, calls=%fused_computation.264, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.315"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1589 custom-call.87.0{} @0>
 positions:
  custom-call.87.0 {}
 uses:
  get-tuple-element.32.0, operand 0 {}
 from instruction: %custom-call.87.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.323, bf16[6144,1024]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1590 custom-call.87.0{0} @0>
 positions:
  custom-call.87.0 {0}
  get-tuple-element.32.0
 uses:
  loop_convert_fusion.10, operand 0
 from instruction: %custom-call.87.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.323, bf16[6144,1024]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1591 custom-call.87.0{1} @0>
 positions:
  custom-call.87.0 {1}
 uses:
 from instruction: %custom-call.87.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.323, bf16[6144,1024]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1592 loop_convert_fusion.10 @0>
 positions:
  loop_convert_fusion.10
 uses:
  custom-call.88.0, operand 0
 from instruction: %loop_convert_fusion.10 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.32.0), kind=kLoop, calls=%fused_convert.10, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1593 custom-call.88.0{} @0>
 positions:
  custom-call.88.0 {}
 uses:
  get-tuple-element.33.0, operand 0 {}
 from instruction: %custom-call.88.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.80), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1594 custom-call.88.0{0} @0>
 positions:
  custom-call.88.0 {0}
  get-tuple-element.33.0
 uses:
  fusion.322, operand 2
  fusion.321, operand 3
  fusion.320, operand 5
  loop_add_fusion.4, operand 4
 from instruction: %custom-call.88.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.80), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1595 custom-call.88.0{1} @0>
 positions:
  custom-call.88.0 {1}
 uses:
 from instruction: %custom-call.88.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.80), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1596 fusion.322 @0>
 positions:
  fusion.322
 uses:
  custom-call.89.0, operand 0
 from instruction: %fusion.322 = bf16[64,1024]{1,0} fusion(f32[] %p.29, f32[64,1024]{1,0} %loop_add_fusion.3, bf16[64,1024]{1,0} %get-tuple-element.33.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.81), kind=kCustom, calls=%fused_computation.263, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1597 custom-call.89.0{} @0>
 positions:
  custom-call.89.0 {}
 uses:
  get-tuple-element.34.0, operand 0 {}
 from instruction: %custom-call.89.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.322, bf16[6144,1024]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1598 custom-call.89.0{0} @0>
 positions:
  custom-call.89.0 {0}
  get-tuple-element.34.0
 uses:
  loop_convert_fusion.9, operand 0
 from instruction: %custom-call.89.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.322, bf16[6144,1024]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1599 custom-call.89.0{1} @0>
 positions:
  custom-call.89.0 {1}
 uses:
 from instruction: %custom-call.89.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.322, bf16[6144,1024]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1600 loop_convert_fusion.9 @0>
 positions:
  loop_convert_fusion.9
 uses:
  custom-call.90.0, operand 0
 from instruction: %loop_convert_fusion.9 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.34.0), kind=kLoop, calls=%fused_convert.9, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1601 custom-call.90.0{} @0>
 positions:
  custom-call.90.0 {}
 uses:
  get-tuple-element.35.0, operand 0 {}
 from instruction: %custom-call.90.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.83), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1602 custom-call.90.0{0} @0>
 positions:
  custom-call.90.0 {0}
  get-tuple-element.35.0
 uses:
  fusion.321, operand 5
  fusion.320, operand 6
  loop_add_fusion.4, operand 5
 from instruction: %custom-call.90.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.83), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1603 custom-call.90.0{1} @0>
 positions:
  custom-call.90.0 {1}
 uses:
 from instruction: %custom-call.90.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.83), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1604 fusion.321 @0>
 positions:
  fusion.321
 uses:
  custom-call.91.0, operand 0
 from instruction: %fusion.321 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[1024]{0} %p.84, f32[64,1024]{1,0} %loop_add_fusion.3, bf16[64,1024]{1,0} %get-tuple-element.33.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.35.0), kind=kCustom, calls=%fused_computation.262, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1605 custom-call.91.0{} @0>
 positions:
  custom-call.91.0 {}
 uses:
  get-tuple-element.36.0, operand 0 {}
 from instruction: %custom-call.91.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.321, bf16[6144,1024]{1,0} %p.85), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1606 custom-call.91.0{0} @0>
 positions:
  custom-call.91.0 {0}
  get-tuple-element.36.0
 uses:
  loop_convert_fusion.8, operand 0
 from instruction: %custom-call.91.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.321, bf16[6144,1024]{1,0} %p.85), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1607 custom-call.91.0{1} @0>
 positions:
  custom-call.91.0 {1}
 uses:
 from instruction: %custom-call.91.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.321, bf16[6144,1024]{1,0} %p.85), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1608 loop_convert_fusion.8 @0>
 positions:
  loop_convert_fusion.8
 uses:
  custom-call.92.0, operand 0
 from instruction: %loop_convert_fusion.8 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.36.0), kind=kLoop, calls=%fused_convert.8, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1609 custom-call.92.0{} @0>
 positions:
  custom-call.92.0 {}
 uses:
  get-tuple-element.37.0, operand 0 {}
 from instruction: %custom-call.92.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.86), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1610 custom-call.92.0{0} @0>
 positions:
  custom-call.92.0 {0}
  get-tuple-element.37.0
 uses:
  fusion.320, operand 1
  loop_add_fusion.4, operand 2
 from instruction: %custom-call.92.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.86), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1611 custom-call.92.0{1} @0>
 positions:
  custom-call.92.0 {1}
 uses:
 from instruction: %custom-call.92.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.86), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1612 fusion.320 @0>
 positions:
  fusion.320
 uses:
  custom-call.93.0, operand 0
 from instruction: %fusion.320 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1024]{1,0} %get-tuple-element.37.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.87, f32[64,1024]{1,0} %loop_add_fusion.3, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.33.0, bf16[64,1024]{1,0} %get-tuple-element.35.0), kind=kCustom, calls=%fused_computation.261, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1613 custom-call.93.0{} @0>
 positions:
  custom-call.93.0 {}
 uses:
  get-tuple-element.38.0, operand 0 {}
 from instruction: %custom-call.93.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.320, bf16[6144,1024]{1,0} %p.88), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1614 custom-call.93.0{0} @0>
 positions:
  custom-call.93.0 {0}
  get-tuple-element.38.0
 uses:
  loop_convert_fusion.7, operand 0
 from instruction: %custom-call.93.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.320, bf16[6144,1024]{1,0} %p.88), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1615 custom-call.93.0{1} @0>
 positions:
  custom-call.93.0 {1}
 uses:
 from instruction: %custom-call.93.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.320, bf16[6144,1024]{1,0} %p.88), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1616 loop_convert_fusion.7 @0>
 positions:
  loop_convert_fusion.7
 uses:
  custom-call.94.0, operand 0
 from instruction: %loop_convert_fusion.7 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.38.0), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1617 custom-call.94.0{} @0>
 positions:
  custom-call.94.0 {}
 uses:
  get-tuple-element.39.0, operand 0 {}
 from instruction: %custom-call.94.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.89), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1618 custom-call.94.0{0} @0>
 positions:
  custom-call.94.0 {0}
  get-tuple-element.39.0
 uses:
  loop_add_fusion.4, operand 1
 from instruction: %custom-call.94.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.89), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1619 custom-call.94.0{1} @0>
 positions:
  custom-call.94.0 {1}
 uses:
 from instruction: %custom-call.94.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.89), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1620 loop_add_fusion.4 @0>
 positions:
  loop_add_fusion.4
 uses:
  fusion.319, operand 0
  fusion.318, operand 1
  fusion.317, operand 2
  fusion.316, operand 4
  loop_add_fusion.5, operand 3
 from instruction: %loop_add_fusion.4 = f32[64,1024]{1,0} fusion(bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.39.0, bf16[64,1024]{1,0} %get-tuple-element.37.0, f32[64,1024]{1,0} %loop_add_fusion.3, bf16[64,1024]{1,0} %get-tuple-element.33.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.35.0), kind=kLoop, calls=%fused_add.4, metadata={op_type="aten__add" op_name="aten__add.1387/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1621 fusion.319 @0>
 positions:
  fusion.319
 uses:
  custom-call.95.0, operand 0
 from instruction: %fusion.319 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.4, f32[] %p.29, bf16[1024]{0} %p.90), kind=kCustom, calls=%fused_computation.260, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.315"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1622 custom-call.95.0{} @0>
 positions:
  custom-call.95.0 {}
 uses:
  get-tuple-element.40.0, operand 0 {}
 from instruction: %custom-call.95.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.319, bf16[6144,1024]{1,0} %p.91), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1623 custom-call.95.0{0} @0>
 positions:
  custom-call.95.0 {0}
  get-tuple-element.40.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.95.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.319, bf16[6144,1024]{1,0} %p.91), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1624 custom-call.95.0{1} @0>
 positions:
  custom-call.95.0 {1}
 uses:
 from instruction: %custom-call.95.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.319, bf16[6144,1024]{1,0} %p.91), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1625 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.96.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.40.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1626 custom-call.96.0{} @0>
 positions:
  custom-call.96.0 {}
 uses:
  get-tuple-element.41.0, operand 0 {}
 from instruction: %custom-call.96.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.92), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1627 custom-call.96.0{0} @0>
 positions:
  custom-call.96.0 {0}
  get-tuple-element.41.0
 uses:
  fusion.318, operand 2
  fusion.317, operand 3
  fusion.316, operand 5
  loop_add_fusion.5, operand 4
 from instruction: %custom-call.96.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.92), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1628 custom-call.96.0{1} @0>
 positions:
  custom-call.96.0 {1}
 uses:
 from instruction: %custom-call.96.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.92), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1629 fusion.318 @0>
 positions:
  fusion.318
 uses:
  custom-call.97.0, operand 0
 from instruction: %fusion.318 = bf16[64,1024]{1,0} fusion(f32[] %p.29, f32[64,1024]{1,0} %loop_add_fusion.4, bf16[64,1024]{1,0} %get-tuple-element.41.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.93), kind=kCustom, calls=%fused_computation.259, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1630 custom-call.97.0{} @0>
 positions:
  custom-call.97.0 {}
 uses:
  get-tuple-element.42.0, operand 0 {}
 from instruction: %custom-call.97.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.318, bf16[6144,1024]{1,0} %p.94), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1631 custom-call.97.0{0} @0>
 positions:
  custom-call.97.0 {0}
  get-tuple-element.42.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.97.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.318, bf16[6144,1024]{1,0} %p.94), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1632 custom-call.97.0{1} @0>
 positions:
  custom-call.97.0 {1}
 uses:
 from instruction: %custom-call.97.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.318, bf16[6144,1024]{1,0} %p.94), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1633 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.98.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.42.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1634 custom-call.98.0{} @0>
 positions:
  custom-call.98.0 {}
 uses:
  get-tuple-element.43.0, operand 0 {}
 from instruction: %custom-call.98.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.95), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1635 custom-call.98.0{0} @0>
 positions:
  custom-call.98.0 {0}
  get-tuple-element.43.0
 uses:
  fusion.317, operand 5
  fusion.316, operand 6
  loop_add_fusion.5, operand 5
 from instruction: %custom-call.98.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.95), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1636 custom-call.98.0{1} @0>
 positions:
  custom-call.98.0 {1}
 uses:
 from instruction: %custom-call.98.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.95), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1637 fusion.317 @0>
 positions:
  fusion.317
 uses:
  custom-call.99.0, operand 0
 from instruction: %fusion.317 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[1024]{0} %p.96, f32[64,1024]{1,0} %loop_add_fusion.4, bf16[64,1024]{1,0} %get-tuple-element.41.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.43.0), kind=kCustom, calls=%fused_computation.258, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1638 custom-call.99.0{} @0>
 positions:
  custom-call.99.0 {}
 uses:
  get-tuple-element.44.0, operand 0 {}
 from instruction: %custom-call.99.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.317, bf16[6144,1024]{1,0} %p.97), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1639 custom-call.99.0{0} @0>
 positions:
  custom-call.99.0 {0}
  get-tuple-element.44.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.99.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.317, bf16[6144,1024]{1,0} %p.97), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1640 custom-call.99.0{1} @0>
 positions:
  custom-call.99.0 {1}
 uses:
 from instruction: %custom-call.99.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.317, bf16[6144,1024]{1,0} %p.97), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1641 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.100.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.44.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1642 custom-call.100.0{} @0>
 positions:
  custom-call.100.0 {}
 uses:
  get-tuple-element.45.0, operand 0 {}
 from instruction: %custom-call.100.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.98), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1643 custom-call.100.0{0} @0>
 positions:
  custom-call.100.0 {0}
  get-tuple-element.45.0
 uses:
  fusion.316, operand 1
  loop_add_fusion.5, operand 2
 from instruction: %custom-call.100.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.98), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1644 custom-call.100.0{1} @0>
 positions:
  custom-call.100.0 {1}
 uses:
 from instruction: %custom-call.100.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.98), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1645 fusion.316 @0>
 positions:
  fusion.316
 uses:
  custom-call.101.0, operand 0
 from instruction: %fusion.316 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1024]{1,0} %get-tuple-element.45.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.99, f32[64,1024]{1,0} %loop_add_fusion.4, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.41.0, bf16[64,1024]{1,0} %get-tuple-element.43.0), kind=kCustom, calls=%fused_computation.257, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1646 custom-call.101.0{} @0>
 positions:
  custom-call.101.0 {}
 uses:
  get-tuple-element.46.0, operand 0 {}
 from instruction: %custom-call.101.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.316, bf16[6144,1024]{1,0} %p.100), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1647 custom-call.101.0{0} @0>
 positions:
  custom-call.101.0 {0}
  get-tuple-element.46.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.101.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.316, bf16[6144,1024]{1,0} %p.100), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1648 custom-call.101.0{1} @0>
 positions:
  custom-call.101.0 {1}
 uses:
 from instruction: %custom-call.101.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.316, bf16[6144,1024]{1,0} %p.100), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1649 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.102.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.46.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1650 custom-call.102.0{} @0>
 positions:
  custom-call.102.0 {}
 uses:
  get-tuple-element.47.0, operand 0 {}
 from instruction: %custom-call.102.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.101), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1651 custom-call.102.0{0} @0>
 positions:
  custom-call.102.0 {0}
  get-tuple-element.47.0
 uses:
  loop_add_fusion.5, operand 1
 from instruction: %custom-call.102.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.101), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1652 custom-call.102.0{1} @0>
 positions:
  custom-call.102.0 {1}
 uses:
 from instruction: %custom-call.102.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.101), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1653 loop_add_fusion.5 @0>
 positions:
  loop_add_fusion.5
 uses:
  fusion.315, operand 0
  fusion.314, operand 1
  fusion.313, operand 2
  fusion.311, operand 3
 from instruction: %loop_add_fusion.5 = f32[64,1024]{1,0} fusion(bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.47.0, bf16[64,1024]{1,0} %get-tuple-element.45.0, f32[64,1024]{1,0} %loop_add_fusion.4, bf16[64,1024]{1,0} %get-tuple-element.41.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.43.0), kind=kLoop, calls=%fused_add.5, metadata={op_type="aten__add" op_name="aten__add.1459/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1654 fusion.315 @0>
 positions:
  fusion.315
 uses:
  custom-call.103.0, operand 0
 from instruction: %fusion.315 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.5, f32[] %p.29, bf16[1024]{0} %p.102), kind=kCustom, calls=%fused_computation.256, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.315"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1655 custom-call.103.0{} @0>
 positions:
  custom-call.103.0 {}
 uses:
  get-tuple-element.48.0, operand 0 {}
 from instruction: %custom-call.103.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.315, bf16[6144,1024]{1,0} %p.103), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1656 custom-call.103.0{0} @0>
 positions:
  custom-call.103.0 {0}
  get-tuple-element.48.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.103.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.315, bf16[6144,1024]{1,0} %p.103), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1657 custom-call.103.0{1} @0>
 positions:
  custom-call.103.0 {1}
 uses:
 from instruction: %custom-call.103.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.315, bf16[6144,1024]{1,0} %p.103), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1658 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.104.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.48.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1659 custom-call.104.0{} @0>
 positions:
  custom-call.104.0 {}
 uses:
  get-tuple-element.49.0, operand 0 {}
 from instruction: %custom-call.104.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.104), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1660 custom-call.104.0{0} @0>
 positions:
  custom-call.104.0 {0}
  get-tuple-element.49.0
 uses:
  fusion.314, operand 2
  fusion.313, operand 3
  fusion.311, operand 4
 from instruction: %custom-call.104.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.104), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1661 custom-call.104.0{1} @0>
 positions:
  custom-call.104.0 {1}
 uses:
 from instruction: %custom-call.104.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.104), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1662 fusion.314 @0>
 positions:
  fusion.314
 uses:
  custom-call.105.0, operand 0
 from instruction: %fusion.314 = bf16[64,1024]{1,0} fusion(f32[] %p.29, f32[64,1024]{1,0} %loop_add_fusion.5, bf16[64,1024]{1,0} %get-tuple-element.49.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[1024]{0} %p.105), kind=kCustom, calls=%fused_computation.255, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1663 custom-call.105.0{} @0>
 positions:
  custom-call.105.0 {}
 uses:
  get-tuple-element.50.0, operand 0 {}
 from instruction: %custom-call.105.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.314, bf16[6144,1024]{1,0} %p.106), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1664 custom-call.105.0{0} @0>
 positions:
  custom-call.105.0 {0}
  get-tuple-element.50.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.105.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.314, bf16[6144,1024]{1,0} %p.106), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1665 custom-call.105.0{1} @0>
 positions:
  custom-call.105.0 {1}
 uses:
 from instruction: %custom-call.105.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.314, bf16[6144,1024]{1,0} %p.106), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1666 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.106.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.50.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1667 custom-call.106.0{} @0>
 positions:
  custom-call.106.0 {}
 uses:
  get-tuple-element.51.0, operand 0 {}
 from instruction: %custom-call.106.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.107), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1668 custom-call.106.0{0} @0>
 positions:
  custom-call.106.0 {0}
  get-tuple-element.51.0
 uses:
  fusion.313, operand 5
  fusion.311, operand 6
 from instruction: %custom-call.106.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.107), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1669 custom-call.106.0{1} @0>
 positions:
  custom-call.106.0 {1}
 uses:
 from instruction: %custom-call.106.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.107), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1670 fusion.313 @0>
 positions:
  fusion.313
 uses:
  custom-call.107.0, operand 0
 from instruction: %fusion.313 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[1024]{0} %p.108, f32[64,1024]{1,0} %loop_add_fusion.5, bf16[64,1024]{1,0} %get-tuple-element.49.0, bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.51.0), kind=kCustom, calls=%fused_computation.254, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1671 custom-call.107.0{} @0>
 positions:
  custom-call.107.0 {}
 uses:
  get-tuple-element.52.0, operand 0 {}
 from instruction: %custom-call.107.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.313, bf16[6144,1024]{1,0} %p.109), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1672 custom-call.107.0{0} @0>
 positions:
  custom-call.107.0 {0}
  get-tuple-element.52.0
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.107.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.313, bf16[6144,1024]{1,0} %p.109), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1673 custom-call.107.0{1} @0>
 positions:
  custom-call.107.0 {1}
 uses:
 from instruction: %custom-call.107.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.313, bf16[6144,1024]{1,0} %p.109), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1674 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.108.0, operand 0
 from instruction: %loop_convert_fusion = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.52.0), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1675 custom-call.108.0{} @0>
 positions:
  custom-call.108.0 {}
 uses:
  get-tuple-element.53.0, operand 0 {}
 from instruction: %custom-call.108.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.110), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1676 custom-call.108.0{0} @0>
 positions:
  custom-call.108.0 {0}
  get-tuple-element.53.0
 uses:
  fusion.311, operand 1
 from instruction: %custom-call.108.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.110), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1677 custom-call.108.0{1} @0>
 positions:
  custom-call.108.0 {1}
 uses:
 from instruction: %custom-call.108.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.110), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1678 fusion.311 @0>
 positions:
  fusion.311
 uses:
  custom-call.109.0, operand 0
 from instruction: %fusion.311 = bf16[64,1024]{1,0} fusion(f32[] %p.29, bf16[64,1024]{1,0} %get-tuple-element.53.0, bf16[1024]{0} %p.111, f32[64,1024]{1,0} %loop_add_fusion.5, bf16[64,1024]{1,0} %get-tuple-element.49.0, /*index=5*/bf16[64,27648]{1,0} %gemm_fusion_dot.108.0, bf16[64,1024]{1,0} %get-tuple-element.51.0), kind=kCustom, calls=%fused_computation.252, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1679 custom-call.109.0{} @0>
 positions:
  custom-call.109.0 {}
 uses:
  get-tuple-element.54.0, operand 0 {}
 from instruction: %custom-call.109.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.311, bf16[4096,1024]{1,0} %p.112), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1680 custom-call.109.0{0} @0>
 positions:
  custom-call.109.0 {0}
  get-tuple-element.54.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.29.0, operand 1
 from instruction: %custom-call.109.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.311, bf16[4096,1024]{1,0} %p.112), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1681 custom-call.109.0{1} @0>
 positions:
  custom-call.109.0 {1}
 uses:
 from instruction: %custom-call.109.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.311, bf16[4096,1024]{1,0} %p.112), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1682 triton_softmax.29.0 @0>
 positions:
  triton_softmax.29.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.29.0 = f32[64,8,128]{2,1,0} fusion(f32[] %p.29, bf16[64,4096]{1,0} %get-tuple-element.54.0), kind=kCustom, calls=%triton_softmax_computation.29, metadata={op_type="aten__mul" op_name="aten__mul.1505/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1683 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.2142.0 {0}
  call {0}
  get-tuple-element.57
  tuple {0}
 uses:
  tuple, operand 0
  tuple.2142.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[64,8,128]{2,1,0} fusion(f32[64,8,128]{2,1,0} %triton_softmax.29.0, bf16[128]{0} %p.113, bf16[40960,128]{1,0} %p.114, s32[64]{0} %p.115), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1684 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.2142.0 {1}
  call {1}
  get-tuple-element.58
  bitcast.5415.0
  tuple {1}
 uses:
  bitcast.5415.0, operand 0
  tuple.2142.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[64,1024]{1,0} fusion(bf16[64,4096]{1,0} %get-tuple-element.54.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1685 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.2142.0 {3}
  call {2}
  get-tuple-element.59
  bitcast.5427.0
  tuple {2}
 uses:
  bitcast.5427.0, operand 0
  tuple.2142.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.116), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<1686 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.2142.0 {2}
  bitcast.5420.0
  call {3}
  get-tuple-element.60
  tuple {3}
 uses:
  tuple, operand 3
  tuple.2142.0, operand 2
  bitcast.5420.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.116), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<1687 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.57, operand 0 {}
  get-tuple-element.58, operand 0 {}
  get-tuple-element.59, operand 0 {}
  get-tuple-element.60, operand 0 {}
 from instruction: %tuple = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %input_concatenate_fusion, bf16[64,8,128]{2,1,0} %bitcast.5415.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.5427.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<1688 p5.68.0 @0>
 positions:
  p5.68.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.68.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1689 p4.66.0 @0>
 positions:
  p4.66.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.66.0 = s32[64]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1690 p110.1944.0 @0>
 positions:
  p110.1944.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p110.1944.0 = bf16[1024,2048]{1,0} parameter(110), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1691 p106.1872.0 @0>
 positions:
  p106.1872.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p106.1872.0 = bf16[1024,2048]{1,0} parameter(106), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1692 p102.1800.0 @0>
 positions:
  p102.1800.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p102.1800.0 = bf16[1024,2048]{1,0} parameter(102), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1693 p98.1728.0 @0>
 positions:
  p98.1728.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p98.1728.0 = bf16[1024,2048]{1,0} parameter(98), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1694 p94.1656.0 @0>
 positions:
  p94.1656.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p94.1656.0 = bf16[1024,2048]{1,0} parameter(94), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1695 p90.1584.0 @0>
 positions:
  p90.1584.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p90.1584.0 = bf16[1024,2048]{1,0} parameter(90), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1696 p86.1512.0 @0>
 positions:
  p86.1512.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p86.1512.0 = bf16[1024,2048]{1,0} parameter(86), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1697 p82.1440.0 @0>
 positions:
  p82.1440.0
  p.9
 uses:
  call, operand 9
  wrapped_concatenate, operand 7
 from instruction: %p82.1440.0 = bf16[1024,2048]{1,0} parameter(82), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1698 p78.1368.0 @0>
 positions:
  p78.1368.0
  p.10
 uses:
  call, operand 10
  wrapped_concatenate, operand 8
 from instruction: %p78.1368.0 = bf16[1024,2048]{1,0} parameter(78), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1699 p74.1296.0 @0>
 positions:
  p74.1296.0
  p.11
 uses:
  call, operand 11
  wrapped_concatenate, operand 9
 from instruction: %p74.1296.0 = bf16[1024,2048]{1,0} parameter(74), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1700 p70.1224.0 @0>
 positions:
  p70.1224.0
  p.12
 uses:
  call, operand 12
  wrapped_concatenate, operand 10
 from instruction: %p70.1224.0 = bf16[1024,2048]{1,0} parameter(70), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1701 p66.1152.0 @0>
 positions:
  p66.1152.0
  p.13
 uses:
  call, operand 13
  wrapped_concatenate, operand 11
 from instruction: %p66.1152.0 = bf16[1024,2048]{1,0} parameter(66), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1702 p62.1080.0 @0>
 positions:
  p62.1080.0
  p.14
 uses:
  call, operand 14
  wrapped_concatenate, operand 12
 from instruction: %p62.1080.0 = bf16[1024,2048]{1,0} parameter(62), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1703 p58.1008.0 @0>
 positions:
  p58.1008.0
  p.15
 uses:
  call, operand 15
  wrapped_concatenate, operand 13
 from instruction: %p58.1008.0 = bf16[1024,2048]{1,0} parameter(58), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1704 p54.936.0 @0>
 positions:
  p54.936.0
  p.16
 uses:
  call, operand 16
  wrapped_concatenate, operand 14
 from instruction: %p54.936.0 = bf16[1024,2048]{1,0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1705 p50.864.0 @0>
 positions:
  p50.864.0
  p.17
 uses:
  call, operand 17
  wrapped_concatenate, operand 15
 from instruction: %p50.864.0 = bf16[1024,2048]{1,0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1706 p46.792.0 @0>
 positions:
  p46.792.0
  p.18
 uses:
  call, operand 18
  wrapped_concatenate, operand 16
 from instruction: %p46.792.0 = bf16[1024,2048]{1,0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1707 p42.720.0 @0>
 positions:
  p42.720.0
  p.19
 uses:
  call, operand 19
  wrapped_concatenate, operand 17
 from instruction: %p42.720.0 = bf16[1024,2048]{1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1708 p38.648.0 @0>
 positions:
  p38.648.0
  p.20
 uses:
  call, operand 20
  wrapped_concatenate, operand 18
 from instruction: %p38.648.0 = bf16[1024,2048]{1,0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1709 p34.576.0 @0>
 positions:
  p34.576.0
  p.21
 uses:
  call, operand 21
  wrapped_concatenate, operand 19
 from instruction: %p34.576.0 = bf16[1024,2048]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1710 p30.504.0 @0>
 positions:
  p30.504.0
  p.22
 uses:
  call, operand 22
  wrapped_concatenate, operand 20
 from instruction: %p30.504.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1711 p26.432.0 @0>
 positions:
  p26.432.0
  p.23
 uses:
  call, operand 23
  wrapped_concatenate, operand 21
 from instruction: %p26.432.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1712 p22.360.0 @0>
 positions:
  p22.360.0
  p.24
 uses:
  call, operand 24
  wrapped_concatenate, operand 22
 from instruction: %p22.360.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1713 p18.288.0 @0>
 positions:
  p18.288.0
  p.25
 uses:
  call, operand 25
  wrapped_concatenate, operand 23
 from instruction: %p18.288.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1714 p14.216.0 @0>
 positions:
  p14.216.0
  p.26
 uses:
  call, operand 26
  wrapped_concatenate, operand 24
 from instruction: %p14.216.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1715 p10.144.0 @0>
 positions:
  p10.144.0
  p.27
 uses:
  call, operand 27
  wrapped_concatenate, operand 25
 from instruction: %p10.144.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1716 p6.72.0 @0>
 positions:
  p6.72.0
  p.28
 uses:
  call, operand 28
  wrapped_concatenate, operand 26
 from instruction: %p6.72.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1717 p1.4.0 @0>
 positions:
  p1.4.0
  p.29
 uses:
  call, operand 29
  fusion.339, operand 0
  fusion.338, operand 0
  fusion.337, operand 0
  fusion.336, operand 0
  fusion.335, operand 1
  fusion.334, operand 0
  fusion.333, operand 0
  fusion.332, operand 0
  fusion.331, operand 1
  fusion.330, operand 0
  fusion.329, operand 0
  fusion.328, operand 0
  fusion.327, operand 1
  fusion.326, operand 0
  fusion.325, operand 0
  fusion.324, operand 0
  fusion.323, operand 1
  fusion.322, operand 0
  fusion.321, operand 0
  fusion.320, operand 0
  fusion.319, operand 1
  fusion.318, operand 0
  fusion.317, operand 0
  fusion.316, operand 0
  fusion.315, operand 1
  fusion.314, operand 0
  fusion.313, operand 0
  fusion.311, operand 0
  triton_softmax.29.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1718 p9.92.0 @0>
 positions:
  p9.92.0
  p.30
 uses:
  call, operand 30
  fusion.339, operand 3
 from instruction: %p9.92.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1719 p8.90.0 @0>
 positions:
  p8.90.0
  p.31
 uses:
  call, operand 31
  custom-call.55.0, operand 1
 from instruction: %p8.90.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1720 p7.88.0 @0>
 positions:
  p7.88.0
  p.32
 uses:
  call, operand 32
  custom-call.56.0, operand 1
 from instruction: %p7.88.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1721 p13.164.0 @0>
 positions:
  p13.164.0
  p.33
 uses:
  call, operand 33
  fusion.338, operand 4
 from instruction: %p13.164.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1722 p12.162.0 @0>
 positions:
  p12.162.0
  p.34
 uses:
  call, operand 34
  custom-call.57.0, operand 1
 from instruction: %p12.162.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1723 p11.160.0 @0>
 positions:
  p11.160.0
  p.35
 uses:
  call, operand 35
  custom-call.58.0, operand 1
 from instruction: %p11.160.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1724 p17.236.0 @0>
 positions:
  p17.236.0
  p.36
 uses:
  call, operand 36
  fusion.337, operand 1
 from instruction: %p17.236.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1725 p16.234.0 @0>
 positions:
  p16.234.0
  p.37
 uses:
  call, operand 37
  custom-call.59.0, operand 1
 from instruction: %p16.234.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1726 p15.232.0 @0>
 positions:
  p15.232.0
  p.38
 uses:
  call, operand 38
  custom-call.60.0, operand 1
 from instruction: %p15.232.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1727 p21.308.0 @0>
 positions:
  p21.308.0
  p.39
 uses:
  call, operand 39
  fusion.336, operand 3
 from instruction: %p21.308.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1728 p20.306.0 @0>
 positions:
  p20.306.0
  p.40
 uses:
  call, operand 40
  custom-call.61.0, operand 1
 from instruction: %p20.306.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1729 p19.304.0 @0>
 positions:
  p19.304.0
  p.41
 uses:
  call, operand 41
  custom-call.62.0, operand 1
 from instruction: %p19.304.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1730 p25.380.0 @0>
 positions:
  p25.380.0
  p.42
 uses:
  call, operand 42
  fusion.335, operand 2
 from instruction: %p25.380.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1731 p24.378.0 @0>
 positions:
  p24.378.0
  p.43
 uses:
  call, operand 43
  custom-call.63.0, operand 1
 from instruction: %p24.378.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1732 p23.376.0 @0>
 positions:
  p23.376.0
  p.44
 uses:
  call, operand 44
  custom-call.64.0, operand 1
 from instruction: %p23.376.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1733 p29.452.0 @0>
 positions:
  p29.452.0
  p.45
 uses:
  call, operand 45
  fusion.334, operand 4
 from instruction: %p29.452.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1734 p28.450.0 @0>
 positions:
  p28.450.0
  p.46
 uses:
  call, operand 46
  custom-call.65.0, operand 1
 from instruction: %p28.450.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1735 p27.448.0 @0>
 positions:
  p27.448.0
  p.47
 uses:
  call, operand 47
  custom-call.66.0, operand 1
 from instruction: %p27.448.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1736 p33.524.0 @0>
 positions:
  p33.524.0
  p.48
 uses:
  call, operand 48
  fusion.333, operand 1
 from instruction: %p33.524.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1737 p32.522.0 @0>
 positions:
  p32.522.0
  p.49
 uses:
  call, operand 49
  custom-call.67.0, operand 1
 from instruction: %p32.522.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1738 p31.520.0 @0>
 positions:
  p31.520.0
  p.50
 uses:
  call, operand 50
  custom-call.68.0, operand 1
 from instruction: %p31.520.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1739 p37.596.0 @0>
 positions:
  p37.596.0
  p.51
 uses:
  call, operand 51
  fusion.332, operand 3
 from instruction: %p37.596.0 = bf16[1024]{0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1740 p36.594.0 @0>
 positions:
  p36.594.0
  p.52
 uses:
  call, operand 52
  custom-call.69.0, operand 1
 from instruction: %p36.594.0 = bf16[6144,1024]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1741 p35.592.0 @0>
 positions:
  p35.592.0
  p.53
 uses:
  call, operand 53
  custom-call.70.0, operand 1
 from instruction: %p35.592.0 = bf16[1024,3072]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1742 p41.668.0 @0>
 positions:
  p41.668.0
  p.54
 uses:
  call, operand 54
  fusion.331, operand 2
 from instruction: %p41.668.0 = bf16[1024]{0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1743 p40.666.0 @0>
 positions:
  p40.666.0
  p.55
 uses:
  call, operand 55
  custom-call.71.0, operand 1
 from instruction: %p40.666.0 = bf16[6144,1024]{1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1744 p39.664.0 @0>
 positions:
  p39.664.0
  p.56
 uses:
  call, operand 56
  custom-call.72.0, operand 1
 from instruction: %p39.664.0 = bf16[1024,3072]{1,0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1745 p45.740.0 @0>
 positions:
  p45.740.0
  p.57
 uses:
  call, operand 57
  fusion.330, operand 4
 from instruction: %p45.740.0 = bf16[1024]{0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1746 p44.738.0 @0>
 positions:
  p44.738.0
  p.58
 uses:
  call, operand 58
  custom-call.73.0, operand 1
 from instruction: %p44.738.0 = bf16[6144,1024]{1,0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1747 p43.736.0 @0>
 positions:
  p43.736.0
  p.59
 uses:
  call, operand 59
  custom-call.74.0, operand 1
 from instruction: %p43.736.0 = bf16[1024,3072]{1,0} parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1748 p49.812.0 @0>
 positions:
  p49.812.0
  p.60
 uses:
  call, operand 60
  fusion.329, operand 1
 from instruction: %p49.812.0 = bf16[1024]{0} parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1749 p48.810.0 @0>
 positions:
  p48.810.0
  p.61
 uses:
  call, operand 61
  custom-call.75.0, operand 1
 from instruction: %p48.810.0 = bf16[6144,1024]{1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1750 p47.808.0 @0>
 positions:
  p47.808.0
  p.62
 uses:
  call, operand 62
  custom-call.76.0, operand 1
 from instruction: %p47.808.0 = bf16[1024,3072]{1,0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1751 p53.884.0 @0>
 positions:
  p53.884.0
  p.63
 uses:
  call, operand 63
  fusion.328, operand 3
 from instruction: %p53.884.0 = bf16[1024]{0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1752 p52.882.0 @0>
 positions:
  p52.882.0
  p.64
 uses:
  call, operand 64
  custom-call.77.0, operand 1
 from instruction: %p52.882.0 = bf16[6144,1024]{1,0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1753 p51.880.0 @0>
 positions:
  p51.880.0
  p.65
 uses:
  call, operand 65
  custom-call.78.0, operand 1
 from instruction: %p51.880.0 = bf16[1024,3072]{1,0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1754 p57.956.0 @0>
 positions:
  p57.956.0
  p.66
 uses:
  call, operand 66
  fusion.327, operand 2
 from instruction: %p57.956.0 = bf16[1024]{0} parameter(57), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1755 p56.954.0 @0>
 positions:
  p56.954.0
  p.67
 uses:
  call, operand 67
  custom-call.79.0, operand 1
 from instruction: %p56.954.0 = bf16[6144,1024]{1,0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1756 p55.952.0 @0>
 positions:
  p55.952.0
  p.68
 uses:
  call, operand 68
  custom-call.80.0, operand 1
 from instruction: %p55.952.0 = bf16[1024,3072]{1,0} parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1757 p61.1028.0 @0>
 positions:
  p61.1028.0
  p.69
 uses:
  call, operand 69
  fusion.326, operand 4
 from instruction: %p61.1028.0 = bf16[1024]{0} parameter(61), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1758 p60.1026.0 @0>
 positions:
  p60.1026.0
  p.70
 uses:
  call, operand 70
  custom-call.81.0, operand 1
 from instruction: %p60.1026.0 = bf16[6144,1024]{1,0} parameter(60), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1759 p59.1024.0 @0>
 positions:
  p59.1024.0
  p.71
 uses:
  call, operand 71
  custom-call.82.0, operand 1
 from instruction: %p59.1024.0 = bf16[1024,3072]{1,0} parameter(59), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1760 p65.1100.0 @0>
 positions:
  p65.1100.0
  p.72
 uses:
  call, operand 72
  fusion.325, operand 1
 from instruction: %p65.1100.0 = bf16[1024]{0} parameter(65), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1761 p64.1098.0 @0>
 positions:
  p64.1098.0
  p.73
 uses:
  call, operand 73
  custom-call.83.0, operand 1
 from instruction: %p64.1098.0 = bf16[6144,1024]{1,0} parameter(64), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1762 p63.1096.0 @0>
 positions:
  p63.1096.0
  p.74
 uses:
  call, operand 74
  custom-call.84.0, operand 1
 from instruction: %p63.1096.0 = bf16[1024,3072]{1,0} parameter(63), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1763 p69.1172.0 @0>
 positions:
  p69.1172.0
  p.75
 uses:
  call, operand 75
  fusion.324, operand 3
 from instruction: %p69.1172.0 = bf16[1024]{0} parameter(69), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1764 p68.1170.0 @0>
 positions:
  p68.1170.0
  p.76
 uses:
  call, operand 76
  custom-call.85.0, operand 1
 from instruction: %p68.1170.0 = bf16[6144,1024]{1,0} parameter(68), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1765 p67.1168.0 @0>
 positions:
  p67.1168.0
  p.77
 uses:
  call, operand 77
  custom-call.86.0, operand 1
 from instruction: %p67.1168.0 = bf16[1024,3072]{1,0} parameter(67), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1766 p73.1244.0 @0>
 positions:
  p73.1244.0
  p.78
 uses:
  call, operand 78
  fusion.323, operand 2
 from instruction: %p73.1244.0 = bf16[1024]{0} parameter(73), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1767 p72.1242.0 @0>
 positions:
  p72.1242.0
  p.79
 uses:
  call, operand 79
  custom-call.87.0, operand 1
 from instruction: %p72.1242.0 = bf16[6144,1024]{1,0} parameter(72), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1768 p71.1240.0 @0>
 positions:
  p71.1240.0
  p.80
 uses:
  call, operand 80
  custom-call.88.0, operand 1
 from instruction: %p71.1240.0 = bf16[1024,3072]{1,0} parameter(71), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1769 p77.1316.0 @0>
 positions:
  p77.1316.0
  p.81
 uses:
  call, operand 81
  fusion.322, operand 4
 from instruction: %p77.1316.0 = bf16[1024]{0} parameter(77), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1770 p76.1314.0 @0>
 positions:
  p76.1314.0
  p.82
 uses:
  call, operand 82
  custom-call.89.0, operand 1
 from instruction: %p76.1314.0 = bf16[6144,1024]{1,0} parameter(76), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1771 p75.1312.0 @0>
 positions:
  p75.1312.0
  p.83
 uses:
  call, operand 83
  custom-call.90.0, operand 1
 from instruction: %p75.1312.0 = bf16[1024,3072]{1,0} parameter(75), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1772 p81.1388.0 @0>
 positions:
  p81.1388.0
  p.84
 uses:
  call, operand 84
  fusion.321, operand 1
 from instruction: %p81.1388.0 = bf16[1024]{0} parameter(81), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1773 p80.1386.0 @0>
 positions:
  p80.1386.0
  p.85
 uses:
  call, operand 85
  custom-call.91.0, operand 1
 from instruction: %p80.1386.0 = bf16[6144,1024]{1,0} parameter(80), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1774 p79.1384.0 @0>
 positions:
  p79.1384.0
  p.86
 uses:
  call, operand 86
  custom-call.92.0, operand 1
 from instruction: %p79.1384.0 = bf16[1024,3072]{1,0} parameter(79), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1775 p85.1460.0 @0>
 positions:
  p85.1460.0
  p.87
 uses:
  call, operand 87
  fusion.320, operand 3
 from instruction: %p85.1460.0 = bf16[1024]{0} parameter(85), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1776 p84.1458.0 @0>
 positions:
  p84.1458.0
  p.88
 uses:
  call, operand 88
  custom-call.93.0, operand 1
 from instruction: %p84.1458.0 = bf16[6144,1024]{1,0} parameter(84), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1777 p83.1456.0 @0>
 positions:
  p83.1456.0
  p.89
 uses:
  call, operand 89
  custom-call.94.0, operand 1
 from instruction: %p83.1456.0 = bf16[1024,3072]{1,0} parameter(83), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1778 p89.1532.0 @0>
 positions:
  p89.1532.0
  p.90
 uses:
  call, operand 90
  fusion.319, operand 2
 from instruction: %p89.1532.0 = bf16[1024]{0} parameter(89), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1779 p88.1530.0 @0>
 positions:
  p88.1530.0
  p.91
 uses:
  call, operand 91
  custom-call.95.0, operand 1
 from instruction: %p88.1530.0 = bf16[6144,1024]{1,0} parameter(88), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1780 p87.1528.0 @0>
 positions:
  p87.1528.0
  p.92
 uses:
  call, operand 92
  custom-call.96.0, operand 1
 from instruction: %p87.1528.0 = bf16[1024,3072]{1,0} parameter(87), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1781 p93.1604.0 @0>
 positions:
  p93.1604.0
  p.93
 uses:
  call, operand 93
  fusion.318, operand 4
 from instruction: %p93.1604.0 = bf16[1024]{0} parameter(93), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1782 p92.1602.0 @0>
 positions:
  p92.1602.0
  p.94
 uses:
  call, operand 94
  custom-call.97.0, operand 1
 from instruction: %p92.1602.0 = bf16[6144,1024]{1,0} parameter(92), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1783 p91.1600.0 @0>
 positions:
  p91.1600.0
  p.95
 uses:
  call, operand 95
  custom-call.98.0, operand 1
 from instruction: %p91.1600.0 = bf16[1024,3072]{1,0} parameter(91), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1784 p97.1676.0 @0>
 positions:
  p97.1676.0
  p.96
 uses:
  call, operand 96
  fusion.317, operand 1
 from instruction: %p97.1676.0 = bf16[1024]{0} parameter(97), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1785 p96.1674.0 @0>
 positions:
  p96.1674.0
  p.97
 uses:
  call, operand 97
  custom-call.99.0, operand 1
 from instruction: %p96.1674.0 = bf16[6144,1024]{1,0} parameter(96), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1786 p95.1672.0 @0>
 positions:
  p95.1672.0
  p.98
 uses:
  call, operand 98
  custom-call.100.0, operand 1
 from instruction: %p95.1672.0 = bf16[1024,3072]{1,0} parameter(95), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1787 p101.1748.0 @0>
 positions:
  p101.1748.0
  p.99
 uses:
  call, operand 99
  fusion.316, operand 3
 from instruction: %p101.1748.0 = bf16[1024]{0} parameter(101), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1788 p100.1746.0 @0>
 positions:
  p100.1746.0
  p.100
 uses:
  call, operand 100
  custom-call.101.0, operand 1
 from instruction: %p100.1746.0 = bf16[6144,1024]{1,0} parameter(100), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1789 p99.1744.0 @0>
 positions:
  p99.1744.0
  p.101
 uses:
  call, operand 101
  custom-call.102.0, operand 1
 from instruction: %p99.1744.0 = bf16[1024,3072]{1,0} parameter(99), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1790 p105.1820.0 @0>
 positions:
  p105.1820.0
  p.102
 uses:
  call, operand 102
  fusion.315, operand 2
 from instruction: %p105.1820.0 = bf16[1024]{0} parameter(105), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1791 p104.1818.0 @0>
 positions:
  p104.1818.0
  p.103
 uses:
  call, operand 103
  custom-call.103.0, operand 1
 from instruction: %p104.1818.0 = bf16[6144,1024]{1,0} parameter(104), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1792 p103.1816.0 @0>
 positions:
  p103.1816.0
  p.104
 uses:
  call, operand 104
  custom-call.104.0, operand 1
 from instruction: %p103.1816.0 = bf16[1024,3072]{1,0} parameter(103), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1793 p109.1892.0 @0>
 positions:
  p109.1892.0
  p.105
 uses:
  call, operand 105
  fusion.314, operand 4
 from instruction: %p109.1892.0 = bf16[1024]{0} parameter(109), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1794 p108.1890.0 @0>
 positions:
  p108.1890.0
  p.106
 uses:
  call, operand 106
  custom-call.105.0, operand 1
 from instruction: %p108.1890.0 = bf16[6144,1024]{1,0} parameter(108), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1795 p107.1888.0 @0>
 positions:
  p107.1888.0
  p.107
 uses:
  call, operand 107
  custom-call.106.0, operand 1
 from instruction: %p107.1888.0 = bf16[1024,3072]{1,0} parameter(107), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1796 p113.1964.0 @0>
 positions:
  p113.1964.0
  p.108
 uses:
  call, operand 108
  fusion.313, operand 1
 from instruction: %p113.1964.0 = bf16[1024]{0} parameter(113), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1797 p112.1962.0 @0>
 positions:
  p112.1962.0
  p.109
 uses:
  call, operand 109
  custom-call.107.0, operand 1
 from instruction: %p112.1962.0 = bf16[6144,1024]{1,0} parameter(112), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1798 p111.1960.0 @0>
 positions:
  p111.1960.0
  p.110
 uses:
  call, operand 110
  custom-call.108.0, operand 1
 from instruction: %p111.1960.0 = bf16[1024,3072]{1,0} parameter(111), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1799 p3.8.0 @0>
 positions:
  p3.8.0
  p.111
 uses:
  call, operand 111
  fusion.311, operand 2
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1800 p2.6.0 @0>
 positions:
  p2.6.0
  p.112
 uses:
  call, operand 112
  custom-call.109.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1801 p0.1.0 @0>
 positions:
  p0.1.0
  p.113
 uses:
  call, operand 113
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1802 p115.2089.0 @0>
 positions:
  p115.2089.0
  p.114
 uses:
  call, operand 114
  input_concatenate_fusion, operand 2
 from instruction: %p115.2089.0 = bf16[40960,128]{1,0} parameter(115), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1803 p114.2088.0 @0>
 positions:
  p114.2088.0
  p.115
 uses:
  call, operand 115
  input_concatenate_fusion, operand 3
 from instruction: %p114.2088.0 = s32[64]{0} parameter(114), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1804 p116.2133.0 @0>
 positions:
  p116.2133.0
  p.116
 uses:
  call, operand 116
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p116.2133.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(116), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1805 tuple.2142.0{} @0>
 positions:
  tuple.2142.0 {}
 uses:
 from instruction: %tuple.2142.0 = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %get-tuple-element.57, bf16[64,8,128]{2,1,0} %get-tuple-element.58, bf16[4233,16,8,128]{3,2,1,0} %bitcast.5420.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.59)


HloLiveRange (max 423):
  InstructionSequence:
    0:p1.4.0
    1:p116.2133.0
    2:p115.2089.0
    3:p114.2088.0
    4:p113.1964.0
    5:p112.1962.0
    6:p111.1960.0
    7:p110.1944.0
    8:p109.1892.0
    9:p108.1890.0
    10:p107.1888.0
    11:p106.1872.0
    12:p105.1820.0
    13:p104.1818.0
    14:p103.1816.0
    15:p102.1800.0
    16:p101.1748.0
    17:p100.1746.0
    18:p99.1744.0
    19:p98.1728.0
    20:p97.1676.0
    21:p96.1674.0
    22:p95.1672.0
    23:p94.1656.0
    24:p93.1604.0
    25:p92.1602.0
    26:p91.1600.0
    27:p90.1584.0
    28:p89.1532.0
    29:p88.1530.0
    30:p87.1528.0
    31:p86.1512.0
    32:p85.1460.0
    33:p84.1458.0
    34:p83.1456.0
    35:p82.1440.0
    36:p81.1388.0
    37:p80.1386.0
    38:p79.1384.0
    39:p78.1368.0
    40:p77.1316.0
    41:p76.1314.0
    42:p75.1312.0
    43:p74.1296.0
    44:p73.1244.0
    45:p72.1242.0
    46:p71.1240.0
    47:p70.1224.0
    48:p69.1172.0
    49:p68.1170.0
    50:p67.1168.0
    51:p66.1152.0
    52:p65.1100.0
    53:p64.1098.0
    54:p63.1096.0
    55:p62.1080.0
    56:p61.1028.0
    57:p60.1026.0
    58:p59.1024.0
    59:p58.1008.0
    60:p57.956.0
    61:p56.954.0
    62:p55.952.0
    63:p54.936.0
    64:p53.884.0
    65:p52.882.0
    66:p51.880.0
    67:p50.864.0
    68:p49.812.0
    69:p48.810.0
    70:p47.808.0
    71:p46.792.0
    72:p45.740.0
    73:p44.738.0
    74:p43.736.0
    75:p42.720.0
    76:p41.668.0
    77:p40.666.0
    78:p39.664.0
    79:p38.648.0
    80:p37.596.0
    81:p36.594.0
    82:p35.592.0
    83:p34.576.0
    84:p33.524.0
    85:p32.522.0
    86:p31.520.0
    87:p30.504.0
    88:p29.452.0
    89:p28.450.0
    90:p27.448.0
    91:p26.432.0
    92:p25.380.0
    93:p24.378.0
    94:p23.376.0
    95:p22.360.0
    96:p21.308.0
    97:p20.306.0
    98:p19.304.0
    99:p18.288.0
    100:p17.236.0
    101:p16.234.0
    102:p15.232.0
    103:p14.216.0
    104:p13.164.0
    105:p12.162.0
    106:p11.160.0
    107:p10.144.0
    108:p9.92.0
    109:p8.90.0
    110:p7.88.0
    111:p6.72.0
    112:p5.68.0
    113:p4.66.0
    114:p3.8.0
    115:p2.6.0
    116:p0.1.0
    117:p
    118:p.1
    119:p.2
    120:p.3
    121:p.4
    122:p.5
    123:p.6
    124:p.7
    125:p.8
    126:p.9
    127:p.10
    128:p.11
    129:p.12
    130:p.13
    131:p.14
    132:p.15
    133:p.16
    134:p.17
    135:p.18
    136:p.19
    137:p.20
    138:p.21
    139:p.22
    140:p.23
    141:p.24
    142:p.25
    143:p.26
    144:p.27
    145:p.28
    146:p.29
    147:p.30
    148:p.31
    149:p.32
    150:p.33
    151:p.34
    152:p.35
    153:p.36
    154:p.37
    155:p.38
    156:p.39
    157:p.40
    158:p.41
    159:p.42
    160:p.43
    161:p.44
    162:p.45
    163:p.46
    164:p.47
    165:p.48
    166:p.49
    167:p.50
    168:p.51
    169:p.52
    170:p.53
    171:p.54
    172:p.55
    173:p.56
    174:p.57
    175:p.58
    176:p.59
    177:p.60
    178:p.61
    179:p.62
    180:p.63
    181:p.64
    182:p.65
    183:p.66
    184:p.67
    185:p.68
    186:p.69
    187:p.70
    188:p.71
    189:p.72
    190:p.73
    191:p.74
    192:p.75
    193:p.76
    194:p.77
    195:p.78
    196:p.79
    197:p.80
    198:p.81
    199:p.82
    200:p.83
    201:p.84
    202:p.85
    203:p.86
    204:p.87
    205:p.88
    206:p.89
    207:p.90
    208:p.91
    209:p.92
    210:p.93
    211:p.94
    212:p.95
    213:p.96
    214:p.97
    215:p.98
    216:p.99
    217:p.100
    218:p.101
    219:p.102
    220:p.103
    221:p.104
    222:p.105
    223:p.106
    224:p.107
    225:p.108
    226:p.109
    227:p.110
    228:p.111
    229:p.112
    230:p.113
    231:p.114
    232:p.115
    233:p.116
    234:loop_gather_fusion
    235:wrapped_concatenate
    236:gemm_fusion_dot.108.0
    237:fusion.339
    238:custom-call.55.0
    239:get-tuple-element.55
    240:loop_convert_fusion.26
    241:custom-call.56.0
    242:get-tuple-element.1.0
    243:fusion.338
    244:custom-call.57.0
    245:get-tuple-element.2.0
    246:loop_convert_fusion.25
    247:custom-call.58.0
    248:get-tuple-element.3.0
    249:fusion.337
    250:custom-call.59.0
    251:get-tuple-element.4.0
    252:loop_convert_fusion.24
    253:custom-call.60.0
    254:get-tuple-element.5.0
    255:fusion.336
    256:custom-call.61.0
    257:get-tuple-element.6.0
    258:loop_convert_fusion.23
    259:custom-call.62.0
    260:get-tuple-element.7.0
    261:loop_add_fusion
    262:fusion.335
    263:custom-call.63.0
    264:get-tuple-element.8.0
    265:loop_convert_fusion.22
    266:custom-call.64.0
    267:get-tuple-element.9.0
    268:fusion.334
    269:custom-call.65.0
    270:get-tuple-element.10.0
    271:loop_convert_fusion.21
    272:custom-call.66.0
    273:get-tuple-element.11.0
    274:fusion.333
    275:custom-call.67.0
    276:get-tuple-element.12.0
    277:loop_convert_fusion.20
    278:custom-call.68.0
    279:get-tuple-element.13.0
    280:fusion.332
    281:custom-call.69.0
    282:get-tuple-element.14.0
    283:loop_convert_fusion.19
    284:custom-call.70.0
    285:get-tuple-element.15.0
    286:loop_add_fusion.1
    287:fusion.331
    288:custom-call.71.0
    289:get-tuple-element.16.0
    290:loop_convert_fusion.18
    291:custom-call.72.0
    292:get-tuple-element.17.0
    293:fusion.330
    294:custom-call.73.0
    295:get-tuple-element.18.0
    296:loop_convert_fusion.17
    297:custom-call.74.0
    298:get-tuple-element.19.0
    299:fusion.329
    300:custom-call.75.0
    301:get-tuple-element.20.0
    302:loop_convert_fusion.16
    303:custom-call.76.0
    304:get-tuple-element.21.0
    305:fusion.328
    306:custom-call.77.0
    307:get-tuple-element.22.0
    308:loop_convert_fusion.15
    309:custom-call.78.0
    310:get-tuple-element.23.0
    311:loop_add_fusion.2
    312:fusion.327
    313:custom-call.79.0
    314:get-tuple-element.24.0
    315:loop_convert_fusion.14
    316:custom-call.80.0
    317:get-tuple-element.25.0
    318:fusion.326
    319:custom-call.81.0
    320:get-tuple-element.26.0
    321:loop_convert_fusion.13
    322:custom-call.82.0
    323:get-tuple-element.27.0
    324:fusion.325
    325:custom-call.83.0
    326:get-tuple-element.28.0
    327:loop_convert_fusion.12
    328:custom-call.84.0
    329:get-tuple-element.29.0
    330:fusion.324
    331:custom-call.85.0
    332:get-tuple-element.30.0
    333:loop_convert_fusion.11
    334:custom-call.86.0
    335:get-tuple-element.31.0
    336:loop_add_fusion.3
    337:fusion.323
    338:custom-call.87.0
    339:get-tuple-element.32.0
    340:loop_convert_fusion.10
    341:custom-call.88.0
    342:get-tuple-element.33.0
    343:fusion.322
    344:custom-call.89.0
    345:get-tuple-element.34.0
    346:loop_convert_fusion.9
    347:custom-call.90.0
    348:get-tuple-element.35.0
    349:fusion.321
    350:custom-call.91.0
    351:get-tuple-element.36.0
    352:loop_convert_fusion.8
    353:custom-call.92.0
    354:get-tuple-element.37.0
    355:fusion.320
    356:custom-call.93.0
    357:get-tuple-element.38.0
    358:loop_convert_fusion.7
    359:custom-call.94.0
    360:get-tuple-element.39.0
    361:loop_add_fusion.4
    362:fusion.319
    363:custom-call.95.0
    364:get-tuple-element.40.0
    365:loop_convert_fusion.6
    366:custom-call.96.0
    367:get-tuple-element.41.0
    368:fusion.318
    369:custom-call.97.0
    370:get-tuple-element.42.0
    371:loop_convert_fusion.5
    372:custom-call.98.0
    373:get-tuple-element.43.0
    374:fusion.317
    375:custom-call.99.0
    376:get-tuple-element.44.0
    377:loop_convert_fusion.4
    378:custom-call.100.0
    379:get-tuple-element.45.0
    380:fusion.316
    381:custom-call.101.0
    382:get-tuple-element.46.0
    383:loop_convert_fusion.3
    384:custom-call.102.0
    385:get-tuple-element.47.0
    386:loop_add_fusion.5
    387:fusion.315
    388:custom-call.103.0
    389:get-tuple-element.48.0
    390:loop_convert_fusion.2
    391:custom-call.104.0
    392:get-tuple-element.49.0
    393:fusion.314
    394:custom-call.105.0
    395:get-tuple-element.50.0
    396:loop_convert_fusion.1
    397:custom-call.106.0
    398:get-tuple-element.51.0
    399:fusion.313
    400:custom-call.107.0
    401:get-tuple-element.52.0
    402:loop_convert_fusion
    403:custom-call.108.0
    404:get-tuple-element.53.0
    405:fusion.311
    406:custom-call.109.0
    407:get-tuple-element.54.0
    408:wrapped_slice
    409:triton_softmax.29.0
    410:input_concatenate_fusion
    411:bitcast.5415.0
    412:loop_slice_fusion
    413:bitcast.5427.0
    414:wrapped_slice.1
    415:tuple
    416:call
    417:get-tuple-element.57
    418:get-tuple-element.58
    419:get-tuple-element.59
    420:get-tuple-element.60
    421:bitcast.5420.0
    422:tuple.2142.0
  BufferLiveRange:
    wrapped_concatenate{}:235-236
    gemm_fusion_dot.108.0{}:236-405
    loop_gather_fusion{}:234-261
    fusion.339{}:237-238
    custom-call.55.0{}:238-239
    custom-call.55.0{0}:238-240
    custom-call.55.0{1}:238-238
    loop_convert_fusion.26{}:240-241
    custom-call.56.0{}:241-242
    custom-call.56.0{0}:241-261
    custom-call.56.0{1}:241-241
    fusion.338{}:243-244
    custom-call.57.0{}:244-245
    custom-call.57.0{0}:244-246
    custom-call.57.0{1}:244-244
    loop_convert_fusion.25{}:246-247
    custom-call.58.0{}:247-248
    custom-call.58.0{0}:247-261
    custom-call.58.0{1}:247-247
    fusion.337{}:249-250
    custom-call.59.0{}:250-251
    custom-call.59.0{0}:250-252
    custom-call.59.0{1}:250-250
    loop_convert_fusion.24{}:252-253
    custom-call.60.0{}:253-254
    custom-call.60.0{0}:253-261
    custom-call.60.0{1}:253-253
    fusion.336{}:255-256
    custom-call.61.0{}:256-257
    custom-call.61.0{0}:256-258
    custom-call.61.0{1}:256-256
    loop_convert_fusion.23{}:258-259
    custom-call.62.0{}:259-260
    custom-call.62.0{0}:259-261
    custom-call.62.0{1}:259-259
    loop_add_fusion{}:261-286
    fusion.335{}:262-263
    custom-call.63.0{}:263-264
    custom-call.63.0{0}:263-265
    custom-call.63.0{1}:263-263
    loop_convert_fusion.22{}:265-266
    custom-call.64.0{}:266-267
    custom-call.64.0{0}:266-286
    custom-call.64.0{1}:266-266
    fusion.334{}:268-269
    custom-call.65.0{}:269-270
    custom-call.65.0{0}:269-271
    custom-call.65.0{1}:269-269
    loop_convert_fusion.21{}:271-272
    custom-call.66.0{}:272-273
    custom-call.66.0{0}:272-286
    custom-call.66.0{1}:272-272
    fusion.333{}:274-275
    custom-call.67.0{}:275-276
    custom-call.67.0{0}:275-277
    custom-call.67.0{1}:275-275
    loop_convert_fusion.20{}:277-278
    custom-call.68.0{}:278-279
    custom-call.68.0{0}:278-286
    custom-call.68.0{1}:278-278
    fusion.332{}:280-281
    custom-call.69.0{}:281-282
    custom-call.69.0{0}:281-283
    custom-call.69.0{1}:281-281
    loop_convert_fusion.19{}:283-284
    custom-call.70.0{}:284-285
    custom-call.70.0{0}:284-286
    custom-call.70.0{1}:284-284
    loop_add_fusion.1{}:286-311
    fusion.331{}:287-288
    custom-call.71.0{}:288-289
    custom-call.71.0{0}:288-290
    custom-call.71.0{1}:288-288
    loop_convert_fusion.18{}:290-291
    custom-call.72.0{}:291-292
    custom-call.72.0{0}:291-311
    custom-call.72.0{1}:291-291
    fusion.330{}:293-294
    custom-call.73.0{}:294-295
    custom-call.73.0{0}:294-296
    custom-call.73.0{1}:294-294
    loop_convert_fusion.17{}:296-297
    custom-call.74.0{}:297-298
    custom-call.74.0{0}:297-311
    custom-call.74.0{1}:297-297
    fusion.329{}:299-300
    custom-call.75.0{}:300-301
    custom-call.75.0{0}:300-302
    custom-call.75.0{1}:300-300
    loop_convert_fusion.16{}:302-303
    custom-call.76.0{}:303-304
    custom-call.76.0{0}:303-311
    custom-call.76.0{1}:303-303
    fusion.328{}:305-306
    custom-call.77.0{}:306-307
    custom-call.77.0{0}:306-308
    custom-call.77.0{1}:306-306
    loop_convert_fusion.15{}:308-309
    custom-call.78.0{}:309-310
    custom-call.78.0{0}:309-311
    custom-call.78.0{1}:309-309
    loop_add_fusion.2{}:311-336
    fusion.327{}:312-313
    custom-call.79.0{}:313-314
    custom-call.79.0{0}:313-315
    custom-call.79.0{1}:313-313
    loop_convert_fusion.14{}:315-316
    custom-call.80.0{}:316-317
    custom-call.80.0{0}:316-336
    custom-call.80.0{1}:316-316
    fusion.326{}:318-319
    custom-call.81.0{}:319-320
    custom-call.81.0{0}:319-321
    custom-call.81.0{1}:319-319
    loop_convert_fusion.13{}:321-322
    custom-call.82.0{}:322-323
    custom-call.82.0{0}:322-336
    custom-call.82.0{1}:322-322
    fusion.325{}:324-325
    custom-call.83.0{}:325-326
    custom-call.83.0{0}:325-327
    custom-call.83.0{1}:325-325
    loop_convert_fusion.12{}:327-328
    custom-call.84.0{}:328-329
    custom-call.84.0{0}:328-336
    custom-call.84.0{1}:328-328
    fusion.324{}:330-331
    custom-call.85.0{}:331-332
    custom-call.85.0{0}:331-333
    custom-call.85.0{1}:331-331
    loop_convert_fusion.11{}:333-334
    custom-call.86.0{}:334-335
    custom-call.86.0{0}:334-336
    custom-call.86.0{1}:334-334
    loop_add_fusion.3{}:336-361
    fusion.323{}:337-338
    custom-call.87.0{}:338-339
    custom-call.87.0{0}:338-340
    custom-call.87.0{1}:338-338
    loop_convert_fusion.10{}:340-341
    custom-call.88.0{}:341-342
    custom-call.88.0{0}:341-361
    custom-call.88.0{1}:341-341
    fusion.322{}:343-344
    custom-call.89.0{}:344-345
    custom-call.89.0{0}:344-346
    custom-call.89.0{1}:344-344
    loop_convert_fusion.9{}:346-347
    custom-call.90.0{}:347-348
    custom-call.90.0{0}:347-361
    custom-call.90.0{1}:347-347
    fusion.321{}:349-350
    custom-call.91.0{}:350-351
    custom-call.91.0{0}:350-352
    custom-call.91.0{1}:350-350
    loop_convert_fusion.8{}:352-353
    custom-call.92.0{}:353-354
    custom-call.92.0{0}:353-361
    custom-call.92.0{1}:353-353
    fusion.320{}:355-356
    custom-call.93.0{}:356-357
    custom-call.93.0{0}:356-358
    custom-call.93.0{1}:356-356
    loop_convert_fusion.7{}:358-359
    custom-call.94.0{}:359-360
    custom-call.94.0{0}:359-361
    custom-call.94.0{1}:359-359
    loop_add_fusion.4{}:361-386
    fusion.319{}:362-363
    custom-call.95.0{}:363-364
    custom-call.95.0{0}:363-365
    custom-call.95.0{1}:363-363
    loop_convert_fusion.6{}:365-366
    custom-call.96.0{}:366-367
    custom-call.96.0{0}:366-386
    custom-call.96.0{1}:366-366
    fusion.318{}:368-369
    custom-call.97.0{}:369-370
    custom-call.97.0{0}:369-371
    custom-call.97.0{1}:369-369
    loop_convert_fusion.5{}:371-372
    custom-call.98.0{}:372-373
    custom-call.98.0{0}:372-386
    custom-call.98.0{1}:372-372
    fusion.317{}:374-375
    custom-call.99.0{}:375-376
    custom-call.99.0{0}:375-377
    custom-call.99.0{1}:375-375
    loop_convert_fusion.4{}:377-378
    custom-call.100.0{}:378-379
    custom-call.100.0{0}:378-386
    custom-call.100.0{1}:378-378
    fusion.316{}:380-381
    custom-call.101.0{}:381-382
    custom-call.101.0{0}:381-383
    custom-call.101.0{1}:381-381
    loop_convert_fusion.3{}:383-384
    custom-call.102.0{}:384-385
    custom-call.102.0{0}:384-386
    custom-call.102.0{1}:384-384
    loop_add_fusion.5{}:386-405
    fusion.315{}:387-388
    custom-call.103.0{}:388-389
    custom-call.103.0{0}:388-390
    custom-call.103.0{1}:388-388
    loop_convert_fusion.2{}:390-391
    custom-call.104.0{}:391-392
    custom-call.104.0{0}:391-405
    custom-call.104.0{1}:391-391
    fusion.314{}:393-394
    custom-call.105.0{}:394-395
    custom-call.105.0{0}:394-396
    custom-call.105.0{1}:394-394
    loop_convert_fusion.1{}:396-397
    custom-call.106.0{}:397-398
    custom-call.106.0{0}:397-405
    custom-call.106.0{1}:397-397
    fusion.313{}:399-400
    custom-call.107.0{}:400-401
    custom-call.107.0{0}:400-402
    custom-call.107.0{1}:400-400
    loop_convert_fusion{}:402-403
    custom-call.108.0{}:403-404
    custom-call.108.0{0}:403-405
    custom-call.108.0{1}:403-403
    fusion.311{}:405-406
    custom-call.109.0{}:406-407
    custom-call.109.0{0}:406-409
    custom-call.109.0{1}:406-406
    triton_softmax.29.0{}:409-410
    input_concatenate_fusion{}:410-423
    wrapped_slice{}:408-423
    loop_slice_fusion{}:412-423
    wrapped_slice.1{}:414-423
    tuple{}:415-420
    p5.68.0{}:0-423
    p4.66.0{}:0-423
    p110.1944.0{}:0-423
    p106.1872.0{}:0-423
    p102.1800.0{}:0-423
    p98.1728.0{}:0-423
    p94.1656.0{}:0-423
    p90.1584.0{}:0-423
    p86.1512.0{}:0-423
    p82.1440.0{}:0-423
    p78.1368.0{}:0-423
    p74.1296.0{}:0-423
    p70.1224.0{}:0-423
    p66.1152.0{}:0-423
    p62.1080.0{}:0-423
    p58.1008.0{}:0-423
    p54.936.0{}:0-423
    p50.864.0{}:0-423
    p46.792.0{}:0-423
    p42.720.0{}:0-423
    p38.648.0{}:0-423
    p34.576.0{}:0-423
    p30.504.0{}:0-423
    p26.432.0{}:0-423
    p22.360.0{}:0-423
    p18.288.0{}:0-423
    p14.216.0{}:0-423
    p10.144.0{}:0-423
    p6.72.0{}:0-423
    p1.4.0{}:0-423
    p9.92.0{}:0-423
    p8.90.0{}:0-423
    p7.88.0{}:0-423
    p13.164.0{}:0-423
    p12.162.0{}:0-423
    p11.160.0{}:0-423
    p17.236.0{}:0-423
    p16.234.0{}:0-423
    p15.232.0{}:0-423
    p21.308.0{}:0-423
    p20.306.0{}:0-423
    p19.304.0{}:0-423
    p25.380.0{}:0-423
    p24.378.0{}:0-423
    p23.376.0{}:0-423
    p29.452.0{}:0-423
    p28.450.0{}:0-423
    p27.448.0{}:0-423
    p33.524.0{}:0-423
    p32.522.0{}:0-423
    p31.520.0{}:0-423
    p37.596.0{}:0-423
    p36.594.0{}:0-423
    p35.592.0{}:0-423
    p41.668.0{}:0-423
    p40.666.0{}:0-423
    p39.664.0{}:0-423
    p45.740.0{}:0-423
    p44.738.0{}:0-423
    p43.736.0{}:0-423
    p49.812.0{}:0-423
    p48.810.0{}:0-423
    p47.808.0{}:0-423
    p53.884.0{}:0-423
    p52.882.0{}:0-423
    p51.880.0{}:0-423
    p57.956.0{}:0-423
    p56.954.0{}:0-423
    p55.952.0{}:0-423
    p61.1028.0{}:0-423
    p60.1026.0{}:0-423
    p59.1024.0{}:0-423
    p65.1100.0{}:0-423
    p64.1098.0{}:0-423
    p63.1096.0{}:0-423
    p69.1172.0{}:0-423
    p68.1170.0{}:0-423
    p67.1168.0{}:0-423
    p73.1244.0{}:0-423
    p72.1242.0{}:0-423
    p71.1240.0{}:0-423
    p77.1316.0{}:0-423
    p76.1314.0{}:0-423
    p75.1312.0{}:0-423
    p81.1388.0{}:0-423
    p80.1386.0{}:0-423
    p79.1384.0{}:0-423
    p85.1460.0{}:0-423
    p84.1458.0{}:0-423
    p83.1456.0{}:0-423
    p89.1532.0{}:0-423
    p88.1530.0{}:0-423
    p87.1528.0{}:0-423
    p93.1604.0{}:0-423
    p92.1602.0{}:0-423
    p91.1600.0{}:0-423
    p97.1676.0{}:0-423
    p96.1674.0{}:0-423
    p95.1672.0{}:0-423
    p101.1748.0{}:0-423
    p100.1746.0{}:0-423
    p99.1744.0{}:0-423
    p105.1820.0{}:0-423
    p104.1818.0{}:0-423
    p103.1816.0{}:0-423
    p109.1892.0{}:0-423
    p108.1890.0{}:0-423
    p107.1888.0{}:0-423
    p113.1964.0{}:0-423
    p112.1962.0{}:0-423
    p111.1960.0{}:0-423
    p3.8.0{}:0-423
    p2.6.0{}:0-423
    p0.1.0{}:0-423
    p115.2089.0{}:0-423
    p114.2088.0{}:0-423
    p116.2133.0{}:0-423
    tuple.2142.0{}:422-423
  Live ranges at 415 (peak):
    input_concatenate_fusion: 131072 bytes
    wrapped_slice: 131072 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.68.0: 311164928 bytes
    p4.66.0: 256 bytes
    p110.1944.0: 4194304 bytes
    p106.1872.0: 4194304 bytes
    p102.1800.0: 4194304 bytes
    p98.1728.0: 4194304 bytes
    p94.1656.0: 4194304 bytes
    p90.1584.0: 4194304 bytes
    p86.1512.0: 4194304 bytes
    p82.1440.0: 4194304 bytes
    p78.1368.0: 4194304 bytes
    p74.1296.0: 4194304 bytes
    p70.1224.0: 4194304 bytes
    p66.1152.0: 4194304 bytes
    p62.1080.0: 4194304 bytes
    p58.1008.0: 4194304 bytes
    p54.936.0: 4194304 bytes
    p50.864.0: 4194304 bytes
    p46.792.0: 4194304 bytes
    p42.720.0: 4194304 bytes
    p38.648.0: 4194304 bytes
    p34.576.0: 4194304 bytes
    p30.504.0: 4194304 bytes
    p26.432.0: 4194304 bytes
    p22.360.0: 4194304 bytes
    p18.288.0: 4194304 bytes
    p14.216.0: 4194304 bytes
    p10.144.0: 4194304 bytes
    p6.72.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.92.0: 2048 bytes
    p8.90.0: 12582912 bytes
    p7.88.0: 6291456 bytes
    p13.164.0: 2048 bytes
    p12.162.0: 12582912 bytes
    p11.160.0: 6291456 bytes
    p17.236.0: 2048 bytes
    p16.234.0: 12582912 bytes
    p15.232.0: 6291456 bytes
    p21.308.0: 2048 bytes
    p20.306.0: 12582912 bytes
    p19.304.0: 6291456 bytes
    p25.380.0: 2048 bytes
    p24.378.0: 12582912 bytes
    p23.376.0: 6291456 bytes
    p29.452.0: 2048 bytes
    p28.450.0: 12582912 bytes
    p27.448.0: 6291456 bytes
    p33.524.0: 2048 bytes
    p32.522.0: 12582912 bytes
    p31.520.0: 6291456 bytes
    p37.596.0: 2048 bytes
    p36.594.0: 12582912 bytes
    p35.592.0: 6291456 bytes
    p41.668.0: 2048 bytes
    p40.666.0: 12582912 bytes
    p39.664.0: 6291456 bytes
    p45.740.0: 2048 bytes
    p44.738.0: 12582912 bytes
    p43.736.0: 6291456 bytes
    p49.812.0: 2048 bytes
    p48.810.0: 12582912 bytes
    p47.808.0: 6291456 bytes
    p53.884.0: 2048 bytes
    p52.882.0: 12582912 bytes
    p51.880.0: 6291456 bytes
    p57.956.0: 2048 bytes
    p56.954.0: 12582912 bytes
    p55.952.0: 6291456 bytes
    p61.1028.0: 2048 bytes
    p60.1026.0: 12582912 bytes
    p59.1024.0: 6291456 bytes
    p65.1100.0: 2048 bytes
    p64.1098.0: 12582912 bytes
    p63.1096.0: 6291456 bytes
    p69.1172.0: 2048 bytes
    p68.1170.0: 12582912 bytes
    p67.1168.0: 6291456 bytes
    p73.1244.0: 2048 bytes
    p72.1242.0: 12582912 bytes
    p71.1240.0: 6291456 bytes
    p77.1316.0: 2048 bytes
    p76.1314.0: 12582912 bytes
    p75.1312.0: 6291456 bytes
    p81.1388.0: 2048 bytes
    p80.1386.0: 12582912 bytes
    p79.1384.0: 6291456 bytes
    p85.1460.0: 2048 bytes
    p84.1458.0: 12582912 bytes
    p83.1456.0: 6291456 bytes
    p89.1532.0: 2048 bytes
    p88.1530.0: 12582912 bytes
    p87.1528.0: 6291456 bytes
    p93.1604.0: 2048 bytes
    p92.1602.0: 12582912 bytes
    p91.1600.0: 6291456 bytes
    p97.1676.0: 2048 bytes
    p96.1674.0: 12582912 bytes
    p95.1672.0: 6291456 bytes
    p101.1748.0: 2048 bytes
    p100.1746.0: 12582912 bytes
    p99.1744.0: 6291456 bytes
    p105.1820.0: 2048 bytes
    p104.1818.0: 12582912 bytes
    p103.1816.0: 6291456 bytes
    p109.1892.0: 2048 bytes
    p108.1890.0: 12582912 bytes
    p107.1888.0: 6291456 bytes
    p113.1964.0: 2048 bytes
    p112.1962.0: 12582912 bytes
    p111.1960.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p115.2089.0: 10485760 bytes
    p114.2088.0: 256 bytes
    p116.2133.0: 277413888 bytes
