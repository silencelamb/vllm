BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <784 p5.38.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 56, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <840 p56.1023.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <673 gemm_fusion_dot.47.0 @0> (size=3145728,offset=0): bf16[128,12288]{1,0}
 value: <776 custom-call.49.0{0} @0> (size=1048576,offset=0): bf16[128,4096]{1,0}
 value: <781 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <672 wrapped_concatenate @0> (size=50331648,offset=0): bf16[12288,2048]{1,0}
 value: <678 custom-call.25.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <682 custom-call.26.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <686 custom-call.27.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <690 custom-call.28.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <694 custom-call.29.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <698 custom-call.30.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <703 custom-call.31.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <707 custom-call.32.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <711 custom-call.33.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <715 custom-call.34.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <720 custom-call.35.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <724 custom-call.36.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <728 custom-call.37.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <732 custom-call.38.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <736 custom-call.39.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <740 custom-call.40.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <744 custom-call.41.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <748 custom-call.42.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <753 custom-call.43.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <757 custom-call.44.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <761 custom-call.45.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <765 custom-call.46.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <769 custom-call.47.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <773 custom-call.48.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <777 custom-call.49.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <778 triton_softmax.14.0 @0> (size=524288,offset=0): f32[128,8,128]{2,1,0}
 value: <782 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <800 p8.60.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <803 p12.132.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <806 p16.204.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 12582912, parameter 20, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <809 p20.276.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 8: size 12582912, parameter 24, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <812 p24.348.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 9: size 12582912, parameter 28, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <815 p28.420.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 10: size 12582912, parameter 32, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <818 p32.492.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 11: size 12582912, parameter 36, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <821 p36.564.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 12: size 12582912, parameter 40, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <824 p40.636.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 13: size 12582912, parameter 44, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <827 p44.708.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 14: size 12582912, parameter 48, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <830 p48.780.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 15: size 12582912, parameter 52, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <833 p52.852.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 16: size 10485760, parameter 55, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <838 p55.979.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 17: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <836 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 18: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <801 p7.58.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 19: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <804 p11.130.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 20: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <807 p15.202.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 21: size 6291456, parameter 19, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <810 p19.274.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 22: size 6291456, parameter 23, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <813 p23.346.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 23: size 6291456, parameter 27, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <816 p27.418.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 24: size 6291456, parameter 31, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <819 p31.490.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 25: size 6291456, parameter 35, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <822 p35.562.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 26: size 6291456, parameter 39, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <825 p39.634.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 27: size 6291456, parameter 43, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <828 p43.706.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 28: size 6291456, parameter 47, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <831 p47.778.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 29: size 6291456, parameter 51, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <834 p51.850.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 30: size 4194304, parameter 50, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <786 p50.834.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 31: size 4194304, parameter 46, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <787 p46.762.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 32: size 4194304, parameter 42, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <788 p42.690.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 33: size 4194304, parameter 38, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <789 p38.618.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 34: size 4194304, parameter 34, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <790 p34.546.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 35: size 4194304, parameter 30, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <791 p30.474.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 36: size 4194304, parameter 26, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <792 p26.402.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 37: size 4194304, parameter 22, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <793 p22.330.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 38: size 4194304, parameter 18, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <794 p18.258.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 39: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <795 p14.186.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 40: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <796 p10.114.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 41: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <797 p6.42.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 42: size 262144, maybe-live-out:
 value: <675 fusion.180 @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <681 custom-call.26.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <708 fusion.172 @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <714 custom-call.34.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <725 fusion.168 @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <731 custom-call.38.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <758 fusion.160 @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <764 custom-call.46.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <779 input_concatenate_fusion @0> (size=262144,offset=0): bf16[128,8,128]{2,1,0}
allocation 43: size 262144, maybe-live-out:
 value: <674 loop_gather_fusion @0> (size=262144,offset=0): bf16[128,1,1024]{2,0,1}
 value: <700 fusion.174 @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <706 custom-call.32.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <717 fusion.170 @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <723 custom-call.36.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <750 fusion.162 @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <756 custom-call.44.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <780 wrapped_slice @0> (size=262144,offset=0): bf16[128,1024]{1,0}
allocation 44: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <799 p9.62.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 45: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <802 p13.134.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 46: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <805 p17.206.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 47: size 2048, parameter 21, shape |bf16[1024]| at ShapeIndex {}:
 value: <808 p21.278.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 48: size 2048, parameter 25, shape |bf16[1024]| at ShapeIndex {}:
 value: <811 p25.350.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 49: size 2048, parameter 29, shape |bf16[1024]| at ShapeIndex {}:
 value: <814 p29.422.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 50: size 2048, parameter 33, shape |bf16[1024]| at ShapeIndex {}:
 value: <817 p33.494.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 51: size 2048, parameter 37, shape |bf16[1024]| at ShapeIndex {}:
 value: <820 p37.566.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 52: size 2048, parameter 41, shape |bf16[1024]| at ShapeIndex {}:
 value: <823 p41.638.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 53: size 2048, parameter 45, shape |bf16[1024]| at ShapeIndex {}:
 value: <826 p45.710.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 54: size 2048, parameter 49, shape |bf16[1024]| at ShapeIndex {}:
 value: <829 p49.782.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 55: size 2048, parameter 53, shape |bf16[1024]| at ShapeIndex {}:
 value: <832 p53.854.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 56: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <835 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 57: size 512, parameter 4, shape |s32[128]| at ShapeIndex {}:
 value: <785 p4.36.0 @0> (size=512,offset=0): s32[128]{0}
allocation 58: size 512, parameter 54, shape |s32[128]| at ShapeIndex {}:
 value: <839 p54.978.0 @0> (size=512,offset=0): s32[128]{0}
allocation 59: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <837 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 60: size 32, output shape is |(bf16[128,8,128], bf16[128,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <841 tuple.1032.0{} @0> (size=32,offset=0): (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 61: size 4, thread-local:
 value: <24 add.165 @0> (size=4,offset=0): f32[]
allocation 62: size 4, thread-local:
 value: <23 y.72 @0> (size=4,offset=0): f32[]
allocation 63: size 4, thread-local:
 value: <22 x.71 @0> (size=4,offset=0): f32[]
allocation 64: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <798 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 65: size 3148960, preallocated-temp:
 value: <676 custom-call.25.0{} @0> (size=16,offset=3148800): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <677 custom-call.25.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <679 loop_convert_fusion @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <680 custom-call.26.0{} @0> (size=16,offset=3148672): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <683 fusion.178 @0> (size=262144,offset=1575808): bf16[128,1024]{1,0}
 value: <684 custom-call.27.0{} @0> (size=16,offset=2816): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <685 custom-call.27.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <687 loop_convert_fusion.1 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <688 custom-call.28.0{} @0> (size=16,offset=0): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <689 custom-call.28.0{0} @0> (size=262144,offset=2886528): bf16[128,1024]{1,0}
 value: <691 fusion.176 @0> (size=262144,offset=1575808): bf16[128,1024]{1,0}
 value: <692 custom-call.29.0{} @0> (size=16,offset=128): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <693 custom-call.29.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <695 loop_convert_fusion.2 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <696 custom-call.30.0{} @0> (size=16,offset=256): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <697 custom-call.30.0{0} @0> (size=262144,offset=2944): bf16[128,1024]{1,0}
 value: <699 loop_add_fusion @0> (size=524288,offset=2362240): f32[128,1024]{1,0}
 value: <701 custom-call.31.0{} @0> (size=16,offset=384): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <702 custom-call.31.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <704 loop_convert_fusion.3 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <705 custom-call.32.0{} @0> (size=16,offset=512): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <709 custom-call.33.0{} @0> (size=16,offset=640): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <710 custom-call.33.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <712 loop_convert_fusion.4 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <713 custom-call.34.0{} @0> (size=16,offset=768): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <716 loop_add_fusion.1 @0> (size=524288,offset=2362240): f32[128,1024]{1,0}
 value: <718 custom-call.35.0{} @0> (size=16,offset=896): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <719 custom-call.35.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <721 loop_convert_fusion.5 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <722 custom-call.36.0{} @0> (size=16,offset=1024): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <726 custom-call.37.0{} @0> (size=16,offset=1152): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <727 custom-call.37.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <729 loop_convert_fusion.6 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <730 custom-call.38.0{} @0> (size=16,offset=1280): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <733 fusion.166 @0> (size=262144,offset=1575808): bf16[128,1024]{1,0}
 value: <734 custom-call.39.0{} @0> (size=16,offset=1408): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <735 custom-call.39.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <737 loop_convert_fusion.7 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <738 custom-call.40.0{} @0> (size=16,offset=1536): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <739 custom-call.40.0{0} @0> (size=262144,offset=2886528): bf16[128,1024]{1,0}
 value: <741 fusion.164 @0> (size=262144,offset=1575808): bf16[128,1024]{1,0}
 value: <742 custom-call.41.0{} @0> (size=16,offset=1664): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <743 custom-call.41.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <745 loop_convert_fusion.8 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <746 custom-call.42.0{} @0> (size=16,offset=1792): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <747 custom-call.42.0{0} @0> (size=262144,offset=2944): bf16[128,1024]{1,0}
 value: <749 loop_add_fusion.2 @0> (size=524288,offset=2362240): f32[128,1024]{1,0}
 value: <751 custom-call.43.0{} @0> (size=16,offset=1920): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <752 custom-call.43.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <754 loop_convert_fusion.9 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <755 custom-call.44.0{} @0> (size=16,offset=2048): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <759 custom-call.45.0{} @0> (size=16,offset=2176): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <760 custom-call.45.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <762 loop_convert_fusion.10 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <763 custom-call.46.0{} @0> (size=16,offset=2304): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <766 fusion.158 @0> (size=262144,offset=1575808): bf16[128,1024]{1,0}
 value: <767 custom-call.47.0{} @0> (size=16,offset=2432): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <768 custom-call.47.0{0} @0> (size=1572864,offset=2944): bf16[128,6144]{1,0}
 value: <770 loop_convert_fusion.11 @0> (size=786432,offset=1575808): bf16[128,3072]{1,0}
 value: <771 custom-call.48.0{} @0> (size=16,offset=2560): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <772 custom-call.48.0{0} @0> (size=262144,offset=2944): bf16[128,1024]{1,0}
 value: <774 fusion.156 @0> (size=262144,offset=265088): bf16[128,1024]{1,0}
 value: <775 custom-call.49.0{} @0> (size=16,offset=2688): (bf16[128,4096]{1,0}, s8[4194304]{0})
 value: <783 tuple{} @0> (size=32,offset=3148928): (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 1165392336 (1.08GiB)

Used values:
<22 x.71 @0>
 positions:
  x.71
 uses:
  add.165, operand 0
 from instruction: %x.71 = f32[] parameter(0)
<23 y.72 @0>
 positions:
  y.72
 uses:
  add.165, operand 1
 from instruction: %y.72 = f32[] parameter(1)
<24 add.165 @0>
 positions:
  add.165
 uses:
 from instruction: %add.165 = f32[] add(f32[] %x.71, f32[] %y.72)
<672 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.47.0, operand 0
 from instruction: %wrapped_concatenate = bf16[12288,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9, bf16[1024,2048]{1,0} %p.10, bf16[1024,2048]{1,0} %p.11, /*index=10*/bf16[1024,2048]{1,0} %p.12, bf16[1024,2048]{1,0} %p.13), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<673 gemm_fusion_dot.47.0 @0>
 positions:
  gemm_fusion_dot.47.0
 uses:
  fusion.180, operand 2
  fusion.178, operand 3
  fusion.176, operand 2
  loop_add_fusion, operand 0
  fusion.172, operand 3
  loop_add_fusion.1, operand 0
  fusion.168, operand 3
  fusion.166, operand 4
  fusion.164, operand 2
  loop_add_fusion.2, operand 0
  fusion.160, operand 3
  fusion.158, operand 4
  fusion.156, operand 5
 from instruction: %gemm_fusion_dot.47.0 = bf16[128,12288]{1,0} fusion(bf16[12288,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.47_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"128","block_n":"64","block_k":"128","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<674 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.180, operand 1
  fusion.178, operand 2
  fusion.176, operand 4
  loop_add_fusion, operand 3
 from instruction: %loop_gather_fusion = bf16[128,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[128]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<675 fusion.180 @0>
 positions:
  fusion.180
 uses:
  custom-call.25.0, operand 0
 from instruction: %fusion.180 = bf16[128,1024]{1,0} fusion(f32[] %p.14, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[1024]{0} %p.15), kind=kCustom, calls=%fused_computation.151, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<676 custom-call.25.0{} @0>
 positions:
  custom-call.25.0 {}
 uses:
  get-tuple-element.25, operand 0 {}
 from instruction: %custom-call.25.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.180, bf16[6144,1024]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<677 custom-call.25.0{0} @0>
 positions:
  custom-call.25.0 {0}
  get-tuple-element.25
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.25.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.180, bf16[6144,1024]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<678 custom-call.25.0{1} @0>
 positions:
  custom-call.25.0 {1}
 uses:
 from instruction: %custom-call.25.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.180, bf16[6144,1024]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<679 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.26.0, operand 0
 from instruction: %loop_convert_fusion = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.25), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<680 custom-call.26.0{} @0>
 positions:
  custom-call.26.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.26.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<681 custom-call.26.0{0} @0>
 positions:
  custom-call.26.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.178, operand 4
  fusion.176, operand 5
  loop_add_fusion, operand 4
 from instruction: %custom-call.26.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<682 custom-call.26.0{1} @0>
 positions:
  custom-call.26.0 {1}
 uses:
 from instruction: %custom-call.26.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<683 fusion.178 @0>
 positions:
  fusion.178
 uses:
  custom-call.27.0, operand 0
 from instruction: %fusion.178 = bf16[128,1024]{1,0} fusion(f32[] %p.14, bf16[1024]{0} %p.18, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.149, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<684 custom-call.27.0{} @0>
 positions:
  custom-call.27.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.27.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.178, bf16[6144,1024]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<685 custom-call.27.0{0} @0>
 positions:
  custom-call.27.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.27.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.178, bf16[6144,1024]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<686 custom-call.27.0{1} @0>
 positions:
  custom-call.27.0 {1}
 uses:
 from instruction: %custom-call.27.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.178, bf16[6144,1024]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<687 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.28.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<688 custom-call.28.0{} @0>
 positions:
  custom-call.28.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.28.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<689 custom-call.28.0{0} @0>
 positions:
  custom-call.28.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.176, operand 1
  loop_add_fusion, operand 2
 from instruction: %custom-call.28.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<690 custom-call.28.0{1} @0>
 positions:
  custom-call.28.0 {1}
 uses:
 from instruction: %custom-call.28.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<691 fusion.176 @0>
 positions:
  fusion.176
 uses:
  custom-call.29.0, operand 0
 from instruction: %fusion.176 = bf16[128,1024]{1,0} fusion(f32[] %p.14, bf16[128,1024]{1,0} %get-tuple-element.3.0, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[1024]{0} %p.21, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.147, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<692 custom-call.29.0{} @0>
 positions:
  custom-call.29.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.29.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.176, bf16[6144,1024]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<693 custom-call.29.0{0} @0>
 positions:
  custom-call.29.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.29.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.176, bf16[6144,1024]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<694 custom-call.29.0{1} @0>
 positions:
  custom-call.29.0 {1}
 uses:
 from instruction: %custom-call.29.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.176, bf16[6144,1024]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<695 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.30.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<696 custom-call.30.0{} @0>
 positions:
  custom-call.30.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.30.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<697 custom-call.30.0{0} @0>
 positions:
  custom-call.30.0 {0}
  get-tuple-element.5.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.30.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<698 custom-call.30.0{1} @0>
 positions:
  custom-call.30.0 {1}
 uses:
 from instruction: %custom-call.30.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<699 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.174, operand 0
  fusion.172, operand 1
  loop_add_fusion.1, operand 2
 from instruction: %loop_add_fusion = f32[128,1024]{1,0} fusion(bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[128,1024]{1,0} %get-tuple-element.5.0, bf16[128,1024]{1,0} %get-tuple-element.3.0, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.1588/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<700 fusion.174 @0>
 positions:
  fusion.174
 uses:
  custom-call.31.0, operand 0
 from instruction: %fusion.174 = bf16[128,1024]{1,0} fusion(f32[128,1024]{1,0} %loop_add_fusion, f32[] %p.14, bf16[1024]{0} %p.24), kind=kCustom, calls=%fused_computation.145, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.162"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<701 custom-call.31.0{} @0>
 positions:
  custom-call.31.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.31.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.174, bf16[6144,1024]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<702 custom-call.31.0{0} @0>
 positions:
  custom-call.31.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.31.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.174, bf16[6144,1024]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<703 custom-call.31.0{1} @0>
 positions:
  custom-call.31.0 {1}
 uses:
 from instruction: %custom-call.31.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.174, bf16[6144,1024]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<704 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.32.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<705 custom-call.32.0{} @0>
 positions:
  custom-call.32.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.32.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<706 custom-call.32.0{0} @0>
 positions:
  custom-call.32.0 {0}
  get-tuple-element.7.0
 uses:
  fusion.172, operand 2
  loop_add_fusion.1, operand 3
 from instruction: %custom-call.32.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<707 custom-call.32.0{1} @0>
 positions:
  custom-call.32.0 {1}
 uses:
 from instruction: %custom-call.32.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<708 fusion.172 @0>
 positions:
  fusion.172
 uses:
  custom-call.33.0, operand 0
 from instruction: %fusion.172 = bf16[128,1024]{1,0} fusion(f32[] %p.14, f32[128,1024]{1,0} %loop_add_fusion, bf16[128,1024]{1,0} %get-tuple-element.7.0, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[1024]{0} %p.27), kind=kCustom, calls=%fused_computation.143, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<709 custom-call.33.0{} @0>
 positions:
  custom-call.33.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.33.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.172, bf16[6144,1024]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<710 custom-call.33.0{0} @0>
 positions:
  custom-call.33.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.33.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.172, bf16[6144,1024]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<711 custom-call.33.0{1} @0>
 positions:
  custom-call.33.0 {1}
 uses:
 from instruction: %custom-call.33.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.172, bf16[6144,1024]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<712 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.34.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<713 custom-call.34.0{} @0>
 positions:
  custom-call.34.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.34.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<714 custom-call.34.0{0} @0>
 positions:
  custom-call.34.0 {0}
  get-tuple-element.9.0
 uses:
  loop_add_fusion.1, operand 1
 from instruction: %custom-call.34.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<715 custom-call.34.0{1} @0>
 positions:
  custom-call.34.0 {1}
 uses:
 from instruction: %custom-call.34.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<716 loop_add_fusion.1 @0>
 positions:
  loop_add_fusion.1
 uses:
  fusion.170, operand 0
  fusion.168, operand 1
  fusion.166, operand 2
  fusion.164, operand 4
  loop_add_fusion.2, operand 3
 from instruction: %loop_add_fusion.1 = f32[128,1024]{1,0} fusion(bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[128,1024]{1,0} %get-tuple-element.9.0, f32[128,1024]{1,0} %loop_add_fusion, bf16[128,1024]{1,0} %get-tuple-element.7.0), kind=kLoop, calls=%fused_add.1, metadata={op_type="aten__add" op_name="aten__add.1624/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<717 fusion.170 @0>
 positions:
  fusion.170
 uses:
  custom-call.35.0, operand 0
 from instruction: %fusion.170 = bf16[128,1024]{1,0} fusion(f32[128,1024]{1,0} %loop_add_fusion.1, f32[] %p.14, bf16[1024]{0} %p.30), kind=kCustom, calls=%fused_computation.141, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.162"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<718 custom-call.35.0{} @0>
 positions:
  custom-call.35.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.35.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.170, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<719 custom-call.35.0{0} @0>
 positions:
  custom-call.35.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.35.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.170, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<720 custom-call.35.0{1} @0>
 positions:
  custom-call.35.0 {1}
 uses:
 from instruction: %custom-call.35.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.170, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<721 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.36.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<722 custom-call.36.0{} @0>
 positions:
  custom-call.36.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.36.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<723 custom-call.36.0{0} @0>
 positions:
  custom-call.36.0 {0}
  get-tuple-element.11.0
 uses:
  fusion.168, operand 2
  fusion.166, operand 3
  fusion.164, operand 5
  loop_add_fusion.2, operand 4
 from instruction: %custom-call.36.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<724 custom-call.36.0{1} @0>
 positions:
  custom-call.36.0 {1}
 uses:
 from instruction: %custom-call.36.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<725 fusion.168 @0>
 positions:
  fusion.168
 uses:
  custom-call.37.0, operand 0
 from instruction: %fusion.168 = bf16[128,1024]{1,0} fusion(f32[] %p.14, f32[128,1024]{1,0} %loop_add_fusion.1, bf16[128,1024]{1,0} %get-tuple-element.11.0, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[1024]{0} %p.33), kind=kCustom, calls=%fused_computation.139, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<726 custom-call.37.0{} @0>
 positions:
  custom-call.37.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.37.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.168, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<727 custom-call.37.0{0} @0>
 positions:
  custom-call.37.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.37.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.168, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<728 custom-call.37.0{1} @0>
 positions:
  custom-call.37.0 {1}
 uses:
 from instruction: %custom-call.37.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.168, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<729 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.38.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<730 custom-call.38.0{} @0>
 positions:
  custom-call.38.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.38.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<731 custom-call.38.0{0} @0>
 positions:
  custom-call.38.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.166, operand 5
  fusion.164, operand 6
  loop_add_fusion.2, operand 5
 from instruction: %custom-call.38.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<732 custom-call.38.0{1} @0>
 positions:
  custom-call.38.0 {1}
 uses:
 from instruction: %custom-call.38.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<733 fusion.166 @0>
 positions:
  fusion.166
 uses:
  custom-call.39.0, operand 0
 from instruction: %fusion.166 = bf16[128,1024]{1,0} fusion(f32[] %p.14, bf16[1024]{0} %p.36, f32[128,1024]{1,0} %loop_add_fusion.1, bf16[128,1024]{1,0} %get-tuple-element.11.0, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.13.0), kind=kCustom, calls=%fused_computation.137, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<734 custom-call.39.0{} @0>
 positions:
  custom-call.39.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.39.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.166, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<735 custom-call.39.0{0} @0>
 positions:
  custom-call.39.0 {0}
  get-tuple-element.14.0
 uses:
  loop_convert_fusion.7, operand 0
 from instruction: %custom-call.39.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.166, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<736 custom-call.39.0{1} @0>
 positions:
  custom-call.39.0 {1}
 uses:
 from instruction: %custom-call.39.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.166, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<737 loop_convert_fusion.7 @0>
 positions:
  loop_convert_fusion.7
 uses:
  custom-call.40.0, operand 0
 from instruction: %loop_convert_fusion.7 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<738 custom-call.40.0{} @0>
 positions:
  custom-call.40.0 {}
 uses:
  get-tuple-element.15.0, operand 0 {}
 from instruction: %custom-call.40.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<739 custom-call.40.0{0} @0>
 positions:
  custom-call.40.0 {0}
  get-tuple-element.15.0
 uses:
  fusion.164, operand 1
  loop_add_fusion.2, operand 2
 from instruction: %custom-call.40.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<740 custom-call.40.0{1} @0>
 positions:
  custom-call.40.0 {1}
 uses:
 from instruction: %custom-call.40.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<741 fusion.164 @0>
 positions:
  fusion.164
 uses:
  custom-call.41.0, operand 0
 from instruction: %fusion.164 = bf16[128,1024]{1,0} fusion(f32[] %p.14, bf16[128,1024]{1,0} %get-tuple-element.15.0, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[1024]{0} %p.39, f32[128,1024]{1,0} %loop_add_fusion.1, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.11.0, bf16[128,1024]{1,0} %get-tuple-element.13.0), kind=kCustom, calls=%fused_computation.135, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<742 custom-call.41.0{} @0>
 positions:
  custom-call.41.0 {}
 uses:
  get-tuple-element.16.0, operand 0 {}
 from instruction: %custom-call.41.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.164, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<743 custom-call.41.0{0} @0>
 positions:
  custom-call.41.0 {0}
  get-tuple-element.16.0
 uses:
  loop_convert_fusion.8, operand 0
 from instruction: %custom-call.41.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.164, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<744 custom-call.41.0{1} @0>
 positions:
  custom-call.41.0 {1}
 uses:
 from instruction: %custom-call.41.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.164, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<745 loop_convert_fusion.8 @0>
 positions:
  loop_convert_fusion.8
 uses:
  custom-call.42.0, operand 0
 from instruction: %loop_convert_fusion.8 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%fused_convert.8, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<746 custom-call.42.0{} @0>
 positions:
  custom-call.42.0 {}
 uses:
  get-tuple-element.17.0, operand 0 {}
 from instruction: %custom-call.42.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<747 custom-call.42.0{0} @0>
 positions:
  custom-call.42.0 {0}
  get-tuple-element.17.0
 uses:
  loop_add_fusion.2, operand 1
 from instruction: %custom-call.42.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<748 custom-call.42.0{1} @0>
 positions:
  custom-call.42.0 {1}
 uses:
 from instruction: %custom-call.42.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<749 loop_add_fusion.2 @0>
 positions:
  loop_add_fusion.2
 uses:
  fusion.162, operand 0
  fusion.160, operand 1
  fusion.158, operand 2
  fusion.156, operand 3
 from instruction: %loop_add_fusion.2 = f32[128,1024]{1,0} fusion(bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[128,1024]{1,0} %get-tuple-element.17.0, bf16[128,1024]{1,0} %get-tuple-element.15.0, f32[128,1024]{1,0} %loop_add_fusion.1, bf16[128,1024]{1,0} %get-tuple-element.11.0, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.13.0), kind=kLoop, calls=%fused_add.2, metadata={op_type="aten__add" op_name="aten__add.1696/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<750 fusion.162 @0>
 positions:
  fusion.162
 uses:
  custom-call.43.0, operand 0
 from instruction: %fusion.162 = bf16[128,1024]{1,0} fusion(f32[128,1024]{1,0} %loop_add_fusion.2, f32[] %p.14, bf16[1024]{0} %p.42), kind=kCustom, calls=%fused_computation.133, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.162"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<751 custom-call.43.0{} @0>
 positions:
  custom-call.43.0 {}
 uses:
  get-tuple-element.18.0, operand 0 {}
 from instruction: %custom-call.43.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.162, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<752 custom-call.43.0{0} @0>
 positions:
  custom-call.43.0 {0}
  get-tuple-element.18.0
 uses:
  loop_convert_fusion.9, operand 0
 from instruction: %custom-call.43.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.162, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<753 custom-call.43.0{1} @0>
 positions:
  custom-call.43.0 {1}
 uses:
 from instruction: %custom-call.43.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.162, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<754 loop_convert_fusion.9 @0>
 positions:
  loop_convert_fusion.9
 uses:
  custom-call.44.0, operand 0
 from instruction: %loop_convert_fusion.9 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.18.0), kind=kLoop, calls=%fused_convert.9, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<755 custom-call.44.0{} @0>
 positions:
  custom-call.44.0 {}
 uses:
  get-tuple-element.19.0, operand 0 {}
 from instruction: %custom-call.44.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<756 custom-call.44.0{0} @0>
 positions:
  custom-call.44.0 {0}
  get-tuple-element.19.0
 uses:
  fusion.160, operand 2
  fusion.158, operand 3
  fusion.156, operand 4
 from instruction: %custom-call.44.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<757 custom-call.44.0{1} @0>
 positions:
  custom-call.44.0 {1}
 uses:
 from instruction: %custom-call.44.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<758 fusion.160 @0>
 positions:
  fusion.160
 uses:
  custom-call.45.0, operand 0
 from instruction: %fusion.160 = bf16[128,1024]{1,0} fusion(f32[] %p.14, f32[128,1024]{1,0} %loop_add_fusion.2, bf16[128,1024]{1,0} %get-tuple-element.19.0, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[1024]{0} %p.45), kind=kCustom, calls=%fused_computation.131, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<759 custom-call.45.0{} @0>
 positions:
  custom-call.45.0 {}
 uses:
  get-tuple-element.20.0, operand 0 {}
 from instruction: %custom-call.45.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.160, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<760 custom-call.45.0{0} @0>
 positions:
  custom-call.45.0 {0}
  get-tuple-element.20.0
 uses:
  loop_convert_fusion.10, operand 0
 from instruction: %custom-call.45.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.160, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<761 custom-call.45.0{1} @0>
 positions:
  custom-call.45.0 {1}
 uses:
 from instruction: %custom-call.45.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.160, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<762 loop_convert_fusion.10 @0>
 positions:
  loop_convert_fusion.10
 uses:
  custom-call.46.0, operand 0
 from instruction: %loop_convert_fusion.10 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.20.0), kind=kLoop, calls=%fused_convert.10, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<763 custom-call.46.0{} @0>
 positions:
  custom-call.46.0 {}
 uses:
  get-tuple-element.21.0, operand 0 {}
 from instruction: %custom-call.46.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<764 custom-call.46.0{0} @0>
 positions:
  custom-call.46.0 {0}
  get-tuple-element.21.0
 uses:
  fusion.158, operand 5
  fusion.156, operand 6
 from instruction: %custom-call.46.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<765 custom-call.46.0{1} @0>
 positions:
  custom-call.46.0 {1}
 uses:
 from instruction: %custom-call.46.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<766 fusion.158 @0>
 positions:
  fusion.158
 uses:
  custom-call.47.0, operand 0
 from instruction: %fusion.158 = bf16[128,1024]{1,0} fusion(f32[] %p.14, bf16[1024]{0} %p.48, f32[128,1024]{1,0} %loop_add_fusion.2, bf16[128,1024]{1,0} %get-tuple-element.19.0, bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.21.0), kind=kCustom, calls=%fused_computation.129, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<767 custom-call.47.0{} @0>
 positions:
  custom-call.47.0 {}
 uses:
  get-tuple-element.22.0, operand 0 {}
 from instruction: %custom-call.47.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.158, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<768 custom-call.47.0{0} @0>
 positions:
  custom-call.47.0 {0}
  get-tuple-element.22.0
 uses:
  loop_convert_fusion.11, operand 0
 from instruction: %custom-call.47.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.158, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<769 custom-call.47.0{1} @0>
 positions:
  custom-call.47.0 {1}
 uses:
 from instruction: %custom-call.47.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.158, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<770 loop_convert_fusion.11 @0>
 positions:
  loop_convert_fusion.11
 uses:
  custom-call.48.0, operand 0
 from instruction: %loop_convert_fusion.11 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.22.0), kind=kLoop, calls=%fused_convert.11, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<771 custom-call.48.0{} @0>
 positions:
  custom-call.48.0 {}
 uses:
  get-tuple-element.23.0, operand 0 {}
 from instruction: %custom-call.48.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<772 custom-call.48.0{0} @0>
 positions:
  custom-call.48.0 {0}
  get-tuple-element.23.0
 uses:
  fusion.156, operand 1
 from instruction: %custom-call.48.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<773 custom-call.48.0{1} @0>
 positions:
  custom-call.48.0 {1}
 uses:
 from instruction: %custom-call.48.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<774 fusion.156 @0>
 positions:
  fusion.156
 uses:
  custom-call.49.0, operand 0
 from instruction: %fusion.156 = bf16[128,1024]{1,0} fusion(f32[] %p.14, bf16[128,1024]{1,0} %get-tuple-element.23.0, bf16[1024]{0} %p.51, f32[128,1024]{1,0} %loop_add_fusion.2, bf16[128,1024]{1,0} %get-tuple-element.19.0, /*index=5*/bf16[128,12288]{1,0} %gemm_fusion_dot.47.0, bf16[128,1024]{1,0} %get-tuple-element.21.0), kind=kCustom, calls=%fused_computation.127, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<775 custom-call.49.0{} @0>
 positions:
  custom-call.49.0 {}
 uses:
  get-tuple-element.24.0, operand 0 {}
 from instruction: %custom-call.49.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.156, bf16[4096,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<776 custom-call.49.0{0} @0>
 positions:
  custom-call.49.0 {0}
  get-tuple-element.24.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.14.0, operand 1
 from instruction: %custom-call.49.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.156, bf16[4096,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<777 custom-call.49.0{1} @0>
 positions:
  custom-call.49.0 {1}
 uses:
 from instruction: %custom-call.49.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.156, bf16[4096,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<778 triton_softmax.14.0 @0>
 positions:
  triton_softmax.14.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.14.0 = f32[128,8,128]{2,1,0} fusion(f32[] %p.14, bf16[128,4096]{1,0} %get-tuple-element.24.0), kind=kCustom, calls=%triton_softmax_computation.14, metadata={op_type="aten__mul" op_name="aten__mul.1742/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<779 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.1032.0 {0}
  call {0}
  get-tuple-element.27
  tuple {0}
 uses:
  tuple, operand 0
  tuple.1032.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[128,8,128]{2,1,0} fusion(f32[128,8,128]{2,1,0} %triton_softmax.14.0, bf16[128]{0} %p.53, bf16[40960,128]{1,0} %p.54, s32[128]{0} %p.55), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<780 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.1032.0 {1}
  call {1}
  get-tuple-element.28
  bitcast.2625.0
  tuple {1}
 uses:
  bitcast.2625.0, operand 0
  tuple.1032.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[128,1024]{1,0} fusion(bf16[128,4096]{1,0} %get-tuple-element.24.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<781 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.1032.0 {3}
  call {2}
  get-tuple-element.29
  bitcast.2637.0
  tuple {2}
 uses:
  bitcast.2637.0, operand 0
  tuple.1032.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.56), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<782 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.1032.0 {2}
  bitcast.2630.0
  call {3}
  get-tuple-element.30
  tuple {3}
 uses:
  tuple, operand 3
  tuple.1032.0, operand 2
  bitcast.2630.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.56), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<783 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.27, operand 0 {}
  get-tuple-element.28, operand 0 {}
  get-tuple-element.29, operand 0 {}
  get-tuple-element.30, operand 0 {}
 from instruction: %tuple = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %input_concatenate_fusion, bf16[128,8,128]{2,1,0} %bitcast.2625.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.2637.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<784 p5.38.0 @0>
 positions:
  p5.38.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.38.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<785 p4.36.0 @0>
 positions:
  p4.36.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.36.0 = s32[128]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<786 p50.834.0 @0>
 positions:
  p50.834.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p50.834.0 = bf16[1024,2048]{1,0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<787 p46.762.0 @0>
 positions:
  p46.762.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p46.762.0 = bf16[1024,2048]{1,0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<788 p42.690.0 @0>
 positions:
  p42.690.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p42.690.0 = bf16[1024,2048]{1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<789 p38.618.0 @0>
 positions:
  p38.618.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p38.618.0 = bf16[1024,2048]{1,0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<790 p34.546.0 @0>
 positions:
  p34.546.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p34.546.0 = bf16[1024,2048]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<791 p30.474.0 @0>
 positions:
  p30.474.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p30.474.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<792 p26.402.0 @0>
 positions:
  p26.402.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p26.402.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<793 p22.330.0 @0>
 positions:
  p22.330.0
  p.9
 uses:
  call, operand 9
  wrapped_concatenate, operand 7
 from instruction: %p22.330.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<794 p18.258.0 @0>
 positions:
  p18.258.0
  p.10
 uses:
  call, operand 10
  wrapped_concatenate, operand 8
 from instruction: %p18.258.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<795 p14.186.0 @0>
 positions:
  p14.186.0
  p.11
 uses:
  call, operand 11
  wrapped_concatenate, operand 9
 from instruction: %p14.186.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<796 p10.114.0 @0>
 positions:
  p10.114.0
  p.12
 uses:
  call, operand 12
  wrapped_concatenate, operand 10
 from instruction: %p10.114.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<797 p6.42.0 @0>
 positions:
  p6.42.0
  p.13
 uses:
  call, operand 13
  wrapped_concatenate, operand 11
 from instruction: %p6.42.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<798 p1.4.0 @0>
 positions:
  p1.4.0
  p.14
 uses:
  call, operand 14
  fusion.180, operand 0
  fusion.178, operand 0
  fusion.176, operand 0
  fusion.174, operand 1
  fusion.172, operand 0
  fusion.170, operand 1
  fusion.168, operand 0
  fusion.166, operand 0
  fusion.164, operand 0
  fusion.162, operand 1
  fusion.160, operand 0
  fusion.158, operand 0
  fusion.156, operand 0
  triton_softmax.14.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<799 p9.62.0 @0>
 positions:
  p9.62.0
  p.15
 uses:
  call, operand 15
  fusion.180, operand 3
 from instruction: %p9.62.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<800 p8.60.0 @0>
 positions:
  p8.60.0
  p.16
 uses:
  call, operand 16
  custom-call.25.0, operand 1
 from instruction: %p8.60.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<801 p7.58.0 @0>
 positions:
  p7.58.0
  p.17
 uses:
  call, operand 17
  custom-call.26.0, operand 1
 from instruction: %p7.58.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<802 p13.134.0 @0>
 positions:
  p13.134.0
  p.18
 uses:
  call, operand 18
  fusion.178, operand 1
 from instruction: %p13.134.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<803 p12.132.0 @0>
 positions:
  p12.132.0
  p.19
 uses:
  call, operand 19
  custom-call.27.0, operand 1
 from instruction: %p12.132.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<804 p11.130.0 @0>
 positions:
  p11.130.0
  p.20
 uses:
  call, operand 20
  custom-call.28.0, operand 1
 from instruction: %p11.130.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<805 p17.206.0 @0>
 positions:
  p17.206.0
  p.21
 uses:
  call, operand 21
  fusion.176, operand 3
 from instruction: %p17.206.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<806 p16.204.0 @0>
 positions:
  p16.204.0
  p.22
 uses:
  call, operand 22
  custom-call.29.0, operand 1
 from instruction: %p16.204.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<807 p15.202.0 @0>
 positions:
  p15.202.0
  p.23
 uses:
  call, operand 23
  custom-call.30.0, operand 1
 from instruction: %p15.202.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<808 p21.278.0 @0>
 positions:
  p21.278.0
  p.24
 uses:
  call, operand 24
  fusion.174, operand 2
 from instruction: %p21.278.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<809 p20.276.0 @0>
 positions:
  p20.276.0
  p.25
 uses:
  call, operand 25
  custom-call.31.0, operand 1
 from instruction: %p20.276.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<810 p19.274.0 @0>
 positions:
  p19.274.0
  p.26
 uses:
  call, operand 26
  custom-call.32.0, operand 1
 from instruction: %p19.274.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<811 p25.350.0 @0>
 positions:
  p25.350.0
  p.27
 uses:
  call, operand 27
  fusion.172, operand 4
 from instruction: %p25.350.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<812 p24.348.0 @0>
 positions:
  p24.348.0
  p.28
 uses:
  call, operand 28
  custom-call.33.0, operand 1
 from instruction: %p24.348.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<813 p23.346.0 @0>
 positions:
  p23.346.0
  p.29
 uses:
  call, operand 29
  custom-call.34.0, operand 1
 from instruction: %p23.346.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<814 p29.422.0 @0>
 positions:
  p29.422.0
  p.30
 uses:
  call, operand 30
  fusion.170, operand 2
 from instruction: %p29.422.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<815 p28.420.0 @0>
 positions:
  p28.420.0
  p.31
 uses:
  call, operand 31
  custom-call.35.0, operand 1
 from instruction: %p28.420.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<816 p27.418.0 @0>
 positions:
  p27.418.0
  p.32
 uses:
  call, operand 32
  custom-call.36.0, operand 1
 from instruction: %p27.418.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<817 p33.494.0 @0>
 positions:
  p33.494.0
  p.33
 uses:
  call, operand 33
  fusion.168, operand 4
 from instruction: %p33.494.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<818 p32.492.0 @0>
 positions:
  p32.492.0
  p.34
 uses:
  call, operand 34
  custom-call.37.0, operand 1
 from instruction: %p32.492.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<819 p31.490.0 @0>
 positions:
  p31.490.0
  p.35
 uses:
  call, operand 35
  custom-call.38.0, operand 1
 from instruction: %p31.490.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<820 p37.566.0 @0>
 positions:
  p37.566.0
  p.36
 uses:
  call, operand 36
  fusion.166, operand 1
 from instruction: %p37.566.0 = bf16[1024]{0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<821 p36.564.0 @0>
 positions:
  p36.564.0
  p.37
 uses:
  call, operand 37
  custom-call.39.0, operand 1
 from instruction: %p36.564.0 = bf16[6144,1024]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<822 p35.562.0 @0>
 positions:
  p35.562.0
  p.38
 uses:
  call, operand 38
  custom-call.40.0, operand 1
 from instruction: %p35.562.0 = bf16[1024,3072]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<823 p41.638.0 @0>
 positions:
  p41.638.0
  p.39
 uses:
  call, operand 39
  fusion.164, operand 3
 from instruction: %p41.638.0 = bf16[1024]{0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<824 p40.636.0 @0>
 positions:
  p40.636.0
  p.40
 uses:
  call, operand 40
  custom-call.41.0, operand 1
 from instruction: %p40.636.0 = bf16[6144,1024]{1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<825 p39.634.0 @0>
 positions:
  p39.634.0
  p.41
 uses:
  call, operand 41
  custom-call.42.0, operand 1
 from instruction: %p39.634.0 = bf16[1024,3072]{1,0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<826 p45.710.0 @0>
 positions:
  p45.710.0
  p.42
 uses:
  call, operand 42
  fusion.162, operand 2
 from instruction: %p45.710.0 = bf16[1024]{0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<827 p44.708.0 @0>
 positions:
  p44.708.0
  p.43
 uses:
  call, operand 43
  custom-call.43.0, operand 1
 from instruction: %p44.708.0 = bf16[6144,1024]{1,0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<828 p43.706.0 @0>
 positions:
  p43.706.0
  p.44
 uses:
  call, operand 44
  custom-call.44.0, operand 1
 from instruction: %p43.706.0 = bf16[1024,3072]{1,0} parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<829 p49.782.0 @0>
 positions:
  p49.782.0
  p.45
 uses:
  call, operand 45
  fusion.160, operand 4
 from instruction: %p49.782.0 = bf16[1024]{0} parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<830 p48.780.0 @0>
 positions:
  p48.780.0
  p.46
 uses:
  call, operand 46
  custom-call.45.0, operand 1
 from instruction: %p48.780.0 = bf16[6144,1024]{1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<831 p47.778.0 @0>
 positions:
  p47.778.0
  p.47
 uses:
  call, operand 47
  custom-call.46.0, operand 1
 from instruction: %p47.778.0 = bf16[1024,3072]{1,0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<832 p53.854.0 @0>
 positions:
  p53.854.0
  p.48
 uses:
  call, operand 48
  fusion.158, operand 1
 from instruction: %p53.854.0 = bf16[1024]{0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<833 p52.852.0 @0>
 positions:
  p52.852.0
  p.49
 uses:
  call, operand 49
  custom-call.47.0, operand 1
 from instruction: %p52.852.0 = bf16[6144,1024]{1,0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<834 p51.850.0 @0>
 positions:
  p51.850.0
  p.50
 uses:
  call, operand 50
  custom-call.48.0, operand 1
 from instruction: %p51.850.0 = bf16[1024,3072]{1,0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<835 p3.8.0 @0>
 positions:
  p3.8.0
  p.51
 uses:
  call, operand 51
  fusion.156, operand 2
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<836 p2.6.0 @0>
 positions:
  p2.6.0
  p.52
 uses:
  call, operand 52
  custom-call.49.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<837 p0.1.0 @0>
 positions:
  p0.1.0
  p.53
 uses:
  call, operand 53
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<838 p55.979.0 @0>
 positions:
  p55.979.0
  p.54
 uses:
  call, operand 54
  input_concatenate_fusion, operand 2
 from instruction: %p55.979.0 = bf16[40960,128]{1,0} parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<839 p54.978.0 @0>
 positions:
  p54.978.0
  p.55
 uses:
  call, operand 55
  input_concatenate_fusion, operand 3
 from instruction: %p54.978.0 = s32[128]{0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<840 p56.1023.0 @0>
 positions:
  p56.1023.0
  p.56
 uses:
  call, operand 56
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p56.1023.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<841 tuple.1032.0{} @0>
 positions:
  tuple.1032.0 {}
 uses:
 from instruction: %tuple.1032.0 = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %get-tuple-element.27, bf16[128,8,128]{2,1,0} %get-tuple-element.28, bf16[4233,16,8,128]{3,2,1,0} %bitcast.2630.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.29)


HloLiveRange (max 210):
  InstructionSequence:
    0:p1.4.0
    1:p56.1023.0
    2:p55.979.0
    3:p54.978.0
    4:p53.854.0
    5:p52.852.0
    6:p51.850.0
    7:p50.834.0
    8:p49.782.0
    9:p48.780.0
    10:p47.778.0
    11:p46.762.0
    12:p45.710.0
    13:p44.708.0
    14:p43.706.0
    15:p42.690.0
    16:p41.638.0
    17:p40.636.0
    18:p39.634.0
    19:p38.618.0
    20:p37.566.0
    21:p36.564.0
    22:p35.562.0
    23:p34.546.0
    24:p33.494.0
    25:p32.492.0
    26:p31.490.0
    27:p30.474.0
    28:p29.422.0
    29:p28.420.0
    30:p27.418.0
    31:p26.402.0
    32:p25.350.0
    33:p24.348.0
    34:p23.346.0
    35:p22.330.0
    36:p21.278.0
    37:p20.276.0
    38:p19.274.0
    39:p18.258.0
    40:p17.206.0
    41:p16.204.0
    42:p15.202.0
    43:p14.186.0
    44:p13.134.0
    45:p12.132.0
    46:p11.130.0
    47:p10.114.0
    48:p9.62.0
    49:p8.60.0
    50:p7.58.0
    51:p6.42.0
    52:p5.38.0
    53:p4.36.0
    54:p3.8.0
    55:p2.6.0
    56:p0.1.0
    57:p
    58:p.1
    59:p.2
    60:p.3
    61:p.4
    62:p.5
    63:p.6
    64:p.7
    65:p.8
    66:p.9
    67:p.10
    68:p.11
    69:p.12
    70:p.13
    71:p.14
    72:p.15
    73:p.16
    74:p.17
    75:p.18
    76:p.19
    77:p.20
    78:p.21
    79:p.22
    80:p.23
    81:p.24
    82:p.25
    83:p.26
    84:p.27
    85:p.28
    86:p.29
    87:p.30
    88:p.31
    89:p.32
    90:p.33
    91:p.34
    92:p.35
    93:p.36
    94:p.37
    95:p.38
    96:p.39
    97:p.40
    98:p.41
    99:p.42
    100:p.43
    101:p.44
    102:p.45
    103:p.46
    104:p.47
    105:p.48
    106:p.49
    107:p.50
    108:p.51
    109:p.52
    110:p.53
    111:p.54
    112:p.55
    113:p.56
    114:loop_gather_fusion
    115:wrapped_concatenate
    116:gemm_fusion_dot.47.0
    117:fusion.180
    118:custom-call.25.0
    119:get-tuple-element.25
    120:loop_convert_fusion
    121:custom-call.26.0
    122:get-tuple-element.1.0
    123:fusion.178
    124:custom-call.27.0
    125:get-tuple-element.2.0
    126:loop_convert_fusion.1
    127:custom-call.28.0
    128:get-tuple-element.3.0
    129:fusion.176
    130:custom-call.29.0
    131:get-tuple-element.4.0
    132:loop_convert_fusion.2
    133:custom-call.30.0
    134:get-tuple-element.5.0
    135:loop_add_fusion
    136:fusion.174
    137:custom-call.31.0
    138:get-tuple-element.6.0
    139:loop_convert_fusion.3
    140:custom-call.32.0
    141:get-tuple-element.7.0
    142:fusion.172
    143:custom-call.33.0
    144:get-tuple-element.8.0
    145:loop_convert_fusion.4
    146:custom-call.34.0
    147:get-tuple-element.9.0
    148:loop_add_fusion.1
    149:fusion.170
    150:custom-call.35.0
    151:get-tuple-element.10.0
    152:loop_convert_fusion.5
    153:custom-call.36.0
    154:get-tuple-element.11.0
    155:fusion.168
    156:custom-call.37.0
    157:get-tuple-element.12.0
    158:loop_convert_fusion.6
    159:custom-call.38.0
    160:get-tuple-element.13.0
    161:fusion.166
    162:custom-call.39.0
    163:get-tuple-element.14.0
    164:loop_convert_fusion.7
    165:custom-call.40.0
    166:get-tuple-element.15.0
    167:fusion.164
    168:custom-call.41.0
    169:get-tuple-element.16.0
    170:loop_convert_fusion.8
    171:custom-call.42.0
    172:get-tuple-element.17.0
    173:loop_add_fusion.2
    174:fusion.162
    175:custom-call.43.0
    176:get-tuple-element.18.0
    177:loop_convert_fusion.9
    178:custom-call.44.0
    179:get-tuple-element.19.0
    180:fusion.160
    181:custom-call.45.0
    182:get-tuple-element.20.0
    183:loop_convert_fusion.10
    184:custom-call.46.0
    185:get-tuple-element.21.0
    186:fusion.158
    187:custom-call.47.0
    188:get-tuple-element.22.0
    189:loop_convert_fusion.11
    190:custom-call.48.0
    191:get-tuple-element.23.0
    192:fusion.156
    193:custom-call.49.0
    194:get-tuple-element.24.0
    195:wrapped_slice
    196:triton_softmax.14.0
    197:input_concatenate_fusion
    198:bitcast.2625.0
    199:loop_slice_fusion
    200:bitcast.2637.0
    201:wrapped_slice.1
    202:tuple
    203:call
    204:get-tuple-element.27
    205:get-tuple-element.28
    206:get-tuple-element.29
    207:get-tuple-element.30
    208:bitcast.2630.0
    209:tuple.1032.0
  BufferLiveRange:
    wrapped_concatenate{}:115-116
    gemm_fusion_dot.47.0{}:116-192
    loop_gather_fusion{}:114-135
    fusion.180{}:117-118
    custom-call.25.0{}:118-119
    custom-call.25.0{0}:118-120
    custom-call.25.0{1}:118-118
    loop_convert_fusion{}:120-121
    custom-call.26.0{}:121-122
    custom-call.26.0{0}:121-135
    custom-call.26.0{1}:121-121
    fusion.178{}:123-124
    custom-call.27.0{}:124-125
    custom-call.27.0{0}:124-126
    custom-call.27.0{1}:124-124
    loop_convert_fusion.1{}:126-127
    custom-call.28.0{}:127-128
    custom-call.28.0{0}:127-135
    custom-call.28.0{1}:127-127
    fusion.176{}:129-130
    custom-call.29.0{}:130-131
    custom-call.29.0{0}:130-132
    custom-call.29.0{1}:130-130
    loop_convert_fusion.2{}:132-133
    custom-call.30.0{}:133-134
    custom-call.30.0{0}:133-135
    custom-call.30.0{1}:133-133
    loop_add_fusion{}:135-148
    fusion.174{}:136-137
    custom-call.31.0{}:137-138
    custom-call.31.0{0}:137-139
    custom-call.31.0{1}:137-137
    loop_convert_fusion.3{}:139-140
    custom-call.32.0{}:140-141
    custom-call.32.0{0}:140-148
    custom-call.32.0{1}:140-140
    fusion.172{}:142-143
    custom-call.33.0{}:143-144
    custom-call.33.0{0}:143-145
    custom-call.33.0{1}:143-143
    loop_convert_fusion.4{}:145-146
    custom-call.34.0{}:146-147
    custom-call.34.0{0}:146-148
    custom-call.34.0{1}:146-146
    loop_add_fusion.1{}:148-173
    fusion.170{}:149-150
    custom-call.35.0{}:150-151
    custom-call.35.0{0}:150-152
    custom-call.35.0{1}:150-150
    loop_convert_fusion.5{}:152-153
    custom-call.36.0{}:153-154
    custom-call.36.0{0}:153-173
    custom-call.36.0{1}:153-153
    fusion.168{}:155-156
    custom-call.37.0{}:156-157
    custom-call.37.0{0}:156-158
    custom-call.37.0{1}:156-156
    loop_convert_fusion.6{}:158-159
    custom-call.38.0{}:159-160
    custom-call.38.0{0}:159-173
    custom-call.38.0{1}:159-159
    fusion.166{}:161-162
    custom-call.39.0{}:162-163
    custom-call.39.0{0}:162-164
    custom-call.39.0{1}:162-162
    loop_convert_fusion.7{}:164-165
    custom-call.40.0{}:165-166
    custom-call.40.0{0}:165-173
    custom-call.40.0{1}:165-165
    fusion.164{}:167-168
    custom-call.41.0{}:168-169
    custom-call.41.0{0}:168-170
    custom-call.41.0{1}:168-168
    loop_convert_fusion.8{}:170-171
    custom-call.42.0{}:171-172
    custom-call.42.0{0}:171-173
    custom-call.42.0{1}:171-171
    loop_add_fusion.2{}:173-192
    fusion.162{}:174-175
    custom-call.43.0{}:175-176
    custom-call.43.0{0}:175-177
    custom-call.43.0{1}:175-175
    loop_convert_fusion.9{}:177-178
    custom-call.44.0{}:178-179
    custom-call.44.0{0}:178-192
    custom-call.44.0{1}:178-178
    fusion.160{}:180-181
    custom-call.45.0{}:181-182
    custom-call.45.0{0}:181-183
    custom-call.45.0{1}:181-181
    loop_convert_fusion.10{}:183-184
    custom-call.46.0{}:184-185
    custom-call.46.0{0}:184-192
    custom-call.46.0{1}:184-184
    fusion.158{}:186-187
    custom-call.47.0{}:187-188
    custom-call.47.0{0}:187-189
    custom-call.47.0{1}:187-187
    loop_convert_fusion.11{}:189-190
    custom-call.48.0{}:190-191
    custom-call.48.0{0}:190-192
    custom-call.48.0{1}:190-190
    fusion.156{}:192-193
    custom-call.49.0{}:193-194
    custom-call.49.0{0}:193-196
    custom-call.49.0{1}:193-193
    triton_softmax.14.0{}:196-197
    input_concatenate_fusion{}:197-210
    wrapped_slice{}:195-210
    loop_slice_fusion{}:199-210
    wrapped_slice.1{}:201-210
    tuple{}:202-207
    p5.38.0{}:0-210
    p4.36.0{}:0-210
    p50.834.0{}:0-210
    p46.762.0{}:0-210
    p42.690.0{}:0-210
    p38.618.0{}:0-210
    p34.546.0{}:0-210
    p30.474.0{}:0-210
    p26.402.0{}:0-210
    p22.330.0{}:0-210
    p18.258.0{}:0-210
    p14.186.0{}:0-210
    p10.114.0{}:0-210
    p6.42.0{}:0-210
    p1.4.0{}:0-210
    p9.62.0{}:0-210
    p8.60.0{}:0-210
    p7.58.0{}:0-210
    p13.134.0{}:0-210
    p12.132.0{}:0-210
    p11.130.0{}:0-210
    p17.206.0{}:0-210
    p16.204.0{}:0-210
    p15.202.0{}:0-210
    p21.278.0{}:0-210
    p20.276.0{}:0-210
    p19.274.0{}:0-210
    p25.350.0{}:0-210
    p24.348.0{}:0-210
    p23.346.0{}:0-210
    p29.422.0{}:0-210
    p28.420.0{}:0-210
    p27.418.0{}:0-210
    p33.494.0{}:0-210
    p32.492.0{}:0-210
    p31.490.0{}:0-210
    p37.566.0{}:0-210
    p36.564.0{}:0-210
    p35.562.0{}:0-210
    p41.638.0{}:0-210
    p40.636.0{}:0-210
    p39.634.0{}:0-210
    p45.710.0{}:0-210
    p44.708.0{}:0-210
    p43.706.0{}:0-210
    p49.782.0{}:0-210
    p48.780.0{}:0-210
    p47.778.0{}:0-210
    p53.854.0{}:0-210
    p52.852.0{}:0-210
    p51.850.0{}:0-210
    p3.8.0{}:0-210
    p2.6.0{}:0-210
    p0.1.0{}:0-210
    p55.979.0{}:0-210
    p54.978.0{}:0-210
    p56.1023.0{}:0-210
    tuple.1032.0{}:209-210
  Live ranges at 202 (peak):
    input_concatenate_fusion: 262144 bytes
    wrapped_slice: 262144 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.38.0: 311164928 bytes
    p4.36.0: 512 bytes
    p50.834.0: 4194304 bytes
    p46.762.0: 4194304 bytes
    p42.690.0: 4194304 bytes
    p38.618.0: 4194304 bytes
    p34.546.0: 4194304 bytes
    p30.474.0: 4194304 bytes
    p26.402.0: 4194304 bytes
    p22.330.0: 4194304 bytes
    p18.258.0: 4194304 bytes
    p14.186.0: 4194304 bytes
    p10.114.0: 4194304 bytes
    p6.42.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.62.0: 2048 bytes
    p8.60.0: 12582912 bytes
    p7.58.0: 6291456 bytes
    p13.134.0: 2048 bytes
    p12.132.0: 12582912 bytes
    p11.130.0: 6291456 bytes
    p17.206.0: 2048 bytes
    p16.204.0: 12582912 bytes
    p15.202.0: 6291456 bytes
    p21.278.0: 2048 bytes
    p20.276.0: 12582912 bytes
    p19.274.0: 6291456 bytes
    p25.350.0: 2048 bytes
    p24.348.0: 12582912 bytes
    p23.346.0: 6291456 bytes
    p29.422.0: 2048 bytes
    p28.420.0: 12582912 bytes
    p27.418.0: 6291456 bytes
    p33.494.0: 2048 bytes
    p32.492.0: 12582912 bytes
    p31.490.0: 6291456 bytes
    p37.566.0: 2048 bytes
    p36.564.0: 12582912 bytes
    p35.562.0: 6291456 bytes
    p41.638.0: 2048 bytes
    p40.636.0: 12582912 bytes
    p39.634.0: 6291456 bytes
    p45.710.0: 2048 bytes
    p44.708.0: 12582912 bytes
    p43.706.0: 6291456 bytes
    p49.782.0: 2048 bytes
    p48.780.0: 12582912 bytes
    p47.778.0: 6291456 bytes
    p53.854.0: 2048 bytes
    p52.852.0: 12582912 bytes
    p51.850.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p55.979.0: 10485760 bytes
    p54.978.0: 512 bytes
    p56.1023.0: 277413888 bytes
