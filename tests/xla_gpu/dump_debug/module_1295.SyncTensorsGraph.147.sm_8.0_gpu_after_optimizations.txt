HloModule SyncTensorsGraph.147, is_scheduled=true, entry_computation_layout={(bf16[128]{0}, f32[], bf16[4096,1024]{1,0}, bf16[1024]{0}, s32[32]{0}, /*index=5*/bf16[151936,1024]{1,0}, s32[32]{0}, bf16[40960,128]{1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0})->(bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})}, frontend_attributes={fingerprint_before_lhs="3ed62c78b475301a87a13b85da64cb94"}

%fused_gather (param_0.7: bf16[151936,1024], param_1.58: s32[32]) -> bf16[32,1,1024] {
  %param_0.7 = bf16[151936,1024]{1,0} parameter(0)
  %param_1.58 = s32[32]{0} parameter(1)
  %convert.72.1 = s64[32]{0} convert(s32[32]{0} %param_1.58), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.73.1 = u32[32]{0} convert(s64[32]{0} %convert.72.1), metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.322.1 = u32[32,1]{1,0} bitcast(u32[32]{0} %convert.73.1)
  ROOT %gather.3 = bf16[32,1,1024]{2,1,0} gather(bf16[151936,1024]{1,0} %param_0.7, u32[32,1]{1,0} %bitcast.322.1), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,1024}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%AddComputation.60 (x.61: f32[], y.62: f32[]) -> f32[] {
  %y.62 = f32[] parameter(1)
  %x.61 = f32[] parameter(0)
  ROOT %add.14 = f32[] add(f32[] %x.61, f32[] %y.62)
}

%fused_computation.7 (param_0.60: f32[], param_1.55: bf16[32,1,1024], param_2.42: bf16[1024]) -> bf16[32,1,1024] {
  %param_1.55 = bf16[32,1,1024]{2,1,0} parameter(1)
  %convert.74.3 = f32[32,1,1024]{2,1,0} convert(bf16[32,1,1024]{2,1,0} %param_1.55), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.62 = f32[32,1,1024]{2,1,0} multiply(f32[32,1,1024]{2,1,0} %convert.74.3, f32[32,1,1024]{2,1,0} %convert.74.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.425 = f32[32,1024]{1,0} bitcast(f32[32,1,1024]{2,1,0} %multiply.62)
  %constant_37 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.9 = f32[32]{0} reduce(f32[32,1024]{1,0} %bitcast.425, f32[] %constant_37), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_36 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.63 = f32[32]{0} broadcast(f32[] %constant_36), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.61 = f32[32]{0} multiply(f32[32]{0} %reduce.9, f32[32]{0} %broadcast.63), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.60 = f32[] parameter(0)
  %broadcast.62 = f32[32]{0} broadcast(f32[] %param_0.60), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.508/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.19 = f32[32]{0} add(f32[32]{0} %multiply.61, f32[32]{0} %broadcast.62), metadata={op_type="aten__add" op_name="aten__add.508/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.13 = f32[32]{0} rsqrt(f32[32]{0} %add.19), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.61 = f32[32,1,1024]{2,1,0} broadcast(f32[32]{0} %rsqrt.13), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.509/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.60 = f32[32,1,1024]{2,1,0} multiply(f32[32,1,1024]{2,1,0} %convert.74.3, f32[32,1,1024]{2,1,0} %broadcast.61), metadata={op_type="aten__mul" op_name="aten__mul.509/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.42 = bf16[1024]{0} parameter(2)
  %convert.76.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.42), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.46.3 = f32[32,1,1024]{2,1,0} broadcast(f32[1024]{0} %convert.76.1), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul.510/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.43.3 = f32[32,1,1024]{2,1,0} multiply(f32[32,1,1024]{2,1,0} %multiply.60, f32[32,1,1024]{2,1,0} %broadcast.46.3), metadata={op_type="aten__mul" op_name="aten__mul.510/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.77.1 = bf16[32,1,1024]{2,1,0} convert(f32[32,1,1024]{2,1,0} %multiply.43.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_slice_computation (param_0.63: bf16[32,4096]) -> bf16[32,1024] {
  %param_0.63 = bf16[32,4096]{1,0} parameter(0)
  ROOT %slice.28.1 = bf16[32,1024]{1,0} slice(bf16[32,4096]{1,0} %param_0.63), slice={[0:32], [3072:4096]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.5 (param_0.61: f32[], param_1.56: bf16[32,4096], param_2.43: bf16[128]) -> f32[32,8,128] {
  %param_1.56 = bf16[32,4096]{1,0} parameter(1)
  %slice.23.1 = bf16[32,1024]{1,0} slice(bf16[32,4096]{1,0} %param_1.56), slice={[0:32], [2048:3072]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.343.5 = bf16[32,8,128]{2,1,0} bitcast(bf16[32,1024]{1,0} %slice.23.1)
  %convert.78.5 = f32[32,8,128]{2,1,0} convert(bf16[32,8,128]{2,1,0} %bitcast.343.5), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.56 = f32[32,8,128]{2,1,0} multiply(f32[32,8,128]{2,1,0} %convert.78.5, f32[32,8,128]{2,1,0} %convert.78.5), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_32 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.7 = f32[32,8]{1,0} reduce(f32[32,8,128]{2,1,0} %multiply.56, f32[] %constant_32), dimensions={2}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_31 = f32[] constant(0.0078125), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.57 = f32[32,8]{1,0} broadcast(f32[] %constant_31), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.55 = f32[32,8]{1,0} multiply(f32[32,8]{1,0} %reduce.7, f32[32,8]{1,0} %broadcast.57), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.61 = f32[] parameter(0)
  %broadcast.55 = f32[32,8]{1,0} broadcast(f32[] %param_0.61), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.511/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.17 = f32[32,8]{1,0} add(f32[32,8]{1,0} %multiply.55, f32[32,8]{1,0} %broadcast.55), metadata={op_type="aten__add" op_name="aten__add.511/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.11 = f32[32,8]{1,0} rsqrt(f32[32,8]{1,0} %add.17), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.54 = f32[32,8,128]{2,1,0} broadcast(f32[32,8]{1,0} %rsqrt.11), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul.512/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.54 = f32[32,8,128]{2,1,0} multiply(f32[32,8,128]{2,1,0} %convert.78.5, f32[32,8,128]{2,1,0} %broadcast.54), metadata={op_type="aten__mul" op_name="aten__mul.512/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.43 = bf16[128]{0} parameter(2)
  %convert.79.1 = f32[128]{0} convert(bf16[128]{0} %param_2.43), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.48.1 = f32[32,8,128]{2,1,0} broadcast(f32[128]{0} %convert.79.1), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul.513/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %multiply.45.1 = f32[32,8,128]{2,1,0} multiply(f32[32,8,128]{2,1,0} %multiply.54, f32[32,8,128]{2,1,0} %broadcast.48.1), metadata={op_type="aten__mul" op_name="aten__mul.513/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_concatenate (param_0.59: f32[32,8,128], param_1.60: bf16[40960,128], param_2.45: s32[32]) -> bf16[32,8,128] {
  %param_0.59 = f32[32,8,128]{2,1,0} parameter(0)
  %slice.24.4 = f32[32,8,64]{2,1,0} slice(f32[32,8,128]{2,1,0} %param_0.59), slice={[0:32], [0:8], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.60 = bf16[40960,128]{1,0} parameter(1)
  %param_2.45 = s32[32]{0} parameter(2)
  %bitcast.359.3 = s32[32,1]{1,0} bitcast(s32[32]{0} %param_2.45)
  %gather.1.3 = bf16[32,1,128]{2,0,1} gather(bf16[40960,128]{1,0} %param_1.60, s32[32,1]{1,0} %bitcast.359.3), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,128}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.362.7 = bf16[32,128]{1,0} bitcast(bf16[32,1,128]{2,0,1} %gather.1.3)
  %convert.80.7 = f32[32,128]{1,0} convert(bf16[32,128]{1,0} %bitcast.362.7), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.25.3 = f32[32,64]{1,0} slice(f32[32,128]{1,0} %convert.80.7), slice={[0:32], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.49.12 = f32[32,8,64]{2,1,0} broadcast(f32[32,64]{1,0} %slice.25.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.514/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.46.7 = f32[32,8,64]{2,1,0} multiply(f32[32,8,64]{2,1,0} %slice.24.4, f32[32,8,64]{2,1,0} %broadcast.49.12), metadata={op_type="aten__mul" op_name="aten__mul.514/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.26.4 = f32[32,8,64]{2,1,0} slice(f32[32,8,128]{2,1,0} %param_0.59), slice={[0:32], [0:8], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.27.3 = f32[32,64]{1,0} slice(f32[32,128]{1,0} %convert.80.7), slice={[0:32], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.50.8 = f32[32,8,64]{2,1,0} broadcast(f32[32,64]{1,0} %slice.27.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.515/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.47.5 = f32[32,8,64]{2,1,0} multiply(f32[32,8,64]{2,1,0} %slice.26.4, f32[32,8,64]{2,1,0} %broadcast.50.8), metadata={op_type="aten__mul" op_name="aten__mul.515/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %subtract.2.5 = f32[32,8,64]{2,1,0} subtract(f32[32,8,64]{2,1,0} %multiply.46.7, f32[32,8,64]{2,1,0} %multiply.47.5), metadata={op_type="aten__sub" op_name="aten__sub.516/aten__sub" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.81.3 = bf16[32,8,64]{2,1,0} convert(f32[32,8,64]{2,1,0} %subtract.2.5)
  %multiply.49.7 = f32[32,8,64]{2,1,0} multiply(f32[32,8,64]{2,1,0} %slice.26.4, f32[32,8,64]{2,1,0} %broadcast.49.12), metadata={op_type="aten__mul" op_name="aten__mul.517/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.50.5 = f32[32,8,64]{2,1,0} multiply(f32[32,8,64]{2,1,0} %slice.24.4, f32[32,8,64]{2,1,0} %broadcast.50.8), metadata={op_type="aten__mul" op_name="aten__mul.518/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.15.5 = f32[32,8,64]{2,1,0} add(f32[32,8,64]{2,1,0} %multiply.49.7, f32[32,8,64]{2,1,0} %multiply.50.5), metadata={op_type="aten__add" op_name="aten__add.519/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.82.3 = bf16[32,8,64]{2,1,0} convert(f32[32,8,64]{2,1,0} %add.15.5)
  ROOT %concatenate.4.1 = bf16[32,8,128]{2,1,0} concatenate(bf16[32,8,64]{2,1,0} %convert.81.3, bf16[32,8,64]{2,1,0} %convert.82.3), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_slice (param_0.1: bf16[2,4233,16,8,128]) -> bf16[69353472] {
  %param_0.1 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  %bitcast.403.1 = bf16[138706944]{0} bitcast(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.1)
  ROOT %slice.30.1 = bf16[69353472]{0} slice(bf16[138706944]{0} %bitcast.403.1), slice={[69353472:138706944]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%wrapped_slice_computation.1 (param_0.64: bf16[2,4233,16,8,128]) -> bf16[1,4233,16,8,128] {
  %param_0.64 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  ROOT %slice.29.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} slice(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.64), slice={[0:1], [0:4233], [0:16], [0:8], [0:128]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%command_buffer (p: bf16[151936,1024], p.1: s32[32], p.2: f32[], p.3: bf16[1024], p.4: bf16[4096,1024], p.5: bf16[128], p.6: bf16[40960,128], p.7: s32[32], p.8: bf16[2,4233,16,8,128]) -> (bf16[32,8,128], bf16[32,8,128], bf16[4233,16,8,128], bf16[1,4233,16,8,128]) {
  %p = bf16[151936,1024]{1,0} parameter(0)
  %p.1 = s32[32]{0} parameter(1)
  %p.2 = f32[] parameter(2)
  %p.3 = bf16[1024]{0} parameter(3)
  %p.4 = bf16[4096,1024]{1,0} parameter(4)
  %p.5 = bf16[128]{0} parameter(5)
  %p.6 = bf16[40960,128]{1,0} parameter(6)
  %p.7 = s32[32]{0} parameter(7)
  %p.8 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(8)
  %loop_gather_fusion = bf16[32,1,1024]{2,1,0} fusion(bf16[151936,1024]{1,0} %p, s32[32]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.11 = bf16[32,1,1024]{2,1,0} fusion(f32[] %p.2, bf16[32,1,1024]{2,1,0} %loop_gather_fusion, bf16[1024]{0} %p.3), kind=kCustom, calls=%fused_computation.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1","256"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %bitcast.337.0 = bf16[32,1024]{1,0} bitcast(bf16[32,1,1024]{2,1,0} %fusion.11)
  %custom-call.1.0 = (bf16[32,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %bitcast.337.0, bf16[4096,1024]{1,0} %p.4), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.1 = bf16[32,4096]{1,0} get-tuple-element((bf16[32,4096]{1,0}, s8[4194304]{0}) %custom-call.1.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_slice = bf16[32,1024]{1,0} fusion(bf16[32,4096]{1,0} %get-tuple-element.1), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.9 = f32[32,8,128]{2,1,0} fusion(f32[] %p.2, bf16[32,4096]{1,0} %get-tuple-element.1, bf16[128]{0} %p.5), kind=kCustom, calls=%fused_computation.5, metadata={op_type="aten__mul" op_name="aten__mul.513/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %input_concatenate_fusion = bf16[32,8,128]{2,1,0} fusion(f32[32,8,128]{2,1,0} %fusion.9, bf16[40960,128]{1,0} %p.6, s32[32]{0} %p.7), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.395.0 = bf16[32,8,128]{2,1,0} bitcast(bf16[32,1024]{1,0} %wrapped_slice)
  %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.8), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  %bitcast.407.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[69353472]{0} %loop_slice_fusion)
  %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.8), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  ROOT %tuple = (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[32,8,128]{2,1,0} %input_concatenate_fusion, bf16[32,8,128]{2,1,0} %bitcast.395.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.407.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
}

ENTRY %SyncTensorsGraph.147 (p0.1.0: bf16[128], p1.4.0: f32[], p2.6.0: bf16[4096,1024], p3.8.0: bf16[1024], p4.12.0: s32[32], p5.14.0: bf16[151936,1024], p6.92.0: s32[32], p7.93.0: bf16[40960,128], p8.137.0: bf16[2,4233,16,8,128]) -> (bf16[32,8,128], bf16[32,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128]) {
  %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %p8.137.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p7.93.0 = bf16[40960,128]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p6.92.0 = s32[32]{0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p5.14.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p4.12.0 = s32[32]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %call = (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) call(bf16[151936,1024]{1,0} %p5.14.0, s32[32]{0} %p4.12.0, f32[] %p1.4.0, bf16[1024]{0} %p3.8.0, bf16[4096,1024]{1,0} %p2.6.0, /*index=5*/bf16[128]{0} %p0.1.0, bf16[40960,128]{1,0} %p7.93.0, s32[32]{0} %p6.92.0, bf16[2,4233,16,8,128]{4,3,2,1,0} %p8.137.0), to_apply=%command_buffer
  %get-tuple-element.3 = bf16[32,8,128]{2,1,0} get-tuple-element((bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=0
  %get-tuple-element.4 = bf16[32,8,128]{2,1,0} get-tuple-element((bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=1
  %get-tuple-element.5 = bf16[4233,16,8,128]{3,2,1,0} get-tuple-element((bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=2
  %get-tuple-element.6 = bf16[1,4233,16,8,128]{4,3,2,1,0} get-tuple-element((bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=3
  %bitcast.400.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[1,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.6)
  ROOT %tuple.146.0 = (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[32,8,128]{2,1,0} %get-tuple-element.3, bf16[32,8,128]{2,1,0} %get-tuple-element.4, bf16[4233,16,8,128]{3,2,1,0} %bitcast.400.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.5)
}

