BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <141 p5.16.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 12, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <153 p12.209.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <125 custom-call.3.0{0} @0> (size=786432,offset=0): bf16[64,6144]{1,0}
 value: <129 custom-call.4.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <133 custom-call.5.0{0} @0> (size=524288,offset=0): bf16[64,4096]{1,0}
 value: <138 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <126 custom-call.3.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <130 custom-call.4.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <134 custom-call.5.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <135 triton_softmax.3.0 @0> (size=262144,offset=0): f32[64,8,128]{2,1,0}
 value: <139 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <146 p8.38.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 10485760, parameter 11, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <151 p11.165.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 6: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <149 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 7: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <147 p7.36.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 8: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <143 p6.20.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 9: size 131072, maybe-live-out:
 value: <122 gemm_fusion_dot.29.0 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <136 input_concatenate_fusion @0> (size=131072,offset=0): bf16[64,8,128]{2,1,0}
allocation 10: size 131072, maybe-live-out:
 value: <121 loop_gather_fusion @0> (size=131072,offset=0): bf16[64,1,1024]{2,0,1}
 value: <137 wrapped_slice @0> (size=131072,offset=0): bf16[64,1024]{1,0}
allocation 11: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <145 p9.40.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 12: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <148 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 13: size 256, parameter 4, shape |s32[64]| at ShapeIndex {}:
 value: <142 p4.14.0 @0> (size=256,offset=0): s32[64]{0}
allocation 14: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <150 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 15: size 256, parameter 10, shape |s32[64]| at ShapeIndex {}:
 value: <152 p10.164.0 @0> (size=256,offset=0): s32[64]{0}
allocation 16: size 32, output shape is |(bf16[64,8,128], bf16[64,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <154 tuple.218.0{} @0> (size=32,offset=0): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 17: size 4, thread-local:
 value: <11 add.26 @0> (size=4,offset=0): f32[]
allocation 18: size 4, thread-local:
 value: <10 y.50 @0> (size=4,offset=0): f32[]
allocation 19: size 4, thread-local:
 value: <9 x.49 @0> (size=4,offset=0): f32[]
allocation 20: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <144 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 21: size 393632, preallocated-temp:
 value: <123 fusion.25 @0> (size=131072,offset=128): bf16[64,1024]{1,0}
 value: <124 custom-call.3.0{} @0> (size=16,offset=393472): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <127 loop_convert_fusion @0> (size=393216,offset=128): bf16[64,3072]{1,0}
 value: <128 custom-call.4.0{} @0> (size=16,offset=393344): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <131 fusion.23 @0> (size=131072,offset=128): bf16[64,1024]{1,0}
 value: <132 custom-call.5.0{} @0> (size=16,offset=0): (bf16[64,4096]{1,0}, s8[4194304]{0})
 value: <140 tuple{} @0> (size=32,offset=393600): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 908596432 (866.50MiB)

Used values:
<9 x.49 @0>
 positions:
  x.49
 uses:
  add.26, operand 0
 from instruction: %x.49 = f32[] parameter(0)
<10 y.50 @0>
 positions:
  y.50
 uses:
  add.26, operand 1
 from instruction: %y.50 = f32[] parameter(1)
<11 add.26 @0>
 positions:
  add.26
 uses:
 from instruction: %add.26 = f32[] add(f32[] %x.49, f32[] %y.50)
<121 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.25, operand 1
  fusion.23, operand 1
 from instruction: %loop_gather_fusion = bf16[64,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[64]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<122 gemm_fusion_dot.29.0 @0>
 positions:
  gemm_fusion_dot.29.0
 uses:
  fusion.25, operand 2
  fusion.23, operand 2
 from instruction: %gemm_fusion_dot.29.0 = bf16[64,1024]{1,0} fusion(bf16[1024,2048]{1,0} %p.2), kind=kCustom, calls=%gemm_fusion_dot.29_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"32","block_n":"16","block_k":"512","split_k":"1","num_stages":"1","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<123 fusion.25 @0>
 positions:
  fusion.25
 uses:
  custom-call.3.0, operand 0
 from instruction: %fusion.25 = bf16[64,1024]{1,0} fusion(f32[] %p.3, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,1024]{1,0} %gemm_fusion_dot.29.0, bf16[1024]{0} %p.4), kind=kCustom, calls=%fused_computation.18, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<124 custom-call.3.0{} @0>
 positions:
  custom-call.3.0 {}
 uses:
  get-tuple-element.3, operand 0 {}
 from instruction: %custom-call.3.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.25, bf16[6144,1024]{1,0} %p.5), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<125 custom-call.3.0{0} @0>
 positions:
  custom-call.3.0 {0}
  get-tuple-element.3
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.3.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.25, bf16[6144,1024]{1,0} %p.5), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<126 custom-call.3.0{1} @0>
 positions:
  custom-call.3.0 {1}
 uses:
 from instruction: %custom-call.3.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.25, bf16[6144,1024]{1,0} %p.5), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<127 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.4.0, operand 0
 from instruction: %loop_convert_fusion = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.3), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<128 custom-call.4.0{} @0>
 positions:
  custom-call.4.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.4.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.6), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<129 custom-call.4.0{0} @0>
 positions:
  custom-call.4.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.23, operand 3
 from instruction: %custom-call.4.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.6), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<130 custom-call.4.0{1} @0>
 positions:
  custom-call.4.0 {1}
 uses:
 from instruction: %custom-call.4.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.6), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<131 fusion.23 @0>
 positions:
  fusion.23
 uses:
  custom-call.5.0, operand 0
 from instruction: %fusion.23 = bf16[64,1024]{1,0} fusion(f32[] %p.3, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,1024]{1,0} %gemm_fusion_dot.29.0, bf16[64,1024]{1,0} %get-tuple-element.1.0, bf16[1024]{0} %p.7), kind=kCustom, calls=%fused_computation.16, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<132 custom-call.5.0{} @0>
 positions:
  custom-call.5.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.5.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.23, bf16[4096,1024]{1,0} %p.8), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<133 custom-call.5.0{0} @0>
 positions:
  custom-call.5.0 {0}
  get-tuple-element.2.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.3.0, operand 1
 from instruction: %custom-call.5.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.23, bf16[4096,1024]{1,0} %p.8), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<134 custom-call.5.0{1} @0>
 positions:
  custom-call.5.0 {1}
 uses:
 from instruction: %custom-call.5.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.23, bf16[4096,1024]{1,0} %p.8), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<135 triton_softmax.3.0 @0>
 positions:
  triton_softmax.3.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.3.0 = f32[64,8,128]{2,1,0} fusion(f32[] %p.3, bf16[64,4096]{1,0} %get-tuple-element.2.0), kind=kCustom, calls=%triton_softmax_computation.3, metadata={op_type="aten__mul" op_name="aten__mul.1037/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<136 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.218.0 {0}
  call {0}
  get-tuple-element.5
  tuple {0}
 uses:
  tuple, operand 0
  tuple.218.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[64,8,128]{2,1,0} fusion(f32[64,8,128]{2,1,0} %triton_softmax.3.0, bf16[128]{0} %p.9, bf16[40960,128]{1,0} %p.10, s32[64]{0} %p.11), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<137 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.218.0 {1}
  call {1}
  get-tuple-element.6
  bitcast.573.0
  tuple {1}
 uses:
  bitcast.573.0, operand 0
  tuple.218.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[64,1024]{1,0} fusion(bf16[64,4096]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<138 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.218.0 {3}
  call {2}
  get-tuple-element.7
  bitcast.585.0
  tuple {2}
 uses:
  bitcast.585.0, operand 0
  tuple.218.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.12), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<139 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.218.0 {2}
  bitcast.578.0
  call {3}
  get-tuple-element.8
  tuple {3}
 uses:
  tuple, operand 3
  tuple.218.0, operand 2
  bitcast.578.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.12), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<140 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.5, operand 0 {}
  get-tuple-element.6, operand 0 {}
  get-tuple-element.7, operand 0 {}
  get-tuple-element.8, operand 0 {}
 from instruction: %tuple = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %input_concatenate_fusion, bf16[64,8,128]{2,1,0} %bitcast.573.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.585.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<141 p5.16.0 @0>
 positions:
  p5.16.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.16.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<142 p4.14.0 @0>
 positions:
  p4.14.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.14.0 = s32[64]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<143 p6.20.0 @0>
 positions:
  p6.20.0
  p.2
 uses:
  call, operand 2
  gemm_fusion_dot.29.0, operand 0
 from instruction: %p6.20.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<144 p1.4.0 @0>
 positions:
  p1.4.0
  p.3
 uses:
  call, operand 3
  fusion.25, operand 0
  fusion.23, operand 0
  triton_softmax.3.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<145 p9.40.0 @0>
 positions:
  p9.40.0
  p.4
 uses:
  call, operand 4
  fusion.25, operand 3
 from instruction: %p9.40.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<146 p8.38.0 @0>
 positions:
  p8.38.0
  p.5
 uses:
  call, operand 5
  custom-call.3.0, operand 1
 from instruction: %p8.38.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<147 p7.36.0 @0>
 positions:
  p7.36.0
  p.6
 uses:
  call, operand 6
  custom-call.4.0, operand 1
 from instruction: %p7.36.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<148 p3.8.0 @0>
 positions:
  p3.8.0
  p.7
 uses:
  call, operand 7
  fusion.23, operand 4
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<149 p2.6.0 @0>
 positions:
  p2.6.0
  p.8
 uses:
  call, operand 8
  custom-call.5.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<150 p0.1.0 @0>
 positions:
  p0.1.0
  p.9
 uses:
  call, operand 9
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<151 p11.165.0 @0>
 positions:
  p11.165.0
  p.10
 uses:
  call, operand 10
  input_concatenate_fusion, operand 2
 from instruction: %p11.165.0 = bf16[40960,128]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<152 p10.164.0 @0>
 positions:
  p10.164.0
  p.11
 uses:
  call, operand 11
  input_concatenate_fusion, operand 3
 from instruction: %p10.164.0 = s32[64]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<153 p12.209.0 @0>
 positions:
  p12.209.0
  p.12
 uses:
  call, operand 12
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p12.209.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<154 tuple.218.0{} @0>
 positions:
  tuple.218.0 {}
 uses:
 from instruction: %tuple.218.0 = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %get-tuple-element.5, bf16[64,8,128]{2,1,0} %get-tuple-element.6, bf16[4233,16,8,128]{3,2,1,0} %bitcast.578.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.7)


HloLiveRange (max 52):
  InstructionSequence:
    0:p1.4.0
    1:p12.209.0
    2:p11.165.0
    3:p10.164.0
    4:p9.40.0
    5:p8.38.0
    6:p7.36.0
    7:p6.20.0
    8:p5.16.0
    9:p4.14.0
    10:p3.8.0
    11:p2.6.0
    12:p0.1.0
    13:p
    14:p.1
    15:p.2
    16:p.3
    17:p.4
    18:p.5
    19:p.6
    20:p.7
    21:p.8
    22:p.9
    23:p.10
    24:p.11
    25:p.12
    26:loop_gather_fusion
    27:gemm_fusion_dot.29.0
    28:fusion.25
    29:custom-call.3.0
    30:get-tuple-element.3
    31:loop_convert_fusion
    32:custom-call.4.0
    33:get-tuple-element.1.0
    34:fusion.23
    35:custom-call.5.0
    36:get-tuple-element.2.0
    37:wrapped_slice
    38:triton_softmax.3.0
    39:input_concatenate_fusion
    40:bitcast.573.0
    41:loop_slice_fusion
    42:bitcast.585.0
    43:wrapped_slice.1
    44:tuple
    45:call
    46:get-tuple-element.5
    47:get-tuple-element.6
    48:get-tuple-element.7
    49:get-tuple-element.8
    50:bitcast.578.0
    51:tuple.218.0
  BufferLiveRange:
    loop_gather_fusion{}:26-34
    gemm_fusion_dot.29.0{}:27-34
    fusion.25{}:28-29
    custom-call.3.0{}:29-30
    custom-call.3.0{0}:29-31
    custom-call.3.0{1}:29-29
    loop_convert_fusion{}:31-32
    custom-call.4.0{}:32-33
    custom-call.4.0{0}:32-34
    custom-call.4.0{1}:32-32
    fusion.23{}:34-35
    custom-call.5.0{}:35-36
    custom-call.5.0{0}:35-38
    custom-call.5.0{1}:35-35
    triton_softmax.3.0{}:38-39
    input_concatenate_fusion{}:39-52
    wrapped_slice{}:37-52
    loop_slice_fusion{}:41-52
    wrapped_slice.1{}:43-52
    tuple{}:44-49
    p5.16.0{}:0-52
    p4.14.0{}:0-52
    p6.20.0{}:0-52
    p1.4.0{}:0-52
    p9.40.0{}:0-52
    p8.38.0{}:0-52
    p7.36.0{}:0-52
    p3.8.0{}:0-52
    p2.6.0{}:0-52
    p0.1.0{}:0-52
    p11.165.0{}:0-52
    p10.164.0{}:0-52
    p12.209.0{}:0-52
    tuple.218.0{}:51-52
  Live ranges at 44 (peak):
    input_concatenate_fusion: 131072 bytes
    wrapped_slice: 131072 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.16.0: 311164928 bytes
    p4.14.0: 256 bytes
    p6.20.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.40.0: 2048 bytes
    p8.38.0: 12582912 bytes
    p7.36.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p11.165.0: 10485760 bytes
    p10.164.0: 256 bytes
    p12.209.0: 277413888 bytes
