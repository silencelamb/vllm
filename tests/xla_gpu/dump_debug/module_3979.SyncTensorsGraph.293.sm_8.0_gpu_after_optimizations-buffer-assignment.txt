BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <203 p5.18.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 16, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <219 p16.283.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <179 custom-call.5.0{0} @0> (size=1572864,offset=0): bf16[128,6144]{1,0}
 value: <187 custom-call.7.0{0} @0> (size=1572864,offset=0): bf16[128,6144]{1,0}
 value: <191 custom-call.8.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <195 custom-call.9.0{0} @0> (size=1048576,offset=0): bf16[128,4096]{1,0}
 value: <200 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <180 custom-call.5.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <184 custom-call.6.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <188 custom-call.7.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <192 custom-call.8.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <196 custom-call.9.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <197 triton_softmax.4.0 @0> (size=524288,offset=0): f32[128,8,128]{2,1,0}
 value: <201 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <209 p8.40.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <212 p12.112.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 10485760, parameter 15, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <217 p15.239.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 7: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <215 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 8: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <210 p7.38.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 9: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <213 p11.110.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 10: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <205 p10.94.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 11: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <206 p6.22.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 12: size 262144, maybe-live-out:
 value: <177 fusion.40 @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <183 custom-call.6.0{0} @0> (size=262144,offset=0): bf16[128,1024]{1,0}
 value: <198 input_concatenate_fusion @0> (size=262144,offset=0): bf16[128,8,128]{2,1,0}
allocation 13: size 262144, maybe-live-out:
 value: <175 loop_gather_fusion @0> (size=262144,offset=0): bf16[128,1,1024]{2,0,1}
 value: <199 wrapped_slice @0> (size=262144,offset=0): bf16[128,1024]{1,0}
allocation 14: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <208 p9.42.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 15: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <211 p13.114.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 16: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <214 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 17: size 512, parameter 4, shape |s32[128]| at ShapeIndex {}:
 value: <204 p4.16.0 @0> (size=512,offset=0): s32[128]{0}
allocation 18: size 512, parameter 14, shape |s32[128]| at ShapeIndex {}:
 value: <218 p14.238.0 @0> (size=512,offset=0): s32[128]{0}
allocation 19: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <216 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 20: size 32, output shape is |(bf16[128,8,128], bf16[128,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <220 tuple.292.0{} @0> (size=32,offset=0): (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 21: size 4, thread-local:
 value: <13 add.39 @0> (size=4,offset=0): f32[]
allocation 22: size 4, thread-local:
 value: <12 y.52 @0> (size=4,offset=0): f32[]
allocation 23: size 4, thread-local:
 value: <11 x.51 @0> (size=4,offset=0): f32[]
allocation 24: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <207 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 25: size 1311392, preallocated-temp:
 value: <176 gemm_fusion_dot.7.0 @0> (size=524288,offset=786816): bf16[128,2048]{1,0}
 value: <178 custom-call.5.0{} @0> (size=16,offset=1311232): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <181 loop_convert_fusion @0> (size=786432,offset=384): bf16[128,3072]{1,0}
 value: <182 custom-call.6.0{} @0> (size=16,offset=1311104): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <185 fusion.38 @0> (size=262144,offset=384): bf16[128,1024]{1,0}
 value: <186 custom-call.7.0{} @0> (size=16,offset=256): (bf16[128,6144]{1,0}, s8[4194304]{0})
 value: <189 loop_convert_fusion.1 @0> (size=786432,offset=384): bf16[128,3072]{1,0}
 value: <190 custom-call.8.0{} @0> (size=16,offset=0): (bf16[128,1024]{1,0}, s8[4194304]{0})
 value: <193 fusion.36 @0> (size=262144,offset=384): bf16[128,1024]{1,0}
 value: <194 custom-call.9.0{} @0> (size=16,offset=128): (bf16[128,4096]{1,0}, s8[4194304]{0})
 value: <202 tuple{} @0> (size=32,offset=1311360): (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 932847568 (889.63MiB)

Used values:
<11 x.51 @0>
 positions:
  x.51
 uses:
  add.39, operand 0
 from instruction: %x.51 = f32[] parameter(0)
<12 y.52 @0>
 positions:
  y.52
 uses:
  add.39, operand 1
 from instruction: %y.52 = f32[] parameter(1)
<13 add.39 @0>
 positions:
  add.39
 uses:
 from instruction: %add.39 = f32[] add(f32[] %x.51, f32[] %y.52)
<175 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.40, operand 1
  fusion.38, operand 1
  fusion.36, operand 2
 from instruction: %loop_gather_fusion = bf16[128,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[128]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<176 gemm_fusion_dot.7.0 @0>
 positions:
  gemm_fusion_dot.7.0
 uses:
  fusion.40, operand 2
  fusion.38, operand 2
  fusion.36, operand 3
 from instruction: %gemm_fusion_dot.7.0 = bf16[128,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3), kind=kCustom, calls=%gemm_fusion_dot.7_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"16","block_n":"64","block_k":"128","split_k":"1","num_stages":"1","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<177 fusion.40 @0>
 positions:
  fusion.40
 uses:
  custom-call.5.0, operand 0
 from instruction: %fusion.40 = bf16[128,1024]{1,0} fusion(f32[] %p.4, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,2048]{1,0} %gemm_fusion_dot.7.0, bf16[1024]{0} %p.5), kind=kCustom, calls=%fused_computation.31, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<178 custom-call.5.0{} @0>
 positions:
  custom-call.5.0 {}
 uses:
  get-tuple-element.5, operand 0 {}
 from instruction: %custom-call.5.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.40, bf16[6144,1024]{1,0} %p.6), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<179 custom-call.5.0{0} @0>
 positions:
  custom-call.5.0 {0}
  get-tuple-element.5
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.5.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.40, bf16[6144,1024]{1,0} %p.6), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<180 custom-call.5.0{1} @0>
 positions:
  custom-call.5.0 {1}
 uses:
 from instruction: %custom-call.5.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.40, bf16[6144,1024]{1,0} %p.6), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<181 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.6.0, operand 0
 from instruction: %loop_convert_fusion = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.5), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<182 custom-call.6.0{} @0>
 positions:
  custom-call.6.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.6.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.7), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<183 custom-call.6.0{0} @0>
 positions:
  custom-call.6.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.38, operand 3
  fusion.36, operand 4
 from instruction: %custom-call.6.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.7), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<184 custom-call.6.0{1} @0>
 positions:
  custom-call.6.0 {1}
 uses:
 from instruction: %custom-call.6.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.7), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<185 fusion.38 @0>
 positions:
  fusion.38
 uses:
  custom-call.7.0, operand 0
 from instruction: %fusion.38 = bf16[128,1024]{1,0} fusion(f32[] %p.4, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,2048]{1,0} %gemm_fusion_dot.7.0, bf16[128,1024]{1,0} %get-tuple-element.1.0, bf16[1024]{0} %p.8), kind=kCustom, calls=%fused_computation.29, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<186 custom-call.7.0{} @0>
 positions:
  custom-call.7.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.7.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.38, bf16[6144,1024]{1,0} %p.9), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<187 custom-call.7.0{0} @0>
 positions:
  custom-call.7.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.7.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.38, bf16[6144,1024]{1,0} %p.9), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<188 custom-call.7.0{1} @0>
 positions:
  custom-call.7.0 {1}
 uses:
 from instruction: %custom-call.7.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.38, bf16[6144,1024]{1,0} %p.9), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<189 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.8.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<190 custom-call.8.0{} @0>
 positions:
  custom-call.8.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.8.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.10), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<191 custom-call.8.0{0} @0>
 positions:
  custom-call.8.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.36, operand 1
 from instruction: %custom-call.8.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.10), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<192 custom-call.8.0{1} @0>
 positions:
  custom-call.8.0 {1}
 uses:
 from instruction: %custom-call.8.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.10), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<193 fusion.36 @0>
 positions:
  fusion.36
 uses:
  custom-call.9.0, operand 0
 from instruction: %fusion.36 = bf16[128,1024]{1,0} fusion(f32[] %p.4, bf16[128,1024]{1,0} %get-tuple-element.3.0, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,2048]{1,0} %gemm_fusion_dot.7.0, bf16[128,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[1024]{0} %p.11), kind=kCustom, calls=%fused_computation.27, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<194 custom-call.9.0{} @0>
 positions:
  custom-call.9.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.9.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.36, bf16[4096,1024]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<195 custom-call.9.0{0} @0>
 positions:
  custom-call.9.0 {0}
  get-tuple-element.4.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.4.0, operand 1
 from instruction: %custom-call.9.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.36, bf16[4096,1024]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<196 custom-call.9.0{1} @0>
 positions:
  custom-call.9.0 {1}
 uses:
 from instruction: %custom-call.9.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.36, bf16[4096,1024]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<197 triton_softmax.4.0 @0>
 positions:
  triton_softmax.4.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.4.0 = f32[128,8,128]{2,1,0} fusion(f32[] %p.4, bf16[128,4096]{1,0} %get-tuple-element.4.0), kind=kCustom, calls=%triton_softmax_computation.4, metadata={op_type="aten__mul" op_name="aten__mul.1562/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<198 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.292.0 {0}
  call {0}
  get-tuple-element.7
  tuple {0}
 uses:
  tuple, operand 0
  tuple.292.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[128,8,128]{2,1,0} fusion(f32[128,8,128]{2,1,0} %triton_softmax.4.0, bf16[128]{0} %p.13, bf16[40960,128]{1,0} %p.14, s32[128]{0} %p.15), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<199 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.292.0 {1}
  call {1}
  get-tuple-element.8
  bitcast.764.0
  tuple {1}
 uses:
  bitcast.764.0, operand 0
  tuple.292.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[128,1024]{1,0} fusion(bf16[128,4096]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<200 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.292.0 {3}
  call {2}
  get-tuple-element.9
  bitcast.776.0
  tuple {2}
 uses:
  bitcast.776.0, operand 0
  tuple.292.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.16), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<201 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.292.0 {2}
  bitcast.769.0
  call {3}
  get-tuple-element.10
  tuple {3}
 uses:
  tuple, operand 3
  tuple.292.0, operand 2
  bitcast.769.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.16), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<202 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.7, operand 0 {}
  get-tuple-element.8, operand 0 {}
  get-tuple-element.9, operand 0 {}
  get-tuple-element.10, operand 0 {}
 from instruction: %tuple = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %input_concatenate_fusion, bf16[128,8,128]{2,1,0} %bitcast.764.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.776.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<203 p5.18.0 @0>
 positions:
  p5.18.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.18.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<204 p4.16.0 @0>
 positions:
  p4.16.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.16.0 = s32[128]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<205 p10.94.0 @0>
 positions:
  p10.94.0
  p.2
 uses:
  call, operand 2
  gemm_fusion_dot.7.0, operand 0
 from instruction: %p10.94.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<206 p6.22.0 @0>
 positions:
  p6.22.0
  p.3
 uses:
  call, operand 3
  gemm_fusion_dot.7.0, operand 1
 from instruction: %p6.22.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<207 p1.4.0 @0>
 positions:
  p1.4.0
  p.4
 uses:
  call, operand 4
  fusion.40, operand 0
  fusion.38, operand 0
  fusion.36, operand 0
  triton_softmax.4.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<208 p9.42.0 @0>
 positions:
  p9.42.0
  p.5
 uses:
  call, operand 5
  fusion.40, operand 3
 from instruction: %p9.42.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<209 p8.40.0 @0>
 positions:
  p8.40.0
  p.6
 uses:
  call, operand 6
  custom-call.5.0, operand 1
 from instruction: %p8.40.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<210 p7.38.0 @0>
 positions:
  p7.38.0
  p.7
 uses:
  call, operand 7
  custom-call.6.0, operand 1
 from instruction: %p7.38.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<211 p13.114.0 @0>
 positions:
  p13.114.0
  p.8
 uses:
  call, operand 8
  fusion.38, operand 4
 from instruction: %p13.114.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<212 p12.112.0 @0>
 positions:
  p12.112.0
  p.9
 uses:
  call, operand 9
  custom-call.7.0, operand 1
 from instruction: %p12.112.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<213 p11.110.0 @0>
 positions:
  p11.110.0
  p.10
 uses:
  call, operand 10
  custom-call.8.0, operand 1
 from instruction: %p11.110.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<214 p3.8.0 @0>
 positions:
  p3.8.0
  p.11
 uses:
  call, operand 11
  fusion.36, operand 5
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<215 p2.6.0 @0>
 positions:
  p2.6.0
  p.12
 uses:
  call, operand 12
  custom-call.9.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<216 p0.1.0 @0>
 positions:
  p0.1.0
  p.13
 uses:
  call, operand 13
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<217 p15.239.0 @0>
 positions:
  p15.239.0
  p.14
 uses:
  call, operand 14
  input_concatenate_fusion, operand 2
 from instruction: %p15.239.0 = bf16[40960,128]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<218 p14.238.0 @0>
 positions:
  p14.238.0
  p.15
 uses:
  call, operand 15
  input_concatenate_fusion, operand 3
 from instruction: %p14.238.0 = s32[128]{0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<219 p16.283.0 @0>
 positions:
  p16.283.0
  p.16
 uses:
  call, operand 16
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p16.283.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<220 tuple.292.0{} @0>
 positions:
  tuple.292.0 {}
 uses:
 from instruction: %tuple.292.0 = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %get-tuple-element.7, bf16[128,8,128]{2,1,0} %get-tuple-element.8, bf16[4233,16,8,128]{3,2,1,0} %bitcast.769.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.9)


HloLiveRange (max 66):
  InstructionSequence:
    0:p1.4.0
    1:p16.283.0
    2:p15.239.0
    3:p14.238.0
    4:p13.114.0
    5:p12.112.0
    6:p11.110.0
    7:p10.94.0
    8:p9.42.0
    9:p8.40.0
    10:p7.38.0
    11:p6.22.0
    12:p5.18.0
    13:p4.16.0
    14:p3.8.0
    15:p2.6.0
    16:p0.1.0
    17:p
    18:p.1
    19:p.2
    20:p.3
    21:p.4
    22:p.5
    23:p.6
    24:p.7
    25:p.8
    26:p.9
    27:p.10
    28:p.11
    29:p.12
    30:p.13
    31:p.14
    32:p.15
    33:p.16
    34:loop_gather_fusion
    35:gemm_fusion_dot.7.0
    36:fusion.40
    37:custom-call.5.0
    38:get-tuple-element.5
    39:loop_convert_fusion
    40:custom-call.6.0
    41:get-tuple-element.1.0
    42:fusion.38
    43:custom-call.7.0
    44:get-tuple-element.2.0
    45:loop_convert_fusion.1
    46:custom-call.8.0
    47:get-tuple-element.3.0
    48:fusion.36
    49:custom-call.9.0
    50:get-tuple-element.4.0
    51:wrapped_slice
    52:triton_softmax.4.0
    53:input_concatenate_fusion
    54:bitcast.764.0
    55:loop_slice_fusion
    56:bitcast.776.0
    57:wrapped_slice.1
    58:tuple
    59:call
    60:get-tuple-element.7
    61:get-tuple-element.8
    62:get-tuple-element.9
    63:get-tuple-element.10
    64:bitcast.769.0
    65:tuple.292.0
  BufferLiveRange:
    loop_gather_fusion{}:34-48
    gemm_fusion_dot.7.0{}:35-48
    fusion.40{}:36-37
    custom-call.5.0{}:37-38
    custom-call.5.0{0}:37-39
    custom-call.5.0{1}:37-37
    loop_convert_fusion{}:39-40
    custom-call.6.0{}:40-41
    custom-call.6.0{0}:40-48
    custom-call.6.0{1}:40-40
    fusion.38{}:42-43
    custom-call.7.0{}:43-44
    custom-call.7.0{0}:43-45
    custom-call.7.0{1}:43-43
    loop_convert_fusion.1{}:45-46
    custom-call.8.0{}:46-47
    custom-call.8.0{0}:46-48
    custom-call.8.0{1}:46-46
    fusion.36{}:48-49
    custom-call.9.0{}:49-50
    custom-call.9.0{0}:49-52
    custom-call.9.0{1}:49-49
    triton_softmax.4.0{}:52-53
    input_concatenate_fusion{}:53-66
    wrapped_slice{}:51-66
    loop_slice_fusion{}:55-66
    wrapped_slice.1{}:57-66
    tuple{}:58-63
    p5.18.0{}:0-66
    p4.16.0{}:0-66
    p10.94.0{}:0-66
    p6.22.0{}:0-66
    p1.4.0{}:0-66
    p9.42.0{}:0-66
    p8.40.0{}:0-66
    p7.38.0{}:0-66
    p13.114.0{}:0-66
    p12.112.0{}:0-66
    p11.110.0{}:0-66
    p3.8.0{}:0-66
    p2.6.0{}:0-66
    p0.1.0{}:0-66
    p15.239.0{}:0-66
    p14.238.0{}:0-66
    p16.283.0{}:0-66
    tuple.292.0{}:65-66
  Live ranges at 58 (peak):
    input_concatenate_fusion: 262144 bytes
    wrapped_slice: 262144 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.18.0: 311164928 bytes
    p4.16.0: 512 bytes
    p10.94.0: 4194304 bytes
    p6.22.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.42.0: 2048 bytes
    p8.40.0: 12582912 bytes
    p7.38.0: 6291456 bytes
    p13.114.0: 2048 bytes
    p12.112.0: 12582912 bytes
    p11.110.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p15.239.0: 10485760 bytes
    p14.238.0: 512 bytes
    p16.283.0: 277413888 bytes
