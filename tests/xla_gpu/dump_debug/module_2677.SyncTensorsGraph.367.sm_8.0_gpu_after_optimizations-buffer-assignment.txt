BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <267 p5.20.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 20, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <287 p20.357.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <235 custom-call.7.0{0} @0> (size=786432,offset=0): bf16[64,6144]{1,0}
 value: <243 custom-call.9.0{0} @0> (size=786432,offset=0): bf16[64,6144]{1,0}
 value: <251 custom-call.11.0{0} @0> (size=786432,offset=0): bf16[64,6144]{1,0}
 value: <255 custom-call.12.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <259 custom-call.13.0{0} @0> (size=524288,offset=0): bf16[64,4096]{1,0}
 value: <264 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <236 custom-call.7.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <240 custom-call.8.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <244 custom-call.9.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <248 custom-call.10.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <252 custom-call.11.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <256 custom-call.12.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <260 custom-call.13.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <261 triton_softmax.5.0 @0> (size=262144,offset=0): f32[64,8,128]{2,1,0}
 value: <265 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <274 p8.42.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <277 p12.114.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <280 p16.186.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 10485760, parameter 19, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <285 p19.313.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 8: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <283 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 9: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <275 p7.40.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 10: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <278 p11.112.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 11: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <281 p15.184.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 12: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <269 p14.168.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 13: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <270 p10.96.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 14: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <271 p6.24.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 15: size 131072, maybe-live-out:
 value: <233 fusion.51 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <239 custom-call.8.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <262 input_concatenate_fusion @0> (size=131072,offset=0): bf16[64,8,128]{2,1,0}
allocation 16: size 131072, maybe-live-out:
 value: <231 loop_gather_fusion @0> (size=131072,offset=0): bf16[64,1,1024]{2,0,1}
 value: <263 wrapped_slice @0> (size=131072,offset=0): bf16[64,1024]{1,0}
allocation 17: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <273 p9.44.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 18: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <276 p13.116.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 19: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <279 p17.188.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 20: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <282 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 21: size 256, parameter 4, shape |s32[64]| at ShapeIndex {}:
 value: <268 p4.18.0 @0> (size=256,offset=0): s32[64]{0}
allocation 22: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <284 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 23: size 256, parameter 18, shape |s32[64]| at ShapeIndex {}:
 value: <286 p18.312.0 @0> (size=256,offset=0): s32[64]{0}
allocation 24: size 32, output shape is |(bf16[64,8,128], bf16[64,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <288 tuple.366.0{} @0> (size=32,offset=0): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 25: size 4, thread-local:
 value: <14 add.51 @0> (size=4,offset=0): f32[]
allocation 26: size 4, thread-local:
 value: <13 y.54 @0> (size=4,offset=0): f32[]
allocation 27: size 4, thread-local:
 value: <12 x.53 @0> (size=4,offset=0): f32[]
allocation 28: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <272 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 29: size 918432, preallocated-temp:
 value: <232 gemm_fusion_dot.11.0 @0> (size=393216,offset=640): bf16[64,3072]{1,0}
 value: <234 custom-call.7.0{} @0> (size=16,offset=918272): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <237 loop_convert_fusion.2 @0> (size=393216,offset=393856): bf16[64,3072]{1,0}
 value: <238 custom-call.8.0{} @0> (size=16,offset=918144): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <241 fusion.50 @0> (size=131072,offset=393856): bf16[64,1024]{1,0}
 value: <242 custom-call.9.0{} @0> (size=16,offset=512): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <245 loop_convert_fusion.1 @0> (size=393216,offset=393856): bf16[64,3072]{1,0}
 value: <246 custom-call.10.0{} @0> (size=16,offset=0): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <247 custom-call.10.0{0} @0> (size=131072,offset=787072): bf16[64,1024]{1,0}
 value: <249 fusion.49 @0> (size=131072,offset=393856): bf16[64,1024]{1,0}
 value: <250 custom-call.11.0{} @0> (size=16,offset=128): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <253 loop_convert_fusion @0> (size=393216,offset=393856): bf16[64,3072]{1,0}
 value: <254 custom-call.12.0{} @0> (size=16,offset=256): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <257 fusion.47 @0> (size=131072,offset=393856): bf16[64,1024]{1,0}
 value: <258 custom-call.13.0{} @0> (size=16,offset=384): (bf16[64,4096]{1,0}, s8[4194304]{0})
 value: <266 tuple{} @0> (size=32,offset=918400): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 955262672 (911.01MiB)

Used values:
<12 x.53 @0>
 positions:
  x.53
 uses:
  add.51, operand 0
 from instruction: %x.53 = f32[] parameter(0)
<13 y.54 @0>
 positions:
  y.54
 uses:
  add.51, operand 1
 from instruction: %y.54 = f32[] parameter(1)
<14 add.51 @0>
 positions:
  add.51
 uses:
 from instruction: %add.51 = f32[] add(f32[] %x.53, f32[] %y.54)
<231 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.51, operand 1
  fusion.50, operand 2
  fusion.49, operand 4
  fusion.47, operand 5
 from instruction: %loop_gather_fusion = bf16[64,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[64]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<232 gemm_fusion_dot.11.0 @0>
 positions:
  gemm_fusion_dot.11.0
 uses:
  fusion.51, operand 2
  fusion.50, operand 3
  fusion.49, operand 2
  fusion.47, operand 3
 from instruction: %gemm_fusion_dot.11.0 = bf16[64,3072]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4), kind=kCustom, calls=%gemm_fusion_dot.11_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"16","block_n":"64","block_k":"128","split_k":"1","num_stages":"1","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<233 fusion.51 @0>
 positions:
  fusion.51
 uses:
  custom-call.7.0, operand 0
 from instruction: %fusion.51 = bf16[64,1024]{1,0} fusion(f32[] %p.5, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,3072]{1,0} %gemm_fusion_dot.11.0, bf16[1024]{0} %p.6), kind=kCustom, calls=%fused_computation.40, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<234 custom-call.7.0{} @0>
 positions:
  custom-call.7.0 {}
 uses:
  get-tuple-element.7, operand 0 {}
 from instruction: %custom-call.7.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.51, bf16[6144,1024]{1,0} %p.7), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<235 custom-call.7.0{0} @0>
 positions:
  custom-call.7.0 {0}
  get-tuple-element.7
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.7.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.51, bf16[6144,1024]{1,0} %p.7), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<236 custom-call.7.0{1} @0>
 positions:
  custom-call.7.0 {1}
 uses:
 from instruction: %custom-call.7.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.51, bf16[6144,1024]{1,0} %p.7), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<237 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.8.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.7), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<238 custom-call.8.0{} @0>
 positions:
  custom-call.8.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.8.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.8), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<239 custom-call.8.0{0} @0>
 positions:
  custom-call.8.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.50, operand 4
  fusion.49, operand 5
  fusion.47, operand 6
 from instruction: %custom-call.8.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.8), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<240 custom-call.8.0{1} @0>
 positions:
  custom-call.8.0 {1}
 uses:
 from instruction: %custom-call.8.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.8), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<241 fusion.50 @0>
 positions:
  fusion.50
 uses:
  custom-call.9.0, operand 0
 from instruction: %fusion.50 = bf16[64,1024]{1,0} fusion(f32[] %p.5, bf16[1024]{0} %p.9, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,3072]{1,0} %gemm_fusion_dot.11.0, bf16[64,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.39, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<242 custom-call.9.0{} @0>
 positions:
  custom-call.9.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.9.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.50, bf16[6144,1024]{1,0} %p.10), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<243 custom-call.9.0{0} @0>
 positions:
  custom-call.9.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.9.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.50, bf16[6144,1024]{1,0} %p.10), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<244 custom-call.9.0{1} @0>
 positions:
  custom-call.9.0 {1}
 uses:
 from instruction: %custom-call.9.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.50, bf16[6144,1024]{1,0} %p.10), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<245 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.10.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<246 custom-call.10.0{} @0>
 positions:
  custom-call.10.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.10.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<247 custom-call.10.0{0} @0>
 positions:
  custom-call.10.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.49, operand 1
  fusion.47, operand 2
 from instruction: %custom-call.10.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<248 custom-call.10.0{1} @0>
 positions:
  custom-call.10.0 {1}
 uses:
 from instruction: %custom-call.10.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<249 fusion.49 @0>
 positions:
  fusion.49
 uses:
  custom-call.11.0, operand 0
 from instruction: %fusion.49 = bf16[64,1024]{1,0} fusion(f32[] %p.5, bf16[64,1024]{1,0} %get-tuple-element.3.0, bf16[64,3072]{1,0} %gemm_fusion_dot.11.0, bf16[1024]{0} %p.12, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.38, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<250 custom-call.11.0{} @0>
 positions:
  custom-call.11.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.11.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.49, bf16[6144,1024]{1,0} %p.13), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<251 custom-call.11.0{0} @0>
 positions:
  custom-call.11.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.11.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.49, bf16[6144,1024]{1,0} %p.13), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<252 custom-call.11.0{1} @0>
 positions:
  custom-call.11.0 {1}
 uses:
 from instruction: %custom-call.11.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.49, bf16[6144,1024]{1,0} %p.13), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<253 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.12.0, operand 0
 from instruction: %loop_convert_fusion = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<254 custom-call.12.0{} @0>
 positions:
  custom-call.12.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.12.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<255 custom-call.12.0{0} @0>
 positions:
  custom-call.12.0 {0}
  get-tuple-element.5.0
 uses:
  fusion.47, operand 1
 from instruction: %custom-call.12.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<256 custom-call.12.0{1} @0>
 positions:
  custom-call.12.0 {1}
 uses:
 from instruction: %custom-call.12.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<257 fusion.47 @0>
 positions:
  fusion.47
 uses:
  custom-call.13.0, operand 0
 from instruction: %fusion.47 = bf16[64,1024]{1,0} fusion(f32[] %p.5, bf16[64,1024]{1,0} %get-tuple-element.5.0, bf16[64,1024]{1,0} %get-tuple-element.3.0, bf16[64,3072]{1,0} %gemm_fusion_dot.11.0, bf16[1024]{0} %p.15, /*index=5*/bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.36, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<258 custom-call.13.0{} @0>
 positions:
  custom-call.13.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.13.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.47, bf16[4096,1024]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<259 custom-call.13.0{0} @0>
 positions:
  custom-call.13.0 {0}
  get-tuple-element.6.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.5.0, operand 1
 from instruction: %custom-call.13.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.47, bf16[4096,1024]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<260 custom-call.13.0{1} @0>
 positions:
  custom-call.13.0 {1}
 uses:
 from instruction: %custom-call.13.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.47, bf16[4096,1024]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<261 triton_softmax.5.0 @0>
 positions:
  triton_softmax.5.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.5.0 = f32[64,8,128]{2,1,0} fusion(f32[] %p.5, bf16[64,4096]{1,0} %get-tuple-element.6.0), kind=kCustom, calls=%triton_softmax_computation.5, metadata={op_type="aten__mul" op_name="aten__mul.1073/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<262 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.366.0 {0}
  call {0}
  get-tuple-element.9
  tuple {0}
 uses:
  tuple, operand 0
  tuple.366.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[64,8,128]{2,1,0} fusion(f32[64,8,128]{2,1,0} %triton_softmax.5.0, bf16[128]{0} %p.17, bf16[40960,128]{1,0} %p.18, s32[64]{0} %p.19), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<263 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.366.0 {1}
  call {1}
  get-tuple-element.10
  bitcast.950.0
  tuple {1}
 uses:
  bitcast.950.0, operand 0
  tuple.366.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[64,1024]{1,0} fusion(bf16[64,4096]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<264 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.366.0 {3}
  call {2}
  get-tuple-element.11
  bitcast.962.0
  tuple {2}
 uses:
  bitcast.962.0, operand 0
  tuple.366.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.20), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<265 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.366.0 {2}
  bitcast.955.0
  call {3}
  get-tuple-element.12
  tuple {3}
 uses:
  tuple, operand 3
  tuple.366.0, operand 2
  bitcast.955.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.20), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<266 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.9, operand 0 {}
  get-tuple-element.10, operand 0 {}
  get-tuple-element.11, operand 0 {}
  get-tuple-element.12, operand 0 {}
 from instruction: %tuple = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %input_concatenate_fusion, bf16[64,8,128]{2,1,0} %bitcast.950.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.962.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<267 p5.20.0 @0>
 positions:
  p5.20.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.20.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<268 p4.18.0 @0>
 positions:
  p4.18.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.18.0 = s32[64]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<269 p14.168.0 @0>
 positions:
  p14.168.0
  p.2
 uses:
  call, operand 2
  gemm_fusion_dot.11.0, operand 0
 from instruction: %p14.168.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<270 p10.96.0 @0>
 positions:
  p10.96.0
  p.3
 uses:
  call, operand 3
  gemm_fusion_dot.11.0, operand 1
 from instruction: %p10.96.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<271 p6.24.0 @0>
 positions:
  p6.24.0
  p.4
 uses:
  call, operand 4
  gemm_fusion_dot.11.0, operand 2
 from instruction: %p6.24.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<272 p1.4.0 @0>
 positions:
  p1.4.0
  p.5
 uses:
  call, operand 5
  fusion.51, operand 0
  fusion.50, operand 0
  fusion.49, operand 0
  fusion.47, operand 0
  triton_softmax.5.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<273 p9.44.0 @0>
 positions:
  p9.44.0
  p.6
 uses:
  call, operand 6
  fusion.51, operand 3
 from instruction: %p9.44.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<274 p8.42.0 @0>
 positions:
  p8.42.0
  p.7
 uses:
  call, operand 7
  custom-call.7.0, operand 1
 from instruction: %p8.42.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<275 p7.40.0 @0>
 positions:
  p7.40.0
  p.8
 uses:
  call, operand 8
  custom-call.8.0, operand 1
 from instruction: %p7.40.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<276 p13.116.0 @0>
 positions:
  p13.116.0
  p.9
 uses:
  call, operand 9
  fusion.50, operand 1
 from instruction: %p13.116.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<277 p12.114.0 @0>
 positions:
  p12.114.0
  p.10
 uses:
  call, operand 10
  custom-call.9.0, operand 1
 from instruction: %p12.114.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<278 p11.112.0 @0>
 positions:
  p11.112.0
  p.11
 uses:
  call, operand 11
  custom-call.10.0, operand 1
 from instruction: %p11.112.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<279 p17.188.0 @0>
 positions:
  p17.188.0
  p.12
 uses:
  call, operand 12
  fusion.49, operand 3
 from instruction: %p17.188.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<280 p16.186.0 @0>
 positions:
  p16.186.0
  p.13
 uses:
  call, operand 13
  custom-call.11.0, operand 1
 from instruction: %p16.186.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<281 p15.184.0 @0>
 positions:
  p15.184.0
  p.14
 uses:
  call, operand 14
  custom-call.12.0, operand 1
 from instruction: %p15.184.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<282 p3.8.0 @0>
 positions:
  p3.8.0
  p.15
 uses:
  call, operand 15
  fusion.47, operand 4
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<283 p2.6.0 @0>
 positions:
  p2.6.0
  p.16
 uses:
  call, operand 16
  custom-call.13.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<284 p0.1.0 @0>
 positions:
  p0.1.0
  p.17
 uses:
  call, operand 17
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<285 p19.313.0 @0>
 positions:
  p19.313.0
  p.18
 uses:
  call, operand 18
  input_concatenate_fusion, operand 2
 from instruction: %p19.313.0 = bf16[40960,128]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<286 p18.312.0 @0>
 positions:
  p18.312.0
  p.19
 uses:
  call, operand 19
  input_concatenate_fusion, operand 3
 from instruction: %p18.312.0 = s32[64]{0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<287 p20.357.0 @0>
 positions:
  p20.357.0
  p.20
 uses:
  call, operand 20
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p20.357.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<288 tuple.366.0{} @0>
 positions:
  tuple.366.0 {}
 uses:
 from instruction: %tuple.366.0 = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %get-tuple-element.9, bf16[64,8,128]{2,1,0} %get-tuple-element.10, bf16[4233,16,8,128]{3,2,1,0} %bitcast.955.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.11)


HloLiveRange (max 80):
  InstructionSequence:
    0:p1.4.0
    1:p20.357.0
    2:p19.313.0
    3:p18.312.0
    4:p17.188.0
    5:p16.186.0
    6:p15.184.0
    7:p14.168.0
    8:p13.116.0
    9:p12.114.0
    10:p11.112.0
    11:p10.96.0
    12:p9.44.0
    13:p8.42.0
    14:p7.40.0
    15:p6.24.0
    16:p5.20.0
    17:p4.18.0
    18:p3.8.0
    19:p2.6.0
    20:p0.1.0
    21:p
    22:p.1
    23:p.2
    24:p.3
    25:p.4
    26:p.5
    27:p.6
    28:p.7
    29:p.8
    30:p.9
    31:p.10
    32:p.11
    33:p.12
    34:p.13
    35:p.14
    36:p.15
    37:p.16
    38:p.17
    39:p.18
    40:p.19
    41:p.20
    42:loop_gather_fusion
    43:gemm_fusion_dot.11.0
    44:fusion.51
    45:custom-call.7.0
    46:get-tuple-element.7
    47:loop_convert_fusion.2
    48:custom-call.8.0
    49:get-tuple-element.1.0
    50:fusion.50
    51:custom-call.9.0
    52:get-tuple-element.2.0
    53:loop_convert_fusion.1
    54:custom-call.10.0
    55:get-tuple-element.3.0
    56:fusion.49
    57:custom-call.11.0
    58:get-tuple-element.4.0
    59:loop_convert_fusion
    60:custom-call.12.0
    61:get-tuple-element.5.0
    62:fusion.47
    63:custom-call.13.0
    64:get-tuple-element.6.0
    65:wrapped_slice
    66:triton_softmax.5.0
    67:input_concatenate_fusion
    68:bitcast.950.0
    69:loop_slice_fusion
    70:bitcast.962.0
    71:wrapped_slice.1
    72:tuple
    73:call
    74:get-tuple-element.9
    75:get-tuple-element.10
    76:get-tuple-element.11
    77:get-tuple-element.12
    78:bitcast.955.0
    79:tuple.366.0
  BufferLiveRange:
    loop_gather_fusion{}:42-62
    gemm_fusion_dot.11.0{}:43-62
    fusion.51{}:44-45
    custom-call.7.0{}:45-46
    custom-call.7.0{0}:45-47
    custom-call.7.0{1}:45-45
    loop_convert_fusion.2{}:47-48
    custom-call.8.0{}:48-49
    custom-call.8.0{0}:48-62
    custom-call.8.0{1}:48-48
    fusion.50{}:50-51
    custom-call.9.0{}:51-52
    custom-call.9.0{0}:51-53
    custom-call.9.0{1}:51-51
    loop_convert_fusion.1{}:53-54
    custom-call.10.0{}:54-55
    custom-call.10.0{0}:54-62
    custom-call.10.0{1}:54-54
    fusion.49{}:56-57
    custom-call.11.0{}:57-58
    custom-call.11.0{0}:57-59
    custom-call.11.0{1}:57-57
    loop_convert_fusion{}:59-60
    custom-call.12.0{}:60-61
    custom-call.12.0{0}:60-62
    custom-call.12.0{1}:60-60
    fusion.47{}:62-63
    custom-call.13.0{}:63-64
    custom-call.13.0{0}:63-66
    custom-call.13.0{1}:63-63
    triton_softmax.5.0{}:66-67
    input_concatenate_fusion{}:67-80
    wrapped_slice{}:65-80
    loop_slice_fusion{}:69-80
    wrapped_slice.1{}:71-80
    tuple{}:72-77
    p5.20.0{}:0-80
    p4.18.0{}:0-80
    p14.168.0{}:0-80
    p10.96.0{}:0-80
    p6.24.0{}:0-80
    p1.4.0{}:0-80
    p9.44.0{}:0-80
    p8.42.0{}:0-80
    p7.40.0{}:0-80
    p13.116.0{}:0-80
    p12.114.0{}:0-80
    p11.112.0{}:0-80
    p17.188.0{}:0-80
    p16.186.0{}:0-80
    p15.184.0{}:0-80
    p3.8.0{}:0-80
    p2.6.0{}:0-80
    p0.1.0{}:0-80
    p19.313.0{}:0-80
    p18.312.0{}:0-80
    p20.357.0{}:0-80
    tuple.366.0{}:79-80
  Live ranges at 72 (peak):
    input_concatenate_fusion: 131072 bytes
    wrapped_slice: 131072 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.20.0: 311164928 bytes
    p4.18.0: 256 bytes
    p14.168.0: 4194304 bytes
    p10.96.0: 4194304 bytes
    p6.24.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.44.0: 2048 bytes
    p8.42.0: 12582912 bytes
    p7.40.0: 6291456 bytes
    p13.116.0: 2048 bytes
    p12.114.0: 12582912 bytes
    p11.112.0: 6291456 bytes
    p17.188.0: 2048 bytes
    p16.186.0: 12582912 bytes
    p15.184.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p19.313.0: 10485760 bytes
    p18.312.0: 256 bytes
    p20.357.0: 277413888 bytes
