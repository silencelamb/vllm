HloModule SyncTensorsGraph.663, is_scheduled=true, entry_computation_layout={(bf16[128]{0}, f32[], bf16[4096,1024]{1,0}, bf16[1024]{0}, s32[128]{0}, /*index=5*/bf16[151936,1024]{1,0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=10*/bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, /*index=15*/bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, /*index=20*/bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, /*index=25*/bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=30*/bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, s32[128]{0}, /*index=35*/bf16[40960,128]{1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0})->(bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})}, frontend_attributes={fingerprint_before_lhs="7258372c28ea3ab7551cbc9a8c6fecb0"}

%fused_gather (param_0.9: bf16[151936,1024], param_1.324: s32[128]) -> bf16[128,1,1024] {
  %param_0.9 = bf16[151936,1024]{1,0} parameter(0)
  %param_1.324 = s32[128]{0} parameter(1)
  %convert.388.3 = s64[128]{0} convert(s32[128]{0} %param_1.324), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.389.3 = u32[128]{0} convert(s64[128]{0} %convert.388.3), metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.1308.1 = u32[128,1]{1,0} bitcast(u32[128]{0} %convert.389.3)
  ROOT %gather.3 = bf16[128,1,1024]{2,0,1} gather(bf16[151936,1024]{1,0} %param_0.9, u32[128,1]{1,0} %bitcast.1308.1), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,1024}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_concatenate_computation (param_0.397: bf16[1024,2048], param_1.336: bf16[1024,2048], param_2.208: bf16[1024,2048], param_3.178: bf16[1024,2048], param_4.102: bf16[1024,2048], param_5.46: bf16[1024,2048], param_6.31: bf16[1024,2048]) -> bf16[7168,2048] {
  %param_0.397 = bf16[1024,2048]{1,0} parameter(0)
  %param_1.336 = bf16[1024,2048]{1,0} parameter(1)
  %param_2.208 = bf16[1024,2048]{1,0} parameter(2)
  %param_3.178 = bf16[1024,2048]{1,0} parameter(3)
  %param_4.102 = bf16[1024,2048]{1,0} parameter(4)
  %param_5.46 = bf16[1024,2048]{1,0} parameter(5)
  %param_6.31 = bf16[1024,2048]{1,0} parameter(6)
  ROOT %concatenate.17.1 = bf16[7168,2048]{1,0} concatenate(bf16[1024,2048]{1,0} %param_0.397, bf16[1024,2048]{1,0} %param_1.336, bf16[1024,2048]{1,0} %param_2.208, bf16[1024,2048]{1,0} %param_3.178, bf16[1024,2048]{1,0} %param_4.102, /*index=5*/bf16[1024,2048]{1,0} %param_5.46, bf16[1024,2048]{1,0} %param_6.31), dimensions={0}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
}

%gemm_fusion_dot.27_computation (parameter_0: bf16[7168,2048]) -> bf16[128,7168] {
  %constant_84 = bf16[] constant(0), metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.73 = bf16[128,2048]{1,0} broadcast(bf16[] %constant_84), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %parameter_0 = bf16[7168,2048]{1,0} parameter(0)
  ROOT %dot.28 = bf16[128,7168]{1,0} dot(bf16[128,2048]{1,0} %broadcast.73, bf16[7168,2048]{1,0} %parameter_0), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%AddComputation.60 (x.61: f32[], y.62: f32[]) -> f32[] {
  %y.62 = f32[] parameter(1)
  %x.61 = f32[] parameter(0)
  ROOT %add.102 = f32[] add(f32[] %x.61, f32[] %y.62)
}

%fused_computation.91 (param_0.382: f32[], param_1.320: bf16[128,1,1024], param_2.192: bf16[128,7168], param_3.152: bf16[1024]) -> bf16[128,1024] {
  %param_2.192 = bf16[128,7168]{1,0} parameter(2)
  %convert.387.27 = f32[128,7168]{1,0} convert(bf16[128,7168]{1,0} %param_2.192), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.107.9 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.27), slice={[0:128], [6144:7168]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.320 = bf16[128,1,1024]{2,0,1} parameter(1)
  %bitcast.1311.9 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_1.320)
  %convert.392.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1311.9), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.103.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.107.9, f32[128,1024]{1,0} %convert.392.9), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.222 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.103.7, f32[128,1024]{1,0} %add.103.7), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_205 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.19 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.222, f32[] %constant_205), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_204 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.255 = f32[128]{0} broadcast(f32[] %constant_204), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.221 = f32[128]{0} multiply(f32[128]{0} %reduce.19, f32[128]{0} %broadcast.255), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.382 = f32[] parameter(0)
  %broadcast.253 = f32[128]{0} broadcast(f32[] %param_0.382), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.127 = f32[128]{0} add(f32[128]{0} %multiply.221, f32[128]{0} %broadcast.253), metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.46 = f32[128]{0} rsqrt(f32[128]{0} %add.127), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.251 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.46), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1536/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.219 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.103.7, f32[128,1024]{1,0} %broadcast.251), metadata={op_type="aten__mul" op_name="aten__mul.1536/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.152 = bf16[1024]{0} parameter(3)
  %convert.393.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.152), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.189.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.393.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1537/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.187.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.219, f32[128,1024]{1,0} %broadcast.189.1), metadata={op_type="aten__mul" op_name="aten__mul.1537/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.394.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.187.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert (param_0.327: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.327 = bf16[128,6144]{1,0} parameter(0)
  %slice.108.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.327), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.395.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.108.1)
  %constant_1_4 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.496.2 = f32[] convert(bf16[] %constant_1_4)
  %broadcast.217.56 = f32[128,3072]{1,0} broadcast(f32[] %convert.496.2), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.21.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.395.8)
  %convert.399.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.21.7)
  %exponential.21.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.399.5)
  %convert.400.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.21.3)
  %add.104.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.217.56, f32[128,3072]{1,0} %convert.400.1)
  %divide.21.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.217.56, f32[128,3072]{1,0} %add.104.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.189.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.395.8, f32[128,3072]{1,0} %divide.21.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.109.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.327), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.401.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.109.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.190.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.189.5, f32[128,3072]{1,0} %convert.401.1), metadata={op_type="aten__mul" op_name="aten__mul.1538/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.403.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.190.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.89 (param_0.381: f32[], param_1.319: bf16[128,1,1024], param_2.191: bf16[128,7168], param_3.151: bf16[128,1024], param_4.81: bf16[1024]) -> bf16[128,1024] {
  %param_2.191 = bf16[128,7168]{1,0} parameter(2)
  %convert.387.39 = f32[128,7168]{1,0} convert(bf16[128,7168]{1,0} %param_2.191), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.106.5 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.39), slice={[0:128], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.151 = bf16[128,1024]{1,0} parameter(3)
  %convert.404.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_3.151), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.107.11 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.39), slice={[0:128], [6144:7168]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.319 = bf16[128,1,1024]{2,0,1} parameter(1)
  %bitcast.1311.11 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_1.319)
  %convert.392.11 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1311.11), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.103.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.107.11, f32[128,1024]{1,0} %convert.392.11), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.105.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.404.5, f32[128,1024]{1,0} %add.103.9), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.106.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.106.5, f32[128,1024]{1,0} %add.105.5), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.229 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.106.3, f32[128,1024]{1,0} %add.106.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_213 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.21 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.229, f32[] %constant_213), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_212 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.262 = f32[128]{0} broadcast(f32[] %constant_212), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.228 = f32[128]{0} multiply(f32[128]{0} %reduce.21, f32[128]{0} %broadcast.262), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.381 = f32[] parameter(0)
  %broadcast.261 = f32[128]{0} broadcast(f32[] %param_0.381), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.129 = f32[128]{0} add(f32[128]{0} %multiply.228, f32[128]{0} %broadcast.261), metadata={op_type="aten__add" op_name="aten__add.1553/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.48 = f32[128]{0} rsqrt(f32[128]{0} %add.129), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.260 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.48), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1554/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.227 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.106.3, f32[128,1024]{1,0} %broadcast.260), metadata={op_type="aten__mul" op_name="aten__mul.1554/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.81 = bf16[1024]{0} parameter(4)
  %convert.405.1 = f32[1024]{0} convert(bf16[1024]{0} %param_4.81), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.192.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.405.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1555/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.191.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.227, f32[128,1024]{1,0} %broadcast.192.1), metadata={op_type="aten__mul" op_name="aten__mul.1555/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.408.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.191.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.1 (param_0.329: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.329 = bf16[128,6144]{1,0} parameter(0)
  %slice.110.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.329), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.409.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.110.1)
  %constant_1_6 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.496.4 = f32[] convert(bf16[] %constant_1_6)
  %broadcast.217.52 = f32[128,3072]{1,0} broadcast(f32[] %convert.496.4), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.22.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.409.8)
  %convert.414.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.22.7)
  %exponential.22.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.414.5)
  %convert.415.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.22.3)
  %add.107.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.217.52, f32[128,3072]{1,0} %convert.415.1)
  %divide.22.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.217.52, f32[128,3072]{1,0} %add.107.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.192.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.409.8, f32[128,3072]{1,0} %divide.22.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.111.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.329), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.416.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.111.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.193.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.192.5, f32[128,3072]{1,0} %convert.416.1), metadata={op_type="aten__mul" op_name="aten__mul.1556/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.417.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.193.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.87 (param_0.394: f32[], param_1.332: bf16[1024], param_2.203: bf16[128,1,1024], param_3.172: bf16[128,7168], param_4.95: bf16[128,1024], param_5.36: bf16[128,1024]) -> bf16[128,1024] {
  %param_3.172 = bf16[128,7168]{1,0} parameter(3)
  %convert.387.57 = f32[128,7168]{1,0} convert(bf16[128,7168]{1,0} %param_3.172), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.105.5 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.57), slice={[0:128], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.36 = bf16[128,1024]{1,0} parameter(5)
  %convert.418.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_5.36), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.106.9 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.57), slice={[0:128], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.95 = bf16[128,1024]{1,0} parameter(4)
  %convert.404.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_4.95), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.107.15 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.57), slice={[0:128], [6144:7168]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.203 = bf16[128,1,1024]{2,0,1} parameter(2)
  %bitcast.1311.15 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_2.203)
  %convert.392.15 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1311.15), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.103.13 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.107.15, f32[128,1024]{1,0} %convert.392.15), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.105.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.404.9, f32[128,1024]{1,0} %add.103.13), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.106.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.106.9, f32[128,1024]{1,0} %add.105.9), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.108.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.418.5, f32[128,1024]{1,0} %add.106.7), metadata={op_type="aten__add" op_name="aten__add.1557/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.109.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.105.5, f32[128,1024]{1,0} %add.108.5), metadata={op_type="aten__add" op_name="aten__add.1570/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.236 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.109.3, f32[128,1024]{1,0} %add.109.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_218 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.23 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.236, f32[] %constant_218), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_217 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.268 = f32[128]{0} broadcast(f32[] %constant_217), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.235 = f32[128]{0} multiply(f32[128]{0} %reduce.23, f32[128]{0} %broadcast.268), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.394 = f32[] parameter(0)
  %broadcast.267 = f32[128]{0} broadcast(f32[] %param_0.394), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.131 = f32[128]{0} add(f32[128]{0} %multiply.235, f32[128]{0} %broadcast.267), metadata={op_type="aten__add" op_name="aten__add.1571/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.50 = f32[128]{0} rsqrt(f32[128]{0} %add.131), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.266 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.50), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1572/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.234 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.109.3, f32[128,1024]{1,0} %broadcast.266), metadata={op_type="aten__mul" op_name="aten__mul.1572/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.332 = bf16[1024]{0} parameter(1)
  %convert.419.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.332), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.195.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.419.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1573/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.194.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.234, f32[128,1024]{1,0} %broadcast.195.1), metadata={op_type="aten__mul" op_name="aten__mul.1573/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.420.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.194.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.2 (param_0.331: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.331 = bf16[128,6144]{1,0} parameter(0)
  %slice.112.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.331), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.421.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.112.1)
  %constant_1_8 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.496.6 = f32[] convert(bf16[] %constant_1_8)
  %broadcast.217.48 = f32[128,3072]{1,0} broadcast(f32[] %convert.496.6), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.23.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.421.8)
  %convert.425.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.23.7)
  %exponential.23.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.425.5)
  %convert.426.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.23.3)
  %add.110.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.217.48, f32[128,3072]{1,0} %convert.426.1)
  %divide.23.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.217.48, f32[128,3072]{1,0} %add.110.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.195.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.421.8, f32[128,3072]{1,0} %divide.23.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.113.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.331), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.427.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.113.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.196.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.195.5, f32[128,3072]{1,0} %convert.427.1), metadata={op_type="aten__mul" op_name="aten__mul.1574/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.429.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.196.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.85 (param_0.379: f32[], param_1.335: bf16[128,1024], param_2.207: bf16[128,7168], param_3.177: bf16[1024], param_4.101: bf16[128,1,1024], param_5.45: bf16[128,1024], param_6.30: bf16[128,1024]) -> bf16[128,1024] {
  %param_2.207 = bf16[128,7168]{1,0} parameter(2)
  %convert.387.33 = f32[128,7168]{1,0} convert(bf16[128,7168]{1,0} %param_2.207), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.104.5 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.33), slice={[0:128], [3072:4096]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.335 = bf16[128,1024]{1,0} parameter(1)
  %convert.430.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_1.335), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.105.9 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.33), slice={[0:128], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_6.30 = bf16[128,1024]{1,0} parameter(6)
  %convert.418.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_6.30), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.106.13 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.33), slice={[0:128], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.45 = bf16[128,1024]{1,0} parameter(5)
  %convert.404.13 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_5.45), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.107.19 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.33), slice={[0:128], [6144:7168]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.101 = bf16[128,1,1024]{2,0,1} parameter(4)
  %bitcast.1311.19 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_4.101)
  %convert.392.19 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1311.19), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.103.17 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.107.19, f32[128,1024]{1,0} %convert.392.19), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.105.13 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.404.13, f32[128,1024]{1,0} %add.103.17), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.106.11 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.106.13, f32[128,1024]{1,0} %add.105.13), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.108.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.418.9, f32[128,1024]{1,0} %add.106.11), metadata={op_type="aten__add" op_name="aten__add.1557/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.109.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.105.9, f32[128,1024]{1,0} %add.108.9), metadata={op_type="aten__add" op_name="aten__add.1570/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.111.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.430.5, f32[128,1024]{1,0} %add.109.7), metadata={op_type="aten__add" op_name="aten__add.1575/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.112.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.104.5, f32[128,1024]{1,0} %add.111.5), metadata={op_type="aten__add" op_name="aten__add.1588/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.245 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.112.3, f32[128,1024]{1,0} %add.112.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_223 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.25 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.245, f32[] %constant_223), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_222 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.275 = f32[128]{0} broadcast(f32[] %constant_222), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.243 = f32[128]{0} multiply(f32[128]{0} %reduce.25, f32[128]{0} %broadcast.275), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.379 = f32[] parameter(0)
  %broadcast.274 = f32[128]{0} broadcast(f32[] %param_0.379), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.133 = f32[128]{0} add(f32[128]{0} %multiply.243, f32[128]{0} %broadcast.274), metadata={op_type="aten__add" op_name="aten__add.1589/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.52 = f32[128]{0} rsqrt(f32[128]{0} %add.133), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.273 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.52), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1590/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.242 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.112.3, f32[128,1024]{1,0} %broadcast.273), metadata={op_type="aten__mul" op_name="aten__mul.1590/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.177 = bf16[1024]{0} parameter(3)
  %convert.431.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.177), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.198.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.431.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1591/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.197.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.242, f32[128,1024]{1,0} %broadcast.198.1), metadata={op_type="aten__mul" op_name="aten__mul.1591/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.432.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.197.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.3 (param_0.328: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.328 = bf16[128,6144]{1,0} parameter(0)
  %slice.114.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.328), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.433.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.114.1)
  %constant_1_5 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.496.3 = f32[] convert(bf16[] %constant_1_5)
  %broadcast.217.44 = f32[128,3072]{1,0} broadcast(f32[] %convert.496.3), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.24.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.433.8)
  %convert.438.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.24.7)
  %exponential.24.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.438.5)
  %convert.439.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.24.3)
  %add.113.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.217.44, f32[128,3072]{1,0} %convert.439.1)
  %divide.24.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.217.44, f32[128,3072]{1,0} %add.113.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.198.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.433.8, f32[128,3072]{1,0} %divide.24.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.115.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.328), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.440.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.115.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.199.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.198.5, f32[128,3072]{1,0} %convert.440.1), metadata={op_type="aten__mul" op_name="aten__mul.1592/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.441.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.199.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_add (param_0.304: bf16[128,7168], param_1.294: bf16[128,1024], param_2.205: bf16[128,1024], param_3.175: bf16[128,1,1024], param_4.99: bf16[128,1024], param_5.41: bf16[128,1024]) -> f32[128,1024] {
  %param_0.304 = bf16[128,7168]{1,0} parameter(0)
  %convert.387.17 = f32[128,7168]{1,0} convert(bf16[128,7168]{1,0} %param_0.304), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.103.3 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.17), slice={[0:128], [2048:3072]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.294 = bf16[128,1024]{1,0} parameter(1)
  %convert.442.3 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_1.294), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.104.7 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.17), slice={[0:128], [3072:4096]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.205 = bf16[128,1024]{1,0} parameter(2)
  %convert.430.7 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_2.205), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.105.7 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.17), slice={[0:128], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.41 = bf16[128,1024]{1,0} parameter(5)
  %convert.418.7 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_5.41), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.106.11 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.17), slice={[0:128], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.99 = bf16[128,1024]{1,0} parameter(4)
  %convert.404.11 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_4.99), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.107.17 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.17), slice={[0:128], [6144:7168]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.175 = bf16[128,1,1024]{2,0,1} parameter(3)
  %bitcast.1311.17 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_3.175)
  %convert.392.17 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1311.17), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.103.15 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.107.17, f32[128,1024]{1,0} %convert.392.17), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.105.11 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.404.11, f32[128,1024]{1,0} %add.103.15), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.106.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.106.11, f32[128,1024]{1,0} %add.105.11), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.108.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.418.7, f32[128,1024]{1,0} %add.106.9), metadata={op_type="aten__add" op_name="aten__add.1557/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.109.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.105.7, f32[128,1024]{1,0} %add.108.7), metadata={op_type="aten__add" op_name="aten__add.1570/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.111.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.430.7, f32[128,1024]{1,0} %add.109.5), metadata={op_type="aten__add" op_name="aten__add.1575/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.112.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.104.7, f32[128,1024]{1,0} %add.111.7), metadata={op_type="aten__add" op_name="aten__add.1588/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.114.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.442.3, f32[128,1024]{1,0} %add.112.5), metadata={op_type="aten__add" op_name="aten__add.1593/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %add.115.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.103.3, f32[128,1024]{1,0} %add.114.3), metadata={op_type="aten__add" op_name="aten__add.1606/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.83 (param_0.378: f32[128,1024], param_1.316: f32[], param_2.188: bf16[1024]) -> bf16[128,1024] {
  %param_0.378 = f32[128,1024]{1,0} parameter(0)
  %multiply.251 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_0.378, f32[128,1024]{1,0} %param_0.378), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_228 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.27 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.251, f32[] %constant_228), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_227 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.281 = f32[128]{0} broadcast(f32[] %constant_227), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.250 = f32[128]{0} multiply(f32[128]{0} %reduce.27, f32[128]{0} %broadcast.281), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.316 = f32[] parameter(1)
  %broadcast.280 = f32[128]{0} broadcast(f32[] %param_1.316), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.136 = f32[128]{0} add(f32[128]{0} %multiply.250, f32[128]{0} %broadcast.280), metadata={op_type="aten__add" op_name="aten__add.1607/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.54 = f32[128]{0} rsqrt(f32[128]{0} %add.136), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.279 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.54), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1608/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.249 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_0.378, f32[128,1024]{1,0} %broadcast.279), metadata={op_type="aten__mul" op_name="aten__mul.1608/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.188 = bf16[1024]{0} parameter(2)
  %convert.445.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.188), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.202.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.445.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1609/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.200.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.249, f32[128,1024]{1,0} %broadcast.202.1), metadata={op_type="aten__mul" op_name="aten__mul.1609/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.446.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.200.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.4 (param_0.332: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.332 = bf16[128,6144]{1,0} parameter(0)
  %slice.116.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.332), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.448.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.116.1)
  %constant_1_1 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.496.7 = f32[] convert(bf16[] %constant_1_1)
  %broadcast.217.40 = f32[128,3072]{1,0} broadcast(f32[] %convert.496.7), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.25.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.448.8)
  %convert.453.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.25.7)
  %exponential.25.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.453.5)
  %convert.455.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.25.3)
  %add.116.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.217.40, f32[128,3072]{1,0} %convert.455.1)
  %divide.25.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.217.40, f32[128,3072]{1,0} %add.116.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.201.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.448.8, f32[128,3072]{1,0} %divide.25.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.117.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.332), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.457.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.117.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.202.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.201.5, f32[128,3072]{1,0} %convert.457.1), metadata={op_type="aten__mul" op_name="aten__mul.1610/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.459.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.202.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.81 (param_0.377: f32[], param_1.315: f32[128,1024], param_2.187: bf16[128,1024], param_3.149: bf16[128,7168], param_4.79: bf16[1024]) -> bf16[128,1024] {
  %param_3.149 = bf16[128,7168]{1,0} parameter(3)
  %convert.387.29 = f32[128,7168]{1,0} convert(bf16[128,7168]{1,0} %param_3.149), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.102.5 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.29), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.187 = bf16[128,1024]{1,0} parameter(2)
  %convert.460.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_2.187), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.315 = f32[128,1024]{1,0} parameter(1)
  %add.118.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.460.5, f32[128,1024]{1,0} %param_1.315), metadata={op_type="aten__add" op_name="aten__add.1611/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.119.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.102.5, f32[128,1024]{1,0} %add.118.5), metadata={op_type="aten__add" op_name="aten__add.1624/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.257 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.119.3, f32[128,1024]{1,0} %add.119.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_233 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.29 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.257, f32[] %constant_233), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_232 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.287 = f32[128]{0} broadcast(f32[] %constant_232), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.256 = f32[128]{0} multiply(f32[128]{0} %reduce.29, f32[128]{0} %broadcast.287), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.377 = f32[] parameter(0)
  %broadcast.286 = f32[128]{0} broadcast(f32[] %param_0.377), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.138 = f32[128]{0} add(f32[128]{0} %multiply.256, f32[128]{0} %broadcast.286), metadata={op_type="aten__add" op_name="aten__add.1625/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.56 = f32[128]{0} rsqrt(f32[128]{0} %add.138), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.285 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.56), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1626/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.255 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.119.3, f32[128,1024]{1,0} %broadcast.285), metadata={op_type="aten__mul" op_name="aten__mul.1626/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.79 = bf16[1024]{0} parameter(4)
  %convert.461.1 = f32[1024]{0} convert(bf16[1024]{0} %param_4.79), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.205.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.461.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1627/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.203.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.255, f32[128,1024]{1,0} %broadcast.205.1), metadata={op_type="aten__mul" op_name="aten__mul.1627/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.464.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.203.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.5 (param_0.330: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.330 = bf16[128,6144]{1,0} parameter(0)
  %slice.118.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.330), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.465.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.118.1)
  %constant_1_7 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.496.5 = f32[] convert(bf16[] %constant_1_7)
  %broadcast.217.36 = f32[128,3072]{1,0} broadcast(f32[] %convert.496.5), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.26.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.465.8)
  %convert.469.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.26.7)
  %exponential.26.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.469.5)
  %convert.470.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.26.3)
  %add.120.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.217.36, f32[128,3072]{1,0} %convert.470.1)
  %divide.26.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.217.36, f32[128,3072]{1,0} %add.120.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.204.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.465.8, f32[128,3072]{1,0} %divide.26.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.119.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.330), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.471.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.119.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.205.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.204.5, f32[128,3072]{1,0} %convert.471.1), metadata={op_type="aten__mul" op_name="aten__mul.1628/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.472.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.205.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.79 (param_0.390: f32[], param_1.326: bf16[1024], param_2.197: f32[128,1024], param_3.162: bf16[128,1024], param_4.86: bf16[128,7168], param_5.25: bf16[128,1024]) -> bf16[128,1024] {
  %param_4.86 = bf16[128,7168]{1,0} parameter(4)
  %convert.387.47 = f32[128,7168]{1,0} convert(bf16[128,7168]{1,0} %param_4.86), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.101.5 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.47), slice={[0:128], [0:1024]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.25 = bf16[128,1024]{1,0} parameter(5)
  %convert.473.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_5.25), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.102.9 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.47), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.162 = bf16[128,1024]{1,0} parameter(3)
  %convert.460.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_3.162), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.197 = f32[128,1024]{1,0} parameter(2)
  %add.118.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.460.9, f32[128,1024]{1,0} %param_2.197), metadata={op_type="aten__add" op_name="aten__add.1611/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.119.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.102.9, f32[128,1024]{1,0} %add.118.9), metadata={op_type="aten__add" op_name="aten__add.1624/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.121.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.473.5, f32[128,1024]{1,0} %add.119.7), metadata={op_type="aten__add" op_name="aten__add.1629/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.122.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.101.5, f32[128,1024]{1,0} %add.121.5), metadata={op_type="aten__add" op_name="aten__add.1642/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.264 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.122.3, f32[128,1024]{1,0} %add.122.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_238 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.31 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.264, f32[] %constant_238), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_237 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.295 = f32[128]{0} broadcast(f32[] %constant_237), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.263 = f32[128]{0} multiply(f32[128]{0} %reduce.31, f32[128]{0} %broadcast.295), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.390 = f32[] parameter(0)
  %broadcast.294 = f32[128]{0} broadcast(f32[] %param_0.390), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.140 = f32[128]{0} add(f32[128]{0} %multiply.263, f32[128]{0} %broadcast.294), metadata={op_type="aten__add" op_name="aten__add.1643/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.58 = f32[128]{0} rsqrt(f32[128]{0} %add.140), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.292 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.58), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1644/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.262 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.122.3, f32[128,1024]{1,0} %broadcast.292), metadata={op_type="aten__mul" op_name="aten__mul.1644/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.326 = bf16[1024]{0} parameter(1)
  %convert.475.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.326), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.208.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.475.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1645/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.206.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.262, f32[128,1024]{1,0} %broadcast.208.1), metadata={op_type="aten__mul" op_name="aten__mul.1645/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.476.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.206.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.6 (param_0.326: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.326 = bf16[128,6144]{1,0} parameter(0)
  %slice.120.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.326), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.477.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.120.1)
  %constant_1_3 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.496.1 = f32[] convert(bf16[] %constant_1_3)
  %broadcast.217.32 = f32[128,3072]{1,0} broadcast(f32[] %convert.496.1), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.27.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.477.8)
  %convert.483.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.27.7)
  %exponential.27.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.483.5)
  %convert.484.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.27.3)
  %add.123.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.217.32, f32[128,3072]{1,0} %convert.484.1)
  %divide.27.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.217.32, f32[128,3072]{1,0} %add.123.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.207.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.477.8, f32[128,3072]{1,0} %divide.27.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.121.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.326), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.486.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.121.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.208.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.207.5, f32[128,3072]{1,0} %convert.486.1), metadata={op_type="aten__mul" op_name="aten__mul.1646/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.487.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.208.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.77 (param_0.375: f32[], param_1.328: bf16[128,1024], param_2.199: bf16[1024], param_3.165: f32[128,1024], param_4.90: bf16[128,1024], param_5.30: bf16[128,7168], param_6.12: bf16[128,1024]) -> bf16[128,1024] {
  %param_1.328 = bf16[128,1024]{1,0} parameter(1)
  %convert.488.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_1.328), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.30 = bf16[128,7168]{1,0} parameter(5)
  %convert.387.51 = f32[128,7168]{1,0} convert(bf16[128,7168]{1,0} %param_5.30), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.101.7 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.51), slice={[0:128], [0:1024]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_6.12 = bf16[128,1024]{1,0} parameter(6)
  %convert.473.7 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_6.12), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.102.11 = f32[128,1024]{1,0} slice(f32[128,7168]{1,0} %convert.387.51), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.90 = bf16[128,1024]{1,0} parameter(4)
  %convert.460.11 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_4.90), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.165 = f32[128,1024]{1,0} parameter(3)
  %add.118.11 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.460.11, f32[128,1024]{1,0} %param_3.165), metadata={op_type="aten__add" op_name="aten__add.1611/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.119.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.102.11, f32[128,1024]{1,0} %add.118.11), metadata={op_type="aten__add" op_name="aten__add.1624/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.121.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.473.7, f32[128,1024]{1,0} %add.119.9), metadata={op_type="aten__add" op_name="aten__add.1629/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.122.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.101.7, f32[128,1024]{1,0} %add.121.7), metadata={op_type="aten__add" op_name="aten__add.1642/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.124.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.488.5, f32[128,1024]{1,0} %add.122.5), metadata={op_type="aten__add" op_name="aten__add.1647/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.270 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.124.5, f32[128,1024]{1,0} %add.124.5), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_243 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.33 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.270, f32[] %constant_243), dimensions={1}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_242 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.303 = f32[128]{0} broadcast(f32[] %constant_242), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.269 = f32[128]{0} multiply(f32[128]{0} %reduce.33, f32[128]{0} %broadcast.303), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.375 = f32[] parameter(0)
  %broadcast.302 = f32[128]{0} broadcast(f32[] %param_0.375), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.142 = f32[128]{0} add(f32[128]{0} %multiply.269, f32[128]{0} %broadcast.302), metadata={op_type="aten__add" op_name="aten__add.1648/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.60 = f32[128]{0} rsqrt(f32[128]{0} %add.142), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.300 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.60), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1649/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.268 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.124.5, f32[128,1024]{1,0} %broadcast.300), metadata={op_type="aten__mul" op_name="aten__mul.1649/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.199 = bf16[1024]{0} parameter(2)
  %convert.489.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.199), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.211.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.489.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1650/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.209.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.268, f32[128,1024]{1,0} %broadcast.211.1), metadata={op_type="aten__mul" op_name="aten__mul.1650/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.490.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.209.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_slice_computation (param_0.398: bf16[128,4096]) -> bf16[128,1024] {
  %param_0.398 = bf16[128,4096]{1,0} parameter(0)
  ROOT %slice.127.1 = bf16[128,1024]{1,0} slice(bf16[128,4096]{1,0} %param_0.398), slice={[0:128], [3072:4096]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%triton_softmax_computation.9 (param_0.292: f32[], param_1.286: bf16[128,4096]) -> f32[128,8,128] {
  %param_1.286 = bf16[128,4096]{1,0} parameter(1)
  %slice.122.1 = bf16[128,1024]{1,0} slice(bf16[128,4096]{1,0} %param_1.286), slice={[0:128], [2048:3072]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.1643.3 = bf16[128,8,128]{2,1,0} bitcast(bf16[128,1024]{1,0} %slice.122.1)
  %convert.491.3 = f32[128,8,128]{2,1,0} convert(bf16[128,8,128]{2,1,0} %bitcast.1643.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.156 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %convert.491.3, f32[128,8,128]{2,1,0} %convert.491.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_103 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.9 = f32[128,8]{1,0} reduce(f32[128,8,128]{2,1,0} %multiply.156, f32[] %constant_103), dimensions={2}, to_apply=%AddComputation.60, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_104 = f32[] constant(0.0078125), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.158 = f32[128,8]{1,0} broadcast(f32[] %constant_104), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.157 = f32[128,8]{1,0} multiply(f32[128,8]{1,0} %reduce.9, f32[128,8]{1,0} %broadcast.158), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.292 = f32[] parameter(0)
  %broadcast.159 = f32[128,8]{1,0} broadcast(f32[] %param_0.292), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1651/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.92 = f32[128,8]{1,0} add(f32[128,8]{1,0} %multiply.157, f32[128,8]{1,0} %broadcast.159), metadata={op_type="aten__add" op_name="aten__add.1651/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.36 = f32[128,8]{1,0} rsqrt(f32[128,8]{1,0} %add.92), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.160 = f32[128,8,128]{2,1,0} broadcast(f32[128,8]{1,0} %rsqrt.36), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul.1652/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %multiply.159 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %convert.491.3, f32[128,8,128]{2,1,0} %broadcast.160), metadata={op_type="aten__mul" op_name="aten__mul.1652/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_concatenate (param_0.385: f32[128,8,128], param_1.330: bf16[128], param_2.200: bf16[40960,128], param_3.167: s32[128]) -> bf16[128,8,128] {
  %param_0.385 = f32[128,8,128]{2,1,0} parameter(0)
  %param_1.330 = bf16[128]{0} parameter(1)
  %convert.492.1 = f32[128]{0} convert(bf16[128]{0} %param_1.330), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.212.18 = f32[128,8,128]{2,1,0} broadcast(f32[128]{0} %convert.492.1), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul.1653/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.210.18 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %param_0.385, f32[128,8,128]{2,1,0} %broadcast.212.18), metadata={op_type="aten__mul" op_name="aten__mul.1653/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.123.9 = f32[128,8,64]{2,1,0} slice(f32[128,8,128]{2,1,0} %multiply.210.18), slice={[0:128], [0:8], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.200 = bf16[40960,128]{1,0} parameter(2)
  %param_3.167 = s32[128]{0} parameter(3)
  %bitcast.1659.3 = s32[128,1]{1,0} bitcast(s32[128]{0} %param_3.167)
  %gather.1.3 = bf16[128,1,128]{2,0,1} gather(bf16[40960,128]{1,0} %param_2.200, s32[128,1]{1,0} %bitcast.1659.3), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,128}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.1662.7 = bf16[128,128]{1,0} bitcast(bf16[128,1,128]{2,0,1} %gather.1.3)
  %convert.493.7 = f32[128,128]{1,0} convert(bf16[128,128]{1,0} %bitcast.1662.7), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.124.3 = f32[128,64]{1,0} slice(f32[128,128]{1,0} %convert.493.7), slice={[0:128], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.213.14 = f32[128,8,64]{2,1,0} broadcast(f32[128,64]{1,0} %slice.124.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.1654/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.211.7 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.123.9, f32[128,8,64]{2,1,0} %broadcast.213.14), metadata={op_type="aten__mul" op_name="aten__mul.1654/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.125.9 = f32[128,8,64]{2,1,0} slice(f32[128,8,128]{2,1,0} %multiply.210.18), slice={[0:128], [0:8], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.126.3 = f32[128,64]{1,0} slice(f32[128,128]{1,0} %convert.493.7), slice={[0:128], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.214.10 = f32[128,8,64]{2,1,0} broadcast(f32[128,64]{1,0} %slice.126.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.1655/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.212.5 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.125.9, f32[128,8,64]{2,1,0} %broadcast.214.10), metadata={op_type="aten__mul" op_name="aten__mul.1655/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %subtract.2.5 = f32[128,8,64]{2,1,0} subtract(f32[128,8,64]{2,1,0} %multiply.211.7, f32[128,8,64]{2,1,0} %multiply.212.5), metadata={op_type="aten__sub" op_name="aten__sub.1656/aten__sub" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.494.3 = bf16[128,8,64]{2,1,0} convert(f32[128,8,64]{2,1,0} %subtract.2.5)
  %multiply.213.7 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.125.9, f32[128,8,64]{2,1,0} %broadcast.213.14), metadata={op_type="aten__mul" op_name="aten__mul.1657/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.214.5 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.123.9, f32[128,8,64]{2,1,0} %broadcast.214.10), metadata={op_type="aten__mul" op_name="aten__mul.1658/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.125.5 = f32[128,8,64]{2,1,0} add(f32[128,8,64]{2,1,0} %multiply.213.7, f32[128,8,64]{2,1,0} %multiply.214.5), metadata={op_type="aten__add" op_name="aten__add.1659/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.495.3 = bf16[128,8,64]{2,1,0} convert(f32[128,8,64]{2,1,0} %add.125.5)
  ROOT %concatenate.18.1 = bf16[128,8,128]{2,1,0} concatenate(bf16[128,8,64]{2,1,0} %convert.494.3, bf16[128,8,64]{2,1,0} %convert.495.3), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_slice (param_0.1: bf16[2,4233,16,8,128]) -> bf16[69353472] {
  %param_0.1 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  %bitcast.1703.1 = bf16[138706944]{0} bitcast(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.1)
  ROOT %slice.129.1 = bf16[69353472]{0} slice(bf16[138706944]{0} %bitcast.1703.1), slice={[69353472:138706944]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%wrapped_slice_computation.1 (param_0.399: bf16[2,4233,16,8,128]) -> bf16[1,4233,16,8,128] {
  %param_0.399 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  ROOT %slice.128.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} slice(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.399), slice={[0:1], [0:4233], [0:16], [0:8], [0:128]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%command_buffer (p: bf16[151936,1024], p.1: s32[128], p.2: bf16[1024,2048], p.3: bf16[1024,2048], p.4: bf16[1024,2048], p.5: bf16[1024,2048], p.6: bf16[1024,2048], p.7: bf16[1024,2048], p.8: bf16[1024,2048], p.9: f32[], p.10: bf16[1024], p.11: bf16[6144,1024], p.12: bf16[1024,3072], p.13: bf16[1024], p.14: bf16[6144,1024], p.15: bf16[1024,3072], p.16: bf16[1024], p.17: bf16[6144,1024], p.18: bf16[1024,3072], p.19: bf16[1024], p.20: bf16[6144,1024], p.21: bf16[1024,3072], p.22: bf16[1024], p.23: bf16[6144,1024], p.24: bf16[1024,3072], p.25: bf16[1024], p.26: bf16[6144,1024], p.27: bf16[1024,3072], p.28: bf16[1024], p.29: bf16[6144,1024], p.30: bf16[1024,3072], p.31: bf16[1024], p.32: bf16[4096,1024], p.33: bf16[128], p.34: bf16[40960,128], p.35: s32[128], p.36: bf16[2,4233,16,8,128]) -> (bf16[128,8,128], bf16[128,8,128], bf16[4233,16,8,128], bf16[1,4233,16,8,128]) {
  %p = bf16[151936,1024]{1,0} parameter(0)
  %p.1 = s32[128]{0} parameter(1)
  %p.2 = bf16[1024,2048]{1,0} parameter(2)
  %p.3 = bf16[1024,2048]{1,0} parameter(3)
  %p.4 = bf16[1024,2048]{1,0} parameter(4)
  %p.5 = bf16[1024,2048]{1,0} parameter(5)
  %p.6 = bf16[1024,2048]{1,0} parameter(6)
  %p.7 = bf16[1024,2048]{1,0} parameter(7)
  %p.8 = bf16[1024,2048]{1,0} parameter(8)
  %p.9 = f32[] parameter(9)
  %p.10 = bf16[1024]{0} parameter(10)
  %p.11 = bf16[6144,1024]{1,0} parameter(11)
  %p.12 = bf16[1024,3072]{1,0} parameter(12)
  %p.13 = bf16[1024]{0} parameter(13)
  %p.14 = bf16[6144,1024]{1,0} parameter(14)
  %p.15 = bf16[1024,3072]{1,0} parameter(15)
  %p.16 = bf16[1024]{0} parameter(16)
  %p.17 = bf16[6144,1024]{1,0} parameter(17)
  %p.18 = bf16[1024,3072]{1,0} parameter(18)
  %p.19 = bf16[1024]{0} parameter(19)
  %p.20 = bf16[6144,1024]{1,0} parameter(20)
  %p.21 = bf16[1024,3072]{1,0} parameter(21)
  %p.22 = bf16[1024]{0} parameter(22)
  %p.23 = bf16[6144,1024]{1,0} parameter(23)
  %p.24 = bf16[1024,3072]{1,0} parameter(24)
  %p.25 = bf16[1024]{0} parameter(25)
  %p.26 = bf16[6144,1024]{1,0} parameter(26)
  %p.27 = bf16[1024,3072]{1,0} parameter(27)
  %p.28 = bf16[1024]{0} parameter(28)
  %p.29 = bf16[6144,1024]{1,0} parameter(29)
  %p.30 = bf16[1024,3072]{1,0} parameter(30)
  %p.31 = bf16[1024]{0} parameter(31)
  %p.32 = bf16[4096,1024]{1,0} parameter(32)
  %p.33 = bf16[128]{0} parameter(33)
  %p.34 = bf16[40960,128]{1,0} parameter(34)
  %p.35 = s32[128]{0} parameter(35)
  %p.36 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(36)
  %loop_gather_fusion = bf16[128,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[128]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_concatenate = bf16[7168,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %gemm_fusion_dot.27.0 = bf16[128,7168]{1,0} fusion(bf16[7168,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.27_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"128","block_n":"32","block_k":"64","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
  %fusion.110 = bf16[128,1024]{1,0} fusion(f32[] %p.9, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,7168]{1,0} %gemm_fusion_dot.27.0, bf16[1024]{0} %p.10), kind=kCustom, calls=%fused_computation.91, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.15.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.110, bf16[6144,1024]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.15 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.15.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.15), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.16.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.1.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.16.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.108 = bf16[128,1024]{1,0} fusion(f32[] %p.9, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,7168]{1,0} %gemm_fusion_dot.27.0, bf16[128,1024]{1,0} %get-tuple-element.1.0, bf16[1024]{0} %p.13), kind=kCustom, calls=%fused_computation.89, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.17.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.108, bf16[6144,1024]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.2.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.17.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.1 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.18.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.15), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.3.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.18.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.106 = bf16[128,1024]{1,0} fusion(f32[] %p.9, bf16[1024]{0} %p.16, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,7168]{1,0} %gemm_fusion_dot.27.0, bf16[128,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.87, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.19.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.106, bf16[6144,1024]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.4.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.19.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.2 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.20.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.5.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.20.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.104 = bf16[128,1024]{1,0} fusion(f32[] %p.9, bf16[128,1024]{1,0} %get-tuple-element.5.0, bf16[128,7168]{1,0} %gemm_fusion_dot.27.0, bf16[1024]{0} %p.19, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.1.0, bf16[128,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.85, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.21.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.104, bf16[6144,1024]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.6.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.21.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.3 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.22.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.7.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.22.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_add_fusion = f32[128,1024]{1,0} fusion(bf16[128,7168]{1,0} %gemm_fusion_dot.27.0, bf16[128,1024]{1,0} %get-tuple-element.7.0, bf16[128,1024]{1,0} %get-tuple-element.5.0, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.3.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.1606/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.102 = bf16[128,1024]{1,0} fusion(f32[128,1024]{1,0} %loop_add_fusion, f32[] %p.9, bf16[1024]{0} %p.22), kind=kCustom, calls=%fused_computation.83, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.23.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.102, bf16[6144,1024]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.8.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.23.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.4 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.24.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.9.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.24.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.100 = bf16[128,1024]{1,0} fusion(f32[] %p.9, f32[128,1024]{1,0} %loop_add_fusion, bf16[128,1024]{1,0} %get-tuple-element.9.0, bf16[128,7168]{1,0} %gemm_fusion_dot.27.0, bf16[1024]{0} %p.25), kind=kCustom, calls=%fused_computation.81, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.25.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.100, bf16[6144,1024]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.10.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.25.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.5 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.26.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.11.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.26.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.98 = bf16[128,1024]{1,0} fusion(f32[] %p.9, bf16[1024]{0} %p.28, f32[128,1024]{1,0} %loop_add_fusion, bf16[128,1024]{1,0} %get-tuple-element.9.0, bf16[128,7168]{1,0} %gemm_fusion_dot.27.0, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.79, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.27.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.98, bf16[6144,1024]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.12.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.27.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.6 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.28.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.13.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.28.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.96 = bf16[128,1024]{1,0} fusion(f32[] %p.9, bf16[128,1024]{1,0} %get-tuple-element.13.0, bf16[1024]{0} %p.31, f32[128,1024]{1,0} %loop_add_fusion, bf16[128,1024]{1,0} %get-tuple-element.9.0, /*index=5*/bf16[128,7168]{1,0} %gemm_fusion_dot.27.0, bf16[128,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.77, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.29.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.96, bf16[4096,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.14.0 = bf16[128,4096]{1,0} get-tuple-element((bf16[128,4096]{1,0}, s8[4194304]{0}) %custom-call.29.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_slice = bf16[128,1024]{1,0} fusion(bf16[128,4096]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %triton_softmax.9.0 = f32[128,8,128]{2,1,0} fusion(f32[] %p.9, bf16[128,4096]{1,0} %get-tuple-element.14.0), kind=kCustom, calls=%triton_softmax_computation.9, metadata={op_type="aten__mul" op_name="aten__mul.1652/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %input_concatenate_fusion = bf16[128,8,128]{2,1,0} fusion(f32[128,8,128]{2,1,0} %triton_softmax.9.0, bf16[128]{0} %p.33, bf16[40960,128]{1,0} %p.34, s32[128]{0} %p.35), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.1695.0 = bf16[128,8,128]{2,1,0} bitcast(bf16[128,1024]{1,0} %wrapped_slice)
  %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.36), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  %bitcast.1707.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[69353472]{0} %loop_slice_fusion)
  %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.36), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  ROOT %tuple = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %input_concatenate_fusion, bf16[128,8,128]{2,1,0} %bitcast.1695.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.1707.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
}

ENTRY %SyncTensorsGraph.663 (p0.1.0: bf16[128], p1.4.0: f32[], p2.6.0: bf16[4096,1024], p3.8.0: bf16[1024], p4.26.0: s32[128], p5.28.0: bf16[151936,1024], p6.32.0: bf16[1024,2048], p7.48.0: bf16[1024,3072], p8.50.0: bf16[6144,1024], p9.52.0: bf16[1024], p10.104.0: bf16[1024,2048], p11.120.0: bf16[1024,3072], p12.122.0: bf16[6144,1024], p13.124.0: bf16[1024], p14.176.0: bf16[1024,2048], p15.192.0: bf16[1024,3072], p16.194.0: bf16[6144,1024], p17.196.0: bf16[1024], p18.248.0: bf16[1024,2048], p19.264.0: bf16[1024,3072], p20.266.0: bf16[6144,1024], p21.268.0: bf16[1024], p22.320.0: bf16[1024,2048], p23.336.0: bf16[1024,3072], p24.338.0: bf16[6144,1024], p25.340.0: bf16[1024], p26.392.0: bf16[1024,2048], p27.408.0: bf16[1024,3072], p28.410.0: bf16[6144,1024], p29.412.0: bf16[1024], p30.464.0: bf16[1024,2048], p31.480.0: bf16[1024,3072], p32.482.0: bf16[6144,1024], p33.484.0: bf16[1024], p34.608.0: s32[128], p35.609.0: bf16[40960,128], p36.653.0: bf16[2,4233,16,8,128]) -> (bf16[128,8,128], bf16[128,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128]) {
  %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %p36.653.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p35.609.0 = bf16[40960,128]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p34.608.0 = s32[128]{0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p33.484.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p32.482.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p31.480.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p30.464.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p29.412.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p28.410.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p27.408.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p26.392.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p25.340.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p24.338.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p23.336.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p22.320.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p21.268.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p20.266.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p19.264.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p18.248.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p17.196.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p16.194.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p15.192.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p14.176.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p13.124.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p12.122.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p11.120.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p10.104.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p9.52.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p8.50.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p7.48.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p6.32.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p5.28.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p4.26.0 = s32[128]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %call = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) call(bf16[151936,1024]{1,0} %p5.28.0, s32[128]{0} %p4.26.0, bf16[1024,2048]{1,0} %p30.464.0, bf16[1024,2048]{1,0} %p26.392.0, bf16[1024,2048]{1,0} %p22.320.0, /*index=5*/bf16[1024,2048]{1,0} %p18.248.0, bf16[1024,2048]{1,0} %p14.176.0, bf16[1024,2048]{1,0} %p10.104.0, bf16[1024,2048]{1,0} %p6.32.0, f32[] %p1.4.0, /*index=10*/bf16[1024]{0} %p9.52.0, bf16[6144,1024]{1,0} %p8.50.0, bf16[1024,3072]{1,0} %p7.48.0, bf16[1024]{0} %p13.124.0, bf16[6144,1024]{1,0} %p12.122.0, /*index=15*/bf16[1024,3072]{1,0} %p11.120.0, bf16[1024]{0} %p17.196.0, bf16[6144,1024]{1,0} %p16.194.0, bf16[1024,3072]{1,0} %p15.192.0, bf16[1024]{0} %p21.268.0, /*index=20*/bf16[6144,1024]{1,0} %p20.266.0, bf16[1024,3072]{1,0} %p19.264.0, bf16[1024]{0} %p25.340.0, bf16[6144,1024]{1,0} %p24.338.0, bf16[1024,3072]{1,0} %p23.336.0, /*index=25*/bf16[1024]{0} %p29.412.0, bf16[6144,1024]{1,0} %p28.410.0, bf16[1024,3072]{1,0} %p27.408.0, bf16[1024]{0} %p33.484.0, bf16[6144,1024]{1,0} %p32.482.0, /*index=30*/bf16[1024,3072]{1,0} %p31.480.0, bf16[1024]{0} %p3.8.0, bf16[4096,1024]{1,0} %p2.6.0, bf16[128]{0} %p0.1.0, bf16[40960,128]{1,0} %p35.609.0, /*index=35*/s32[128]{0} %p34.608.0, bf16[2,4233,16,8,128]{4,3,2,1,0} %p36.653.0), to_apply=%command_buffer
  %get-tuple-element.17 = bf16[128,8,128]{2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=0
  %get-tuple-element.18 = bf16[128,8,128]{2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=1
  %get-tuple-element.19 = bf16[4233,16,8,128]{3,2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=2
  %get-tuple-element.20 = bf16[1,4233,16,8,128]{4,3,2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=3
  %bitcast.1700.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[1,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.20)
  ROOT %tuple.662.0 = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %get-tuple-element.17, bf16[128,8,128]{2,1,0} %get-tuple-element.18, bf16[4233,16,8,128]{3,2,1,0} %bitcast.1700.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.19)
}

