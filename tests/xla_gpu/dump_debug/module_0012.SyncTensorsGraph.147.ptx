//
// Generated by LLVM NVPTX Back-End
//

.version 8.5
.target sm_80
.address_size 64

	// .globl	loop_gather_fusion
.extern .shared .align 16 .b8 global_smem[];

.visible .entry loop_gather_fusion(
	.param .u64 .ptr .align 16 loop_gather_fusion_param_0,
	.param .u64 .ptr .align 16 loop_gather_fusion_param_1,
	.param .u64 .ptr .align 128 loop_gather_fusion_param_2
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd1, [loop_gather_fusion_param_0];
	ld.param.u64 	%rd2, [loop_gather_fusion_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [loop_gather_fusion_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shr.u32 	%r3, %r2, 3;
	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd5, %rd7;
	ld.global.nc.u32 	%r4, [%rd8];
	min.u32 	%r5, %r4, 151935;
	shl.b32 	%r6, %r2, 7;
	and.b32  	%r7, %r6, 896;
	shl.b32 	%r8, %r5, 10;
	or.b32  	%r9, %r8, %r7;
	or.b32  	%r10, %r9, %r1;
	mul.wide.u32 	%rd9, %r10, 2;
	add.s64 	%rd10, %rd6, %rd9;
	ld.global.nc.u16 	%rs1, [%rd10];
	or.b32  	%r11, %r6, %r1;
	mul.wide.u32 	%rd11, %r11, 2;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.b16 	[%rd12], %rs1;
	ret;

}
	// .globl	fusion_9
.visible .entry fusion_9(
	.param .u64 .ptr .align 16 fusion_9_param_0,
	.param .u64 .ptr .align 128 fusion_9_param_1,
	.param .u64 .ptr .align 16 fusion_9_param_2,
	.param .u64 .ptr .align 128 fusion_9_param_3
)
.reqntid 64, 1, 1
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<29>;
	.reg .b32 	%r<74>;
	.reg .f32 	%f<82>;
	.reg .b64 	%rd<29>;

	ld.param.u64 	%rd7, [fusion_9_param_0];
	ld.param.u64 	%rd8, [fusion_9_param_3];
	cvta.to.global.u64 	%rd9, %rd8;
	ld.param.u64 	%rd10, [fusion_9_param_1];
	ld.param.u64 	%rd11, [fusion_9_param_2];
	cvta.to.global.u64 	%rd12, %rd11;
	cvta.to.global.u64 	%rd13, %rd10;
	cvta.to.global.u64 	%rd4, %rd7;
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	mul.wide.s32 	%rd14, %r1, 256;
	shl.b64 	%rd15, %rd14, 1;
	add.s64 	%rd16, %rd13, %rd15;
	mov.u32 	%r51, %tid.x;
	and.b32  	%r52, %r51, 31;
	shl.b32 	%r53, %r51, 2;
	and.b32  	%r54, %r53, 124;
	setp.lt.u32 	%p4, %r51, 32;
	selp.b32 	%r55, 0, 128, %p4;
	or.b32  	%r56, %r54, %r55;
	mul.wide.u32 	%rd17, %r56, 2;
	add.s64 	%rd1, %rd16, %rd17;
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	ld.global.v2.b32 { %r2, %r3 }, [ %rd1 + 0 ];
	// end inline asm
	mov.b32 	{%rs1, %rs2}, %r2;
	mov.b32 	{%rs3, %rs4}, %r3;
	// begin inline asm
	cvt.f32.bf16 %r4, %rs1;
	// end inline asm
	mov.b32 	%f1, %r4;
	// begin inline asm
	cvt.f32.bf16 %r5, %rs2;
	// end inline asm
	mov.b32 	%f2, %r5;
	// begin inline asm
	cvt.f32.bf16 %r6, %rs3;
	// end inline asm
	mov.b32 	%f3, %r6;
	// begin inline asm
	cvt.f32.bf16 %r7, %rs4;
	// end inline asm
	mov.b32 	%f4, %r7;
	and.b64  	%rd18, %rd15, -2048;
	add.s64 	%rd19, %rd13, %rd18;
	shl.b32 	%r57, %r51, 3;
	and.b32  	%r58, %r57, 248;
	selp.b32 	%r59, 0, 256, %p4;
	or.b32  	%r60, %r58, %r59;
	mul.wide.u32 	%rd20, %r60, 2;
	add.s64 	%rd2, %rd19, %rd20;
	add.s64 	%rd3, %rd2, 1024;
	// begin inline asm
	mov.u32 %r8, 0x0;
	mov.u32 %r9, 0x0;
	mov.u32 %r10, 0x0;
	mov.u32 %r11, 0x0;
	ld.global.v4.b32 { %r8, %r9, %r10, %r11 }, [ %rd2 + 0 ];
	// end inline asm
	mov.b32 	{%rs5, %rs6}, %r8;
	mov.b32 	{%rs7, %rs8}, %r9;
	mov.b32 	{%rs9, %rs10}, %r10;
	mov.b32 	{%rs11, %rs12}, %r11;
	// begin inline asm
	mov.u32 %r12, 0x0;
	mov.u32 %r13, 0x0;
	mov.u32 %r14, 0x0;
	mov.u32 %r15, 0x0;
	ld.global.v4.b32 { %r12, %r13, %r14, %r15 }, [ %rd3 + 0 ];
	// end inline asm
	mov.b32 	{%rs13, %rs14}, %r12;
	mov.b32 	{%rs15, %rs16}, %r13;
	mov.b32 	{%rs17, %rs18}, %r14;
	mov.b32 	{%rs19, %rs20}, %r15;
	// begin inline asm
	cvt.f32.bf16 %r16, %rs5;
	// end inline asm
	mov.b32 	%f5, %r16;
	// begin inline asm
	cvt.f32.bf16 %r17, %rs6;
	// end inline asm
	mov.b32 	%f6, %r17;
	// begin inline asm
	cvt.f32.bf16 %r18, %rs7;
	// end inline asm
	mov.b32 	%f7, %r18;
	// begin inline asm
	cvt.f32.bf16 %r19, %rs8;
	// end inline asm
	mov.b32 	%f8, %r19;
	// begin inline asm
	cvt.f32.bf16 %r20, %rs9;
	// end inline asm
	mov.b32 	%f9, %r20;
	// begin inline asm
	cvt.f32.bf16 %r21, %rs10;
	// end inline asm
	mov.b32 	%f10, %r21;
	// begin inline asm
	cvt.f32.bf16 %r22, %rs11;
	// end inline asm
	mov.b32 	%f11, %r22;
	// begin inline asm
	cvt.f32.bf16 %r23, %rs12;
	// end inline asm
	mov.b32 	%f12, %r23;
	// begin inline asm
	cvt.f32.bf16 %r24, %rs13;
	// end inline asm
	mov.b32 	%f13, %r24;
	// begin inline asm
	cvt.f32.bf16 %r25, %rs14;
	// end inline asm
	mov.b32 	%f14, %r25;
	// begin inline asm
	cvt.f32.bf16 %r26, %rs15;
	// end inline asm
	mov.b32 	%f15, %r26;
	// begin inline asm
	cvt.f32.bf16 %r27, %rs16;
	// end inline asm
	mov.b32 	%f16, %r27;
	// begin inline asm
	cvt.f32.bf16 %r28, %rs17;
	// end inline asm
	mov.b32 	%f17, %r28;
	// begin inline asm
	cvt.f32.bf16 %r29, %rs18;
	// end inline asm
	mov.b32 	%f18, %r29;
	// begin inline asm
	cvt.f32.bf16 %r30, %rs19;
	// end inline asm
	mov.b32 	%f19, %r30;
	// begin inline asm
	cvt.f32.bf16 %r31, %rs20;
	// end inline asm
	mov.b32 	%f20, %r31;
	mul.rn.f32 	%f21, %f5, %f5;
	mul.rn.f32 	%f22, %f6, %f6;
	mul.rn.f32 	%f23, %f7, %f7;
	mul.rn.f32 	%f24, %f8, %f8;
	mul.rn.f32 	%f25, %f9, %f9;
	mul.rn.f32 	%f26, %f10, %f10;
	mul.rn.f32 	%f27, %f11, %f11;
	mul.rn.f32 	%f28, %f12, %f12;
	mul.rn.f32 	%f29, %f13, %f13;
	mul.rn.f32 	%f30, %f14, %f14;
	mul.rn.f32 	%f31, %f15, %f15;
	mul.rn.f32 	%f32, %f16, %f16;
	mul.rn.f32 	%f33, %f17, %f17;
	mul.rn.f32 	%f34, %f18, %f18;
	mul.rn.f32 	%f35, %f19, %f19;
	mul.rn.f32 	%f36, %f20, %f20;
	add.rn.f32 	%f37, %f21, %f22;
	add.rn.f32 	%f38, %f37, %f23;
	add.rn.f32 	%f39, %f38, %f24;
	add.rn.f32 	%f40, %f39, %f25;
	add.rn.f32 	%f41, %f40, %f26;
	add.rn.f32 	%f42, %f41, %f27;
	add.rn.f32 	%f43, %f42, %f28;
	add.rn.f32 	%f44, %f43, %f29;
	add.rn.f32 	%f45, %f44, %f30;
	add.rn.f32 	%f46, %f45, %f31;
	add.rn.f32 	%f47, %f46, %f32;
	add.rn.f32 	%f48, %f47, %f33;
	add.rn.f32 	%f49, %f48, %f34;
	add.rn.f32 	%f50, %f49, %f35;
	add.rn.f32 	%f51, %f50, %f36;
	mov.b32 	%r61, %f51;
	shfl.sync.bfly.b32	%r62, %r61, 16, 31, -1;
	mov.b32 	%f52, %r62;
	add.rn.f32 	%f53, %f51, %f52;
	mov.b32 	%r63, %f53;
	shfl.sync.bfly.b32	%r64, %r63, 8, 31, -1;
	mov.b32 	%f54, %r64;
	add.rn.f32 	%f55, %f53, %f54;
	mov.b32 	%r65, %f55;
	shfl.sync.bfly.b32	%r66, %r65, 4, 31, -1;
	mov.b32 	%f56, %r66;
	add.rn.f32 	%f57, %f55, %f56;
	mov.b32 	%r67, %f57;
	shfl.sync.bfly.b32	%r68, %r67, 2, 31, -1;
	mov.b32 	%f58, %r68;
	add.rn.f32 	%f59, %f57, %f58;
	mov.b32 	%r69, %f59;
	shfl.sync.bfly.b32	%r70, %r69, 1, 31, -1;
	mov.b32 	%f60, %r70;
	add.rn.f32 	%f61, %f59, %f60;
	setp.eq.s32 	%p1, %r52, 0;
	shr.u32 	%r71, %r51, 3;
	and.b32  	%r72, %r71, 4;
	cvt.u64.u32 	%rd21, %r72;
	mov.u64 	%rd22, global_smem;
	add.s64 	%rd23, %rd22, %rd21;
	mov.b32 	%r33, %f61;
	cvt.u32.u64 	%r32, %rd23;
	// begin inline asm
	@%p1 st.shared.b32 [ %r32 + 0 ], %r33;
	// end inline asm
	bar.sync 	0;
	setp.lt.u32 	%p2, %r51, 2;
	cvt.u64.u32 	%rd24, %r53;
	add.s64 	%rd25, %rd22, %rd24;
	cvt.u32.u64 	%r35, %rd25;
	// begin inline asm
	@%p2 ld.shared.b32 %r34, [ %r35 + 0 ];
	// end inline asm
	mov.b32 	%f62, %r34;
	shfl.sync.bfly.b32	%r73, %r34, 1, 31, -1;
	mov.b32 	%f63, %r73;
	add.rn.f32 	%f64, %f62, %f63;
	setp.eq.s32 	%p3, %r51, 0;
	mov.b32 	%r37, %f64;
	// begin inline asm
	@%p3 st.shared.b32 [ %r35 + 0 ], %r37;
	// end inline asm
	bar.sync 	0;
	ld.shared.f32 	%f65, [global_smem];
	mul.rn.f32 	%f66, %f65, 0f3A800000;
	// begin inline asm
	mov.u32 %r38, 0x0;
	ld.global.b32 { %r38 }, [ %rd4 + 0 ];
	// end inline asm
	mov.b32 	%f67, %r38;
	add.rn.f32 	%f68, %f66, %f67;
	rsqrt.approx.f32 	%f69, %f68;
	mul.rn.f32 	%f70, %f1, %f69;
	mul.rn.f32 	%f71, %f2, %f69;
	mul.rn.f32 	%f72, %f3, %f69;
	mul.rn.f32 	%f73, %f4, %f69;
	and.b64  	%rd26, %rd15, 1536;
	add.s64 	%rd27, %rd12, %rd26;
	add.s64 	%rd5, %rd27, %rd17;
	// begin inline asm
	mov.u32 %r39, 0x0;
	mov.u32 %r40, 0x0;
	ld.global.v2.b32 { %r39, %r40 }, [ %rd5 + 0 ];
	// end inline asm
	mov.b32 	{%rs21, %rs22}, %r39;
	mov.b32 	{%rs23, %rs24}, %r40;
	// begin inline asm
	cvt.f32.bf16 %r41, %rs21;
	// end inline asm
	mov.b32 	%f74, %r41;
	// begin inline asm
	cvt.f32.bf16 %r42, %rs22;
	// end inline asm
	mov.b32 	%f75, %r42;
	// begin inline asm
	cvt.f32.bf16 %r43, %rs23;
	// end inline asm
	mov.b32 	%f76, %r43;
	// begin inline asm
	cvt.f32.bf16 %r44, %rs24;
	// end inline asm
	mov.b32 	%f77, %r44;
	mul.rn.f32 	%f78, %f70, %f74;
	mul.rn.f32 	%f79, %f71, %f75;
	mul.rn.f32 	%f80, %f72, %f76;
	mul.rn.f32 	%f81, %f73, %f77;
	mov.b32 	%r45, %f78;
	// begin inline asm
	cvt.rn.bf16.f32 %rs25, %r45;
	// end inline asm
	mov.b32 	%r46, %f79;
	// begin inline asm
	cvt.rn.bf16.f32 %rs26, %r46;
	// end inline asm
	mov.b32 	%r47, %f80;
	// begin inline asm
	cvt.rn.bf16.f32 %rs27, %r47;
	// end inline asm
	mov.b32 	%r48, %f81;
	// begin inline asm
	cvt.rn.bf16.f32 %rs28, %r48;
	// end inline asm
	add.s64 	%rd28, %rd9, %rd15;
	add.s64 	%rd6, %rd28, %rd17;
	mov.b32 	%r49, {%rs25, %rs26};
	mov.b32 	%r50, {%rs27, %rs28};
	// begin inline asm
	st.global.v2.b32 [ %rd6 + 0 ], { %r49, %r50 };
	// end inline asm
	ret;

}
	// .globl	wrapped_slice
.visible .entry wrapped_slice(
	.param .u64 .ptr .align 128 wrapped_slice_param_0,
	.param .u64 .ptr .align 128 wrapped_slice_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd1, [wrapped_slice_param_0];
	ld.param.u64 	%rd2, [wrapped_slice_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r2, 7;
	and.b32  	%r4, %r3, 896;
	shl.b32 	%r5, %r2, 9;
	and.b32  	%r6, %r5, 61440;
	or.b32  	%r7, %r6, %r4;
	or.b32  	%r8, %r7, %r1;
	mul.wide.u32 	%rd5, %r8, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6+6144];
	or.b32  	%r9, %r3, %r1;
	mul.wide.u32 	%rd7, %r9, 2;
	add.s64 	%rd8, %rd3, %rd7;
	st.global.b16 	[%rd8], %rs1;
	ret;

}
	// .globl	triton_softmax_2_0
.visible .entry triton_softmax_2_0(
	.param .u64 .ptr .align 16 triton_softmax_2_0_param_0,
	.param .u64 .ptr .align 128 triton_softmax_2_0_param_1,
	.param .u64 .ptr .align 128 triton_softmax_2_0_param_2
)
.reqntid 32, 1, 1
{
	.reg .pred 	%p<2>;
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<29>;
	.reg .f32 	%f<30>;
	.reg .b64 	%rd<20>;

	ld.param.u64 	%rd4, [triton_softmax_2_0_param_0];
	ld.param.u64 	%rd5, [triton_softmax_2_0_param_2];
	cvta.to.global.u64 	%rd6, %rd5;
	ld.param.u64 	%rd7, [triton_softmax_2_0_param_1];
	cvta.to.global.u64 	%rd8, %rd7;
	cvta.to.global.u64 	%rd2, %rd4;
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	shl.b32 	%r13, %r1, 7;
	cvt.u64.u32 	%rd9, %r13;
	and.b64  	%rd10, %rd9, 896;
	mul.wide.s32 	%rd11, %r1, 512;
	and.b64  	%rd12, %rd11, 9223372036854771712;
	or.b64  	%rd13, %rd12, %rd10;
	shl.b64 	%rd14, %rd13, 1;
	add.s64 	%rd15, %rd8, %rd14;
	mov.u32 	%r14, %tid.x;
	shl.b32 	%r15, %r14, 2;
	and.b32  	%r16, %r15, 60;
	setp.lt.u32 	%p1, %r14, 16;
	selp.b32 	%r17, 0, 64, %p1;
	or.b32  	%r18, %r16, %r17;
	mul.wide.u32 	%rd16, %r18, 2;
	add.s64 	%rd17, %rd15, %rd16;
	add.s64 	%rd1, %rd17, 4096;
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	ld.global.v2.b32 { %r2, %r3 }, [ %rd1 + 0 ];
	// end inline asm
	mov.b32 	{%rs1, %rs2}, %r2;
	mov.b32 	{%rs3, %rs4}, %r3;
	// begin inline asm
	cvt.f32.bf16 %r4, %rs1;
	// end inline asm
	mov.b32 	%f1, %r4;
	// begin inline asm
	cvt.f32.bf16 %r5, %rs2;
	// end inline asm
	mov.b32 	%f2, %r5;
	// begin inline asm
	cvt.f32.bf16 %r6, %rs3;
	// end inline asm
	mov.b32 	%f3, %r6;
	// begin inline asm
	cvt.f32.bf16 %r7, %rs4;
	// end inline asm
	mov.b32 	%f4, %r7;
	mul.rn.f32 	%f5, %f1, %f1;
	mul.rn.f32 	%f6, %f2, %f2;
	mul.rn.f32 	%f7, %f3, %f3;
	mul.rn.f32 	%f8, %f4, %f4;
	add.rn.f32 	%f9, %f5, %f6;
	add.rn.f32 	%f10, %f9, %f7;
	add.rn.f32 	%f11, %f10, %f8;
	mov.b32 	%r19, %f11;
	shfl.sync.bfly.b32	%r20, %r19, 16, 31, -1;
	mov.b32 	%f12, %r20;
	add.rn.f32 	%f13, %f11, %f12;
	mov.b32 	%r21, %f13;
	shfl.sync.bfly.b32	%r22, %r21, 8, 31, -1;
	mov.b32 	%f14, %r22;
	add.rn.f32 	%f15, %f13, %f14;
	mov.b32 	%r23, %f15;
	shfl.sync.bfly.b32	%r24, %r23, 4, 31, -1;
	mov.b32 	%f16, %r24;
	add.rn.f32 	%f17, %f15, %f16;
	mov.b32 	%r25, %f17;
	shfl.sync.bfly.b32	%r26, %r25, 2, 31, -1;
	mov.b32 	%f18, %r26;
	add.rn.f32 	%f19, %f17, %f18;
	mov.b32 	%r27, %f19;
	shfl.sync.bfly.b32	%r28, %r27, 1, 31, -1;
	mov.b32 	%f20, %r28;
	add.rn.f32 	%f21, %f19, %f20;
	mul.rn.f32 	%f22, %f21, 0f3C000000;
	// begin inline asm
	mov.u32 %r8, 0x0;
	ld.global.b32 { %r8 }, [ %rd2 + 0 ];
	// end inline asm
	mov.b32 	%f23, %r8;
	add.rn.f32 	%f24, %f22, %f23;
	rsqrt.approx.f32 	%f25, %f24;
	mul.rn.f32 	%f26, %f1, %f25;
	mul.rn.f32 	%f27, %f2, %f25;
	mul.rn.f32 	%f28, %f3, %f25;
	mul.rn.f32 	%f29, %f4, %f25;
	add.s64 	%rd18, %rd6, %rd11;
	mul.wide.u32 	%rd19, %r18, 4;
	add.s64 	%rd3, %rd18, %rd19;
	mov.b32 	%r9, %f26;
	mov.b32 	%r10, %f27;
	mov.b32 	%r11, %f28;
	mov.b32 	%r12, %f29;
	// begin inline asm
	st.global.v4.b32 [ %rd3 + 0 ], { %r9, %r10, %r11, %r12 };
	// end inline asm
	ret;

}
	// .globl	input_concatenate_fusion
.visible .entry input_concatenate_fusion(
	.param .u64 .ptr .align 128 input_concatenate_fusion_param_0,
	.param .u64 .ptr .align 16 input_concatenate_fusion_param_1,
	.param .u64 .ptr .align 16 input_concatenate_fusion_param_2,
	.param .u64 .ptr .align 16 input_concatenate_fusion_param_3,
	.param .u64 .ptr .align 128 input_concatenate_fusion_param_4
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<7>;
	.reg .b32 	%r<17>;
	.reg .f32 	%f<15>;
	.reg .b64 	%rd<25>;

	ld.param.u64 	%rd1, [input_concatenate_fusion_param_0];
	ld.param.u64 	%rd2, [input_concatenate_fusion_param_4];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [input_concatenate_fusion_param_1];
	ld.param.u64 	%rd5, [input_concatenate_fusion_param_3];
	cvta.to.global.u64 	%rd6, %rd5;
	ld.param.u64 	%rd7, [input_concatenate_fusion_param_2];
	cvta.to.global.u64 	%rd8, %rd7;
	cvta.to.global.u64 	%rd9, %rd4;
	cvta.to.global.u64 	%rd10, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	and.b32  	%r3, %r1, 63;
	shl.b32 	%r4, %r1, 1;
	and.b32  	%r5, %r4, 128;
	shl.b32 	%r6, %r2, 8;
	or.b32  	%r7, %r5, %r6;
	or.b32  	%r8, %r7, %r3;
	mul.wide.u32 	%rd11, %r8, 4;
	add.s64 	%rd12, %rd10, %rd11;
	ld.global.nc.f32 	%f1, [%rd12];
	mul.wide.u32 	%rd13, %r3, 2;
	add.s64 	%rd14, %rd9, %rd13;
	ld.global.nc.u16 	%rs1, [%rd14];
	cvt.f32.bf16 	%f2, %rs1;
	mul.rn.f32 	%f3, %f1, %f2;
	and.b32  	%r9, %r2, -4;
	cvt.u64.u32 	%rd15, %r9;
	add.s64 	%rd16, %rd6, %rd15;
	ld.global.nc.u32 	%r10, [%rd16];
	min.s32 	%r11, %r10, 40959;
	max.s32 	%r12, %r11, 0;
	shl.b32 	%r13, %r12, 7;
	or.b32  	%r14, %r13, %r3;
	mul.wide.u32 	%rd17, %r14, 2;
	add.s64 	%rd18, %rd8, %rd17;
	ld.global.nc.u16 	%rs2, [%rd18];
	cvt.f32.bf16 	%f4, %rs2;
	mul.rn.f32 	%f5, %f3, %f4;
	or.b32  	%r15, %r1, 64;
	ld.global.nc.f32 	%f6, [%rd12+256];
	mul.wide.u32 	%rd19, %r15, 2;
	add.s64 	%rd20, %rd9, %rd19;
	ld.global.nc.u16 	%rs3, [%rd20];
	cvt.f32.bf16 	%f7, %rs3;
	mul.rn.f32 	%f8, %f6, %f7;
	or.b32  	%r16, %r13, %r15;
	mul.wide.u32 	%rd21, %r16, 2;
	add.s64 	%rd22, %rd8, %rd21;
	ld.global.nc.u16 	%rs4, [%rd22];
	cvt.f32.bf16 	%f9, %rs4;
	mul.rn.f32 	%f10, %f8, %f9;
	sub.rn.f32 	%f11, %f5, %f10;
	cvt.rn.bf16.f32 	%rs5, %f11;
	mul.wide.u32 	%rd23, %r8, 2;
	add.s64 	%rd24, %rd3, %rd23;
	st.global.b16 	[%rd24], %rs5;
	mul.rn.f32 	%f12, %f8, %f4;
	mul.rn.f32 	%f13, %f3, %f9;
	add.rn.f32 	%f14, %f12, %f13;
	cvt.rn.bf16.f32 	%rs6, %f14;
	st.global.b16 	[%rd24+128], %rs6;
	ret;

}
	// .globl	loop_slice_fusion
.visible .entry loop_slice_fusion(
	.param .u64 .ptr .align 16 loop_slice_fusion_param_0,
	.param .u64 .ptr .align 128 loop_slice_fusion_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	ld.param.u64 	%rd1, [loop_slice_fusion_param_0];
	ld.param.u64 	%rd2, [loop_slice_fusion_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r1, 2;
	shl.b32 	%r4, %r2, 9;
	or.b32  	%r5, %r3, %r4;
	mul.wide.u32 	%rd5, %r5, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd6+138706944];
	add.s64 	%rd7, %rd3, %rd5;
	st.global.v4.b16 	[%rd7], {%rs1, %rs2, %rs3, %rs4};
	ret;

}
	// .globl	wrapped_slice_1
.visible .entry wrapped_slice_1(
	.param .u64 .ptr .align 16 wrapped_slice_1_param_0,
	.param .u64 .ptr .align 128 wrapped_slice_1_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	ld.param.u64 	%rd1, [wrapped_slice_1_param_0];
	ld.param.u64 	%rd2, [wrapped_slice_1_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r1, 2;
	shl.b32 	%r4, %r2, 9;
	or.b32  	%r5, %r3, %r4;
	mul.wide.u32 	%rd5, %r5, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd6];
	add.s64 	%rd7, %rd3, %rd5;
	st.global.v4.b16 	[%rd7], {%rs1, %rs2, %rs3, %rs4};
	ret;

}
