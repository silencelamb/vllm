HloModule SyncTensorsGraph.219, is_scheduled=true, entry_computation_layout={(bf16[128]{0}, f32[], bf16[4096,1024]{1,0}, bf16[1024]{0}, s32[32]{0}, /*index=5*/bf16[151936,1024]{1,0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=10*/s32[32]{0}, bf16[40960,128]{1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0})->(bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})}, frontend_attributes={fingerprint_before_lhs="3eb5f1a8d8fedeeabe08a3ddaaf4a863"}

%fused_gather (param_0.9: bf16[151936,1024], param_1.100: s32[32]) -> bf16[32,1,1024] {
  %param_0.9 = bf16[151936,1024]{1,0} parameter(0)
  %param_1.100 = s32[32]{0} parameter(1)
  %convert.114.1 = s64[32]{0} convert(s32[32]{0} %param_1.100), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.115.1 = u32[32]{0} convert(s64[32]{0} %convert.114.1), metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.457.1 = u32[32,1]{1,0} bitcast(u32[32]{0} %convert.115.1)
  ROOT %gather.3 = bf16[32,1,1024]{2,0,1} gather(bf16[151936,1024]{1,0} %param_0.9, u32[32,1]{1,0} %bitcast.457.1), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,1024}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%gemm_fusion_dot.29_computation (parameter_0: bf16[1024,2048]) -> bf16[32,1024] {
  %parameter_0 = bf16[1024,2048]{1,0} parameter(0)
  %constant_23 = bf16[] constant(0), metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.28 = bf16[32,2048]{1,0} broadcast(bf16[] %constant_23), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %dot.5 = bf16[1024,32]{0,1} dot(bf16[1024,2048]{1,0} %parameter_0, bf16[32,2048]{1,0} %broadcast.28), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %bitcast.225 = bf16[32,1024]{1,0} bitcast(bf16[1024,32]{0,1} %dot.5), metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%AddComputation.48 (x.49: f32[], y.50: f32[]) -> f32[] {
  %y.50 = f32[] parameter(1)
  %x.49 = f32[] parameter(0)
  ROOT %add.26 = f32[] add(f32[] %x.49, f32[] %y.50)
}

%fused_computation.17 (param_0.106: f32[], param_1.95: bf16[32,1,1024], param_2.80: bf16[32,1024], param_3.54: bf16[1024]) -> bf16[32,1024] {
  %param_2.80 = bf16[32,1024]{1,0} parameter(2)
  %convert.113.9 = f32[32,1024]{1,0} convert(bf16[32,1024]{1,0} %param_2.80), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.95 = bf16[32,1,1024]{2,0,1} parameter(1)
  %bitcast.460.11 = bf16[32,1024]{1,0} bitcast(bf16[32,1,1024]{2,0,1} %param_1.95)
  %convert.116.11 = f32[32,1024]{1,0} convert(bf16[32,1024]{1,0} %bitcast.460.11), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.27.9 = f32[32,1024]{1,0} add(f32[32,1024]{1,0} %convert.113.9, f32[32,1024]{1,0} %convert.116.11), metadata={op_type="aten__add" op_name="aten__add.520/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.84 = f32[32,1024]{1,0} multiply(f32[32,1024]{1,0} %add.27.9, f32[32,1024]{1,0} %add.27.9), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_71 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.9 = f32[32]{0} reduce(f32[32,1024]{1,0} %multiply.84, f32[] %constant_71), dimensions={1}, to_apply=%AddComputation.48, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_70 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.92 = f32[32]{0} broadcast(f32[] %constant_70), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.82 = f32[32]{0} multiply(f32[32]{0} %reduce.9, f32[32]{0} %broadcast.92), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.106 = f32[] parameter(0)
  %broadcast.91 = f32[32]{0} broadcast(f32[] %param_0.106), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.521/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.35 = f32[32]{0} add(f32[32]{0} %multiply.82, f32[32]{0} %broadcast.91), metadata={op_type="aten__add" op_name="aten__add.521/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.18 = f32[32]{0} rsqrt(f32[32]{0} %add.35), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.89 = f32[32,1024]{1,0} broadcast(f32[32]{0} %rsqrt.18), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.522/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.80 = f32[32,1024]{1,0} multiply(f32[32,1024]{1,0} %add.27.9, f32[32,1024]{1,0} %broadcast.89), metadata={op_type="aten__mul" op_name="aten__mul.522/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.54 = bf16[1024]{0} parameter(3)
  %convert.119.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.54), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.66.3 = f32[32,1024]{1,0} broadcast(f32[1024]{0} %convert.119.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.523/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.58.3 = f32[32,1024]{1,0} multiply(f32[32,1024]{1,0} %multiply.80, f32[32,1024]{1,0} %broadcast.66.3), metadata={op_type="aten__mul" op_name="aten__mul.523/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.120.1 = bf16[32,1024]{1,0} convert(f32[32,1024]{1,0} %multiply.58.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert (param_0.108: bf16[32,6144]) -> bf16[32,3072] {
  %param_0.108 = bf16[32,6144]{1,0} parameter(0)
  %slice.27.1 = bf16[32,3072]{1,0} slice(bf16[32,6144]{1,0} %param_0.108), slice={[0:32], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.122.8 = f32[32,3072]{1,0} convert(bf16[32,3072]{1,0} %slice.27.1)
  %constant_1_1 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.139.1 = f32[] convert(bf16[] %constant_1_1)
  %broadcast.76.3 = f32[32,3072]{1,0} broadcast(f32[] %convert.139.1), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.3.7 = f32[32,3072]{1,0} negate(f32[32,3072]{1,0} %convert.122.8)
  %convert.126.5 = bf16[32,3072]{1,0} convert(f32[32,3072]{1,0} %negate.3.7)
  %exponential.3.3 = bf16[32,3072]{1,0} exponential(bf16[32,3072]{1,0} %convert.126.5)
  %convert.128.1 = f32[32,3072]{1,0} convert(bf16[32,3072]{1,0} %exponential.3.3)
  %add.28.7 = f32[32,3072]{1,0} add(f32[32,3072]{1,0} %broadcast.76.3, f32[32,3072]{1,0} %convert.128.1)
  %divide.3.7 = f32[32,3072]{1,0} divide(f32[32,3072]{1,0} %broadcast.76.3, f32[32,3072]{1,0} %add.28.7), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.59.5 = f32[32,3072]{1,0} multiply(f32[32,3072]{1,0} %convert.122.8, f32[32,3072]{1,0} %divide.3.7), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.28.1 = bf16[32,3072]{1,0} slice(bf16[32,6144]{1,0} %param_0.108), slice={[0:32], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.129.1 = f32[32,3072]{1,0} convert(bf16[32,3072]{1,0} %slice.28.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.60.3 = f32[32,3072]{1,0} multiply(f32[32,3072]{1,0} %multiply.59.5, f32[32,3072]{1,0} %convert.129.1), metadata={op_type="aten__mul" op_name="aten__mul.524/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.130.1 = bf16[32,3072]{1,0} convert(f32[32,3072]{1,0} %multiply.60.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.18 (param_0.109: f32[], param_1.99: bf16[32,1,1024], param_2.82: bf16[32,1024], param_3.55: bf16[32,1024], param_4.19: bf16[1024]) -> bf16[32,1024] {
  %param_3.55 = bf16[32,1024]{1,0} parameter(3)
  %convert.131.5 = f32[32,1024]{1,0} convert(bf16[32,1024]{1,0} %param_3.55), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.82 = bf16[32,1024]{1,0} parameter(2)
  %convert.113.11 = f32[32,1024]{1,0} convert(bf16[32,1024]{1,0} %param_2.82), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.99 = bf16[32,1,1024]{2,0,1} parameter(1)
  %bitcast.460.13 = bf16[32,1024]{1,0} bitcast(bf16[32,1,1024]{2,0,1} %param_1.99)
  %convert.116.13 = f32[32,1024]{1,0} convert(bf16[32,1024]{1,0} %bitcast.460.13), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.27.11 = f32[32,1024]{1,0} add(f32[32,1024]{1,0} %convert.113.11, f32[32,1024]{1,0} %convert.116.13), metadata={op_type="aten__add" op_name="aten__add.520/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.29.5 = f32[32,1024]{1,0} add(f32[32,1024]{1,0} %convert.131.5, f32[32,1024]{1,0} %add.27.11), metadata={op_type="aten__add" op_name="aten__add.525/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.91 = f32[32,1024]{1,0} multiply(f32[32,1024]{1,0} %add.29.5, f32[32,1024]{1,0} %add.29.5), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_76 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.11 = f32[32]{0} reduce(f32[32,1024]{1,0} %multiply.91, f32[] %constant_76), dimensions={1}, to_apply=%AddComputation.48, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_75 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.98 = f32[32]{0} broadcast(f32[] %constant_75), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.90 = f32[32]{0} multiply(f32[32]{0} %reduce.11, f32[32]{0} %broadcast.98), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.109 = f32[] parameter(0)
  %broadcast.97 = f32[32]{0} broadcast(f32[] %param_0.109), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.521/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.37 = f32[32]{0} add(f32[32]{0} %multiply.90, f32[32]{0} %broadcast.97), metadata={op_type="aten__add" op_name="aten__add.526/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.20 = f32[32]{0} rsqrt(f32[32]{0} %add.37), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.96 = f32[32,1024]{1,0} broadcast(f32[32]{0} %rsqrt.20), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.527/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.89 = f32[32,1024]{1,0} multiply(f32[32,1024]{1,0} %add.29.5, f32[32,1024]{1,0} %broadcast.96), metadata={op_type="aten__mul" op_name="aten__mul.527/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.19 = bf16[1024]{0} parameter(4)
  %convert.132.1 = f32[1024]{0} convert(bf16[1024]{0} %param_4.19), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.70.3 = f32[32,1024]{1,0} broadcast(f32[1024]{0} %convert.132.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.528/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.62.3 = f32[32,1024]{1,0} multiply(f32[32,1024]{1,0} %multiply.89, f32[32,1024]{1,0} %broadcast.70.3), metadata={op_type="aten__mul" op_name="aten__mul.528/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.133.1 = bf16[32,1024]{1,0} convert(f32[32,1024]{1,0} %multiply.62.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_slice_computation (param_0.111: bf16[32,4096]) -> bf16[32,1024] {
  %param_0.111 = bf16[32,4096]{1,0} parameter(0)
  ROOT %slice.34.1 = bf16[32,1024]{1,0} slice(bf16[32,4096]{1,0} %param_0.111), slice={[0:32], [3072:4096]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.14 (param_0.107: f32[], param_1.96: bf16[32,4096], param_2.81: bf16[128]) -> f32[32,8,128] {
  %param_1.96 = bf16[32,4096]{1,0} parameter(1)
  %slice.29.1 = bf16[32,1024]{1,0} slice(bf16[32,4096]{1,0} %param_1.96), slice={[0:32], [2048:3072]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.522.5 = bf16[32,8,128]{2,1,0} bitcast(bf16[32,1024]{1,0} %slice.29.1)
  %convert.134.5 = f32[32,8,128]{2,1,0} convert(bf16[32,8,128]{2,1,0} %bitcast.522.5), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.76 = f32[32,8,128]{2,1,0} multiply(f32[32,8,128]{2,1,0} %convert.134.5, f32[32,8,128]{2,1,0} %convert.134.5), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_57 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.7 = f32[32,8]{1,0} reduce(f32[32,8,128]{2,1,0} %multiply.76, f32[] %constant_57), dimensions={2}, to_apply=%AddComputation.48, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_56 = f32[] constant(0.0078125), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.84 = f32[32,8]{1,0} broadcast(f32[] %constant_56), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.75 = f32[32,8]{1,0} multiply(f32[32,8]{1,0} %reduce.7, f32[32,8]{1,0} %broadcast.84), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.107 = f32[] parameter(0)
  %broadcast.83 = f32[32,8]{1,0} broadcast(f32[] %param_0.107), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.529/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.32 = f32[32,8]{1,0} add(f32[32,8]{1,0} %multiply.75, f32[32,8]{1,0} %broadcast.83), metadata={op_type="aten__add" op_name="aten__add.529/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.16 = f32[32,8]{1,0} rsqrt(f32[32,8]{1,0} %add.32), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.82 = f32[32,8,128]{2,1,0} broadcast(f32[32,8]{1,0} %rsqrt.16), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul.530/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.73 = f32[32,8,128]{2,1,0} multiply(f32[32,8,128]{2,1,0} %convert.134.5, f32[32,8,128]{2,1,0} %broadcast.82), metadata={op_type="aten__mul" op_name="aten__mul.530/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.81 = bf16[128]{0} parameter(2)
  %convert.135.1 = f32[128]{0} convert(bf16[128]{0} %param_2.81), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.71.1 = f32[32,8,128]{2,1,0} broadcast(f32[128]{0} %convert.135.1), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul.531/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %multiply.63.1 = f32[32,8,128]{2,1,0} multiply(f32[32,8,128]{2,1,0} %multiply.73, f32[32,8,128]{2,1,0} %broadcast.71.1), metadata={op_type="aten__mul" op_name="aten__mul.531/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_concatenate (param_0.105: f32[32,8,128], param_1.102: bf16[40960,128], param_2.84: s32[32]) -> bf16[32,8,128] {
  %param_0.105 = f32[32,8,128]{2,1,0} parameter(0)
  %slice.30.4 = f32[32,8,64]{2,1,0} slice(f32[32,8,128]{2,1,0} %param_0.105), slice={[0:32], [0:8], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.102 = bf16[40960,128]{1,0} parameter(1)
  %param_2.84 = s32[32]{0} parameter(2)
  %bitcast.538.3 = s32[32,1]{1,0} bitcast(s32[32]{0} %param_2.84)
  %gather.1.3 = bf16[32,1,128]{2,0,1} gather(bf16[40960,128]{1,0} %param_1.102, s32[32,1]{1,0} %bitcast.538.3), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,128}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.541.7 = bf16[32,128]{1,0} bitcast(bf16[32,1,128]{2,0,1} %gather.1.3)
  %convert.136.7 = f32[32,128]{1,0} convert(bf16[32,128]{1,0} %bitcast.541.7), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.31.3 = f32[32,64]{1,0} slice(f32[32,128]{1,0} %convert.136.7), slice={[0:32], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.72.12 = f32[32,8,64]{2,1,0} broadcast(f32[32,64]{1,0} %slice.31.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.532/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.65.7 = f32[32,8,64]{2,1,0} multiply(f32[32,8,64]{2,1,0} %slice.30.4, f32[32,8,64]{2,1,0} %broadcast.72.12), metadata={op_type="aten__mul" op_name="aten__mul.532/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.32.4 = f32[32,8,64]{2,1,0} slice(f32[32,8,128]{2,1,0} %param_0.105), slice={[0:32], [0:8], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.33.3 = f32[32,64]{1,0} slice(f32[32,128]{1,0} %convert.136.7), slice={[0:32], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.74.8 = f32[32,8,64]{2,1,0} broadcast(f32[32,64]{1,0} %slice.33.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.533/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.66.5 = f32[32,8,64]{2,1,0} multiply(f32[32,8,64]{2,1,0} %slice.32.4, f32[32,8,64]{2,1,0} %broadcast.74.8), metadata={op_type="aten__mul" op_name="aten__mul.533/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %subtract.2.5 = f32[32,8,64]{2,1,0} subtract(f32[32,8,64]{2,1,0} %multiply.65.7, f32[32,8,64]{2,1,0} %multiply.66.5), metadata={op_type="aten__sub" op_name="aten__sub.534/aten__sub" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.137.3 = bf16[32,8,64]{2,1,0} convert(f32[32,8,64]{2,1,0} %subtract.2.5)
  %multiply.67.7 = f32[32,8,64]{2,1,0} multiply(f32[32,8,64]{2,1,0} %slice.32.4, f32[32,8,64]{2,1,0} %broadcast.72.12), metadata={op_type="aten__mul" op_name="aten__mul.535/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.68.5 = f32[32,8,64]{2,1,0} multiply(f32[32,8,64]{2,1,0} %slice.30.4, f32[32,8,64]{2,1,0} %broadcast.74.8), metadata={op_type="aten__mul" op_name="aten__mul.536/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.30.5 = f32[32,8,64]{2,1,0} add(f32[32,8,64]{2,1,0} %multiply.67.7, f32[32,8,64]{2,1,0} %multiply.68.5), metadata={op_type="aten__add" op_name="aten__add.537/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.138.3 = bf16[32,8,64]{2,1,0} convert(f32[32,8,64]{2,1,0} %add.30.5)
  ROOT %concatenate.4.1 = bf16[32,8,128]{2,1,0} concatenate(bf16[32,8,64]{2,1,0} %convert.137.3, bf16[32,8,64]{2,1,0} %convert.138.3), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_slice (param_0.1: bf16[2,4233,16,8,128]) -> bf16[69353472] {
  %param_0.1 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  %bitcast.582.1 = bf16[138706944]{0} bitcast(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.1)
  ROOT %slice.36.1 = bf16[69353472]{0} slice(bf16[138706944]{0} %bitcast.582.1), slice={[69353472:138706944]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%wrapped_slice_computation.1 (param_0.112: bf16[2,4233,16,8,128]) -> bf16[1,4233,16,8,128] {
  %param_0.112 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  ROOT %slice.35.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} slice(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.112), slice={[0:1], [0:4233], [0:16], [0:8], [0:128]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%command_buffer (p: bf16[151936,1024], p.1: s32[32], p.2: bf16[1024,2048], p.3: f32[], p.4: bf16[1024], p.5: bf16[6144,1024], p.6: bf16[1024,3072], p.7: bf16[1024], p.8: bf16[4096,1024], p.9: bf16[128], p.10: bf16[40960,128], p.11: s32[32], p.12: bf16[2,4233,16,8,128]) -> (bf16[32,8,128], bf16[32,8,128], bf16[4233,16,8,128], bf16[1,4233,16,8,128]) {
  %p = bf16[151936,1024]{1,0} parameter(0)
  %p.1 = s32[32]{0} parameter(1)
  %p.2 = bf16[1024,2048]{1,0} parameter(2)
  %p.3 = f32[] parameter(3)
  %p.4 = bf16[1024]{0} parameter(4)
  %p.5 = bf16[6144,1024]{1,0} parameter(5)
  %p.6 = bf16[1024,3072]{1,0} parameter(6)
  %p.7 = bf16[1024]{0} parameter(7)
  %p.8 = bf16[4096,1024]{1,0} parameter(8)
  %p.9 = bf16[128]{0} parameter(9)
  %p.10 = bf16[40960,128]{1,0} parameter(10)
  %p.11 = s32[32]{0} parameter(11)
  %p.12 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(12)
  %loop_gather_fusion = bf16[32,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[32]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %gemm_fusion_dot.29.0 = bf16[32,1024]{1,0} fusion(bf16[1024,2048]{1,0} %p.2), kind=kCustom, calls=%gemm_fusion_dot.29_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"32","block_n":"16","block_k":"512","split_k":"1","num_stages":"1","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
  %fusion.24 = bf16[32,1024]{1,0} fusion(f32[] %p.3, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,1024]{1,0} %gemm_fusion_dot.29.0, bf16[1024]{0} %p.4), kind=kCustom, calls=%fused_computation.17, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","256"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.3.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.24, bf16[6144,1024]{1,0} %p.5), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.3 = bf16[32,6144]{1,0} get-tuple-element((bf16[32,6144]{1,0}, s8[4194304]{0}) %custom-call.3.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.3), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %custom-call.4.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.6), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.1.0 = bf16[32,1024]{1,0} get-tuple-element((bf16[32,1024]{1,0}, s8[4194304]{0}) %custom-call.4.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.25 = bf16[32,1024]{1,0} fusion(f32[] %p.3, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,1024]{1,0} %gemm_fusion_dot.29.0, bf16[32,1024]{1,0} %get-tuple-element.1.0, bf16[1024]{0} %p.7), kind=kCustom, calls=%fused_computation.18, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.5.0 = (bf16[32,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.25, bf16[4096,1024]{1,0} %p.8), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.2.0 = bf16[32,4096]{1,0} get-tuple-element((bf16[32,4096]{1,0}, s8[4194304]{0}) %custom-call.5.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_slice = bf16[32,1024]{1,0} fusion(bf16[32,4096]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.21 = f32[32,8,128]{2,1,0} fusion(f32[] %p.3, bf16[32,4096]{1,0} %get-tuple-element.2.0, bf16[128]{0} %p.9), kind=kCustom, calls=%fused_computation.14, metadata={op_type="aten__mul" op_name="aten__mul.531/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %input_concatenate_fusion = bf16[32,8,128]{2,1,0} fusion(f32[32,8,128]{2,1,0} %fusion.21, bf16[40960,128]{1,0} %p.10, s32[32]{0} %p.11), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.574.0 = bf16[32,8,128]{2,1,0} bitcast(bf16[32,1024]{1,0} %wrapped_slice)
  %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.12), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  %bitcast.586.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[69353472]{0} %loop_slice_fusion)
  %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.12), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  ROOT %tuple = (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[32,8,128]{2,1,0} %input_concatenate_fusion, bf16[32,8,128]{2,1,0} %bitcast.574.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.586.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
}

ENTRY %SyncTensorsGraph.219 (p0.1.0: bf16[128], p1.4.0: f32[], p2.6.0: bf16[4096,1024], p3.8.0: bf16[1024], p4.14.0: s32[32], p5.16.0: bf16[151936,1024], p6.20.0: bf16[1024,2048], p7.36.0: bf16[1024,3072], p8.38.0: bf16[6144,1024], p9.40.0: bf16[1024], p10.164.0: s32[32], p11.165.0: bf16[40960,128], p12.209.0: bf16[2,4233,16,8,128]) -> (bf16[32,8,128], bf16[32,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128]) {
  %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %p12.209.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p11.165.0 = bf16[40960,128]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p10.164.0 = s32[32]{0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p9.40.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p8.38.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p7.36.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p6.20.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p5.16.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p4.14.0 = s32[32]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %call = (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) call(bf16[151936,1024]{1,0} %p5.16.0, s32[32]{0} %p4.14.0, bf16[1024,2048]{1,0} %p6.20.0, f32[] %p1.4.0, bf16[1024]{0} %p9.40.0, /*index=5*/bf16[6144,1024]{1,0} %p8.38.0, bf16[1024,3072]{1,0} %p7.36.0, bf16[1024]{0} %p3.8.0, bf16[4096,1024]{1,0} %p2.6.0, bf16[128]{0} %p0.1.0, /*index=10*/bf16[40960,128]{1,0} %p11.165.0, s32[32]{0} %p10.164.0, bf16[2,4233,16,8,128]{4,3,2,1,0} %p12.209.0), to_apply=%command_buffer
  %get-tuple-element.5 = bf16[32,8,128]{2,1,0} get-tuple-element((bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=0
  %get-tuple-element.6 = bf16[32,8,128]{2,1,0} get-tuple-element((bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=1
  %get-tuple-element.7 = bf16[4233,16,8,128]{3,2,1,0} get-tuple-element((bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=2
  %get-tuple-element.8 = bf16[1,4233,16,8,128]{4,3,2,1,0} get-tuple-element((bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=3
  %bitcast.579.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[1,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.8)
  ROOT %tuple.218.0 = (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[32,8,128]{2,1,0} %get-tuple-element.5, bf16[32,8,128]{2,1,0} %get-tuple-element.6, bf16[4233,16,8,128]{3,2,1,0} %bitcast.579.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.7)
}

