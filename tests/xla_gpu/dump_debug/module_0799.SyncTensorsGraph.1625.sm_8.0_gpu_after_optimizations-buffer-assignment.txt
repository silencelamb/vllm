BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <1290 p5.54.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 88, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1378 p88.1615.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <1114 gemm_fusion_dot.80.0 @0> (size=655360,offset=0): bf16[16,20480]{1,0}
 value: <1282 custom-call.81.0{0} @0> (size=131072,offset=0): bf16[16,4096]{1,0}
 value: <1287 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <1113 wrapped_concatenate @0> (size=83886080,offset=0): bf16[20480,2048]{1,0}
 value: <1119 custom-call.41.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1123 custom-call.42.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1127 custom-call.43.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1131 custom-call.44.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1135 custom-call.45.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1139 custom-call.46.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1143 custom-call.47.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1147 custom-call.48.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1151 custom-call.49.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1155 custom-call.50.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1160 custom-call.51.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1164 custom-call.52.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1168 custom-call.53.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1172 custom-call.54.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1176 custom-call.55.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1180 custom-call.56.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1184 custom-call.57.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1188 custom-call.58.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1193 custom-call.59.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1197 custom-call.60.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1201 custom-call.61.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1205 custom-call.62.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1209 custom-call.63.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1213 custom-call.64.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1217 custom-call.65.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1221 custom-call.66.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1226 custom-call.67.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1230 custom-call.68.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1234 custom-call.69.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1238 custom-call.70.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1242 custom-call.71.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1246 custom-call.72.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1250 custom-call.73.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1254 custom-call.74.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1259 custom-call.75.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1263 custom-call.76.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1267 custom-call.77.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1271 custom-call.78.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1275 custom-call.79.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1279 custom-call.80.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1283 custom-call.81.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1284 triton_softmax.22.0 @0> (size=65536,offset=0): f32[16,8,128]{2,1,0}
 value: <1288 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1314 p8.76.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1317 p12.148.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1320 p16.220.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 12582912, parameter 20, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1323 p20.292.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 8: size 12582912, parameter 24, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1326 p24.364.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 9: size 12582912, parameter 28, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1329 p28.436.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 10: size 12582912, parameter 32, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1332 p32.508.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 11: size 12582912, parameter 36, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1335 p36.580.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 12: size 12582912, parameter 40, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1338 p40.652.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 13: size 12582912, parameter 44, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1341 p44.724.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 14: size 12582912, parameter 48, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1344 p48.796.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 15: size 12582912, parameter 52, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1347 p52.868.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 16: size 12582912, parameter 56, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1350 p56.940.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 17: size 12582912, parameter 60, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1353 p60.1012.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 18: size 12582912, parameter 64, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1356 p64.1084.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 19: size 12582912, parameter 68, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1359 p68.1156.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 20: size 12582912, parameter 72, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1362 p72.1228.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 21: size 12582912, parameter 76, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1365 p76.1300.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 22: size 12582912, parameter 80, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1368 p80.1372.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 23: size 12582912, parameter 84, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1371 p84.1444.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 24: size 10485760, parameter 87, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <1376 p87.1571.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 25: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <1374 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 26: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1315 p7.74.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 27: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1318 p11.146.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 28: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1321 p15.218.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 29: size 6291456, parameter 19, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1324 p19.290.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 30: size 6291456, parameter 23, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1327 p23.362.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 31: size 6291456, parameter 27, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1330 p27.434.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 32: size 6291456, parameter 31, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1333 p31.506.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 33: size 6291456, parameter 35, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1336 p35.578.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 34: size 6291456, parameter 39, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1339 p39.650.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 35: size 6291456, parameter 43, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1342 p43.722.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 36: size 6291456, parameter 47, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1345 p47.794.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 37: size 6291456, parameter 51, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1348 p51.866.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 38: size 6291456, parameter 55, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1351 p55.938.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 39: size 6291456, parameter 59, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1354 p59.1010.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 40: size 6291456, parameter 63, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1357 p63.1082.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 41: size 6291456, parameter 67, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1360 p67.1154.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 42: size 6291456, parameter 71, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1363 p71.1226.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 43: size 6291456, parameter 75, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1366 p75.1298.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 44: size 6291456, parameter 79, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1369 p79.1370.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 45: size 6291456, parameter 83, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1372 p83.1442.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 46: size 4194304, parameter 82, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1292 p82.1426.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 47: size 4194304, parameter 78, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1293 p78.1354.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 48: size 4194304, parameter 74, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1294 p74.1282.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 49: size 4194304, parameter 70, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1295 p70.1210.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 50: size 4194304, parameter 66, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1296 p66.1138.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 51: size 4194304, parameter 62, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1297 p62.1066.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 52: size 4194304, parameter 58, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1298 p58.994.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 53: size 4194304, parameter 54, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1299 p54.922.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 54: size 4194304, parameter 50, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1300 p50.850.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 55: size 4194304, parameter 46, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1301 p46.778.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 56: size 4194304, parameter 42, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1302 p42.706.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 57: size 4194304, parameter 38, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1303 p38.634.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 58: size 4194304, parameter 34, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1304 p34.562.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 59: size 4194304, parameter 30, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1305 p30.490.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 60: size 4194304, parameter 26, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1306 p26.418.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 61: size 4194304, parameter 22, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1307 p22.346.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 62: size 4194304, parameter 18, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1308 p18.274.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 63: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1309 p14.202.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 64: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1310 p10.130.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 65: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1311 p6.58.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 66: size 32768, maybe-live-out:
 value: <1116 fusion.272 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1122 custom-call.42.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1165 fusion.260 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1171 custom-call.54.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1198 fusion.252 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1204 custom-call.62.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1231 fusion.244 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1237 custom-call.70.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1264 fusion.236 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1270 custom-call.78.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1285 input_concatenate_fusion @0> (size=32768,offset=0): bf16[16,8,128]{2,1,0}
allocation 67: size 32768, maybe-live-out:
 value: <1115 loop_gather_fusion @0> (size=32768,offset=0): bf16[16,1,1024]{2,0,1}
 value: <1157 fusion.262 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1163 custom-call.52.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1190 fusion.254 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1196 custom-call.60.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1223 fusion.246 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1229 custom-call.68.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1256 fusion.238 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1262 custom-call.76.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <1286 wrapped_slice @0> (size=32768,offset=0): bf16[16,1024]{1,0}
allocation 68: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <1313 p9.78.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 69: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <1316 p13.150.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 70: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <1319 p17.222.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 71: size 2048, parameter 21, shape |bf16[1024]| at ShapeIndex {}:
 value: <1322 p21.294.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 72: size 2048, parameter 25, shape |bf16[1024]| at ShapeIndex {}:
 value: <1325 p25.366.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 73: size 2048, parameter 29, shape |bf16[1024]| at ShapeIndex {}:
 value: <1328 p29.438.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 74: size 2048, parameter 33, shape |bf16[1024]| at ShapeIndex {}:
 value: <1331 p33.510.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 75: size 2048, parameter 37, shape |bf16[1024]| at ShapeIndex {}:
 value: <1334 p37.582.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 76: size 2048, parameter 41, shape |bf16[1024]| at ShapeIndex {}:
 value: <1337 p41.654.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 77: size 2048, parameter 45, shape |bf16[1024]| at ShapeIndex {}:
 value: <1340 p45.726.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 78: size 2048, parameter 49, shape |bf16[1024]| at ShapeIndex {}:
 value: <1343 p49.798.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 79: size 2048, parameter 53, shape |bf16[1024]| at ShapeIndex {}:
 value: <1346 p53.870.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 80: size 2048, parameter 57, shape |bf16[1024]| at ShapeIndex {}:
 value: <1349 p57.942.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 81: size 2048, parameter 61, shape |bf16[1024]| at ShapeIndex {}:
 value: <1352 p61.1014.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 82: size 2048, parameter 65, shape |bf16[1024]| at ShapeIndex {}:
 value: <1355 p65.1086.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 83: size 2048, parameter 69, shape |bf16[1024]| at ShapeIndex {}:
 value: <1358 p69.1158.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 84: size 2048, parameter 73, shape |bf16[1024]| at ShapeIndex {}:
 value: <1361 p73.1230.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 85: size 2048, parameter 77, shape |bf16[1024]| at ShapeIndex {}:
 value: <1364 p77.1302.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 86: size 2048, parameter 81, shape |bf16[1024]| at ShapeIndex {}:
 value: <1367 p81.1374.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 87: size 2048, parameter 85, shape |bf16[1024]| at ShapeIndex {}:
 value: <1370 p85.1446.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 88: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <1373 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 89: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <1375 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 90: size 64, parameter 4, shape |s32[16]| at ShapeIndex {}:
 value: <1291 p4.52.0 @0> (size=64,offset=0): s32[16]{0}
allocation 91: size 64, parameter 86, shape |s32[16]| at ShapeIndex {}:
 value: <1377 p86.1570.0 @0> (size=64,offset=0): s32[16]{0}
allocation 92: size 32, output shape is |(bf16[16,8,128], bf16[16,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <1379 tuple.1624.0{} @0> (size=32,offset=0): (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 93: size 4, thread-local:
 value: <32 add.265 @0> (size=4,offset=0): f32[]
allocation 94: size 4, thread-local:
 value: <31 y.88 @0> (size=4,offset=0): f32[]
allocation 95: size 4, thread-local:
 value: <30 x.87 @0> (size=4,offset=0): f32[]
allocation 96: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <1312 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 97: size 464032, preallocated-temp:
 value: <1117 custom-call.41.0{} @0> (size=16,offset=463872): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1118 custom-call.41.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1120 loop_convert_fusion.19 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1121 custom-call.42.0{} @0> (size=16,offset=463744): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1124 fusion.270 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1125 custom-call.43.0{} @0> (size=16,offset=4864): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1126 custom-call.43.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1128 loop_convert_fusion.18 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1129 custom-call.44.0{} @0> (size=16,offset=0): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1130 custom-call.44.0{0} @0> (size=32768,offset=365440): bf16[16,1024]{1,0}
 value: <1132 fusion.268 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1133 custom-call.45.0{} @0> (size=16,offset=128): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1134 custom-call.45.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1136 loop_convert_fusion.17 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1137 custom-call.46.0{} @0> (size=16,offset=256): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1138 custom-call.46.0{0} @0> (size=32768,offset=398208): bf16[16,1024]{1,0}
 value: <1140 fusion.266 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1141 custom-call.47.0{} @0> (size=16,offset=384): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1142 custom-call.47.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1144 loop_convert_fusion.16 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1145 custom-call.48.0{} @0> (size=16,offset=512): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1146 custom-call.48.0{0} @0> (size=32768,offset=430976): bf16[16,1024]{1,0}
 value: <1148 fusion.264 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1149 custom-call.49.0{} @0> (size=16,offset=640): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1150 custom-call.49.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1152 loop_convert_fusion.15 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1153 custom-call.50.0{} @0> (size=16,offset=768): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1154 custom-call.50.0{0} @0> (size=32768,offset=4992): bf16[16,1024]{1,0}
 value: <1156 loop_add_fusion @0> (size=65536,offset=299904): f32[16,1024]{1,0}
 value: <1158 custom-call.51.0{} @0> (size=16,offset=896): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1159 custom-call.51.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1161 loop_convert_fusion.14 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1162 custom-call.52.0{} @0> (size=16,offset=1024): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1166 custom-call.53.0{} @0> (size=16,offset=1152): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1167 custom-call.53.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1169 loop_convert_fusion.13 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1170 custom-call.54.0{} @0> (size=16,offset=1280): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1173 fusion.258 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1174 custom-call.55.0{} @0> (size=16,offset=1408): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1175 custom-call.55.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1177 loop_convert_fusion.12 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1178 custom-call.56.0{} @0> (size=16,offset=1536): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1179 custom-call.56.0{0} @0> (size=32768,offset=365440): bf16[16,1024]{1,0}
 value: <1181 fusion.256 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1182 custom-call.57.0{} @0> (size=16,offset=1664): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1183 custom-call.57.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1185 loop_convert_fusion.11 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1186 custom-call.58.0{} @0> (size=16,offset=1792): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1187 custom-call.58.0{0} @0> (size=32768,offset=4992): bf16[16,1024]{1,0}
 value: <1189 loop_add_fusion.1 @0> (size=65536,offset=299904): f32[16,1024]{1,0}
 value: <1191 custom-call.59.0{} @0> (size=16,offset=1920): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1192 custom-call.59.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1194 loop_convert_fusion.10 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1195 custom-call.60.0{} @0> (size=16,offset=2048): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1199 custom-call.61.0{} @0> (size=16,offset=2176): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1200 custom-call.61.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1202 loop_convert_fusion.9 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1203 custom-call.62.0{} @0> (size=16,offset=2304): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1206 fusion.250 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1207 custom-call.63.0{} @0> (size=16,offset=2432): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1208 custom-call.63.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1210 loop_convert_fusion.8 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1211 custom-call.64.0{} @0> (size=16,offset=2560): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1212 custom-call.64.0{0} @0> (size=32768,offset=365440): bf16[16,1024]{1,0}
 value: <1214 fusion.248 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1215 custom-call.65.0{} @0> (size=16,offset=2688): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1216 custom-call.65.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1218 loop_convert_fusion.7 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1219 custom-call.66.0{} @0> (size=16,offset=2816): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1220 custom-call.66.0{0} @0> (size=32768,offset=4992): bf16[16,1024]{1,0}
 value: <1222 loop_add_fusion.2 @0> (size=65536,offset=299904): f32[16,1024]{1,0}
 value: <1224 custom-call.67.0{} @0> (size=16,offset=2944): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1225 custom-call.67.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1227 loop_convert_fusion.6 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1228 custom-call.68.0{} @0> (size=16,offset=3072): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1232 custom-call.69.0{} @0> (size=16,offset=3200): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1233 custom-call.69.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1235 loop_convert_fusion.5 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1236 custom-call.70.0{} @0> (size=16,offset=3328): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1239 fusion.242 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1240 custom-call.71.0{} @0> (size=16,offset=3456): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1241 custom-call.71.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1243 loop_convert_fusion.4 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1244 custom-call.72.0{} @0> (size=16,offset=3584): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1245 custom-call.72.0{0} @0> (size=32768,offset=365440): bf16[16,1024]{1,0}
 value: <1247 fusion.240 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1248 custom-call.73.0{} @0> (size=16,offset=3712): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1249 custom-call.73.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1251 loop_convert_fusion.3 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1252 custom-call.74.0{} @0> (size=16,offset=3840): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1253 custom-call.74.0{0} @0> (size=32768,offset=4992): bf16[16,1024]{1,0}
 value: <1255 loop_add_fusion.3 @0> (size=65536,offset=299904): f32[16,1024]{1,0}
 value: <1257 custom-call.75.0{} @0> (size=16,offset=3968): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1258 custom-call.75.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1260 loop_convert_fusion.2 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1261 custom-call.76.0{} @0> (size=16,offset=4096): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1265 custom-call.77.0{} @0> (size=16,offset=4224): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1266 custom-call.77.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1268 loop_convert_fusion.1 @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1269 custom-call.78.0{} @0> (size=16,offset=4352): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1272 fusion.234 @0> (size=32768,offset=201600): bf16[16,1024]{1,0}
 value: <1273 custom-call.79.0{} @0> (size=16,offset=4480): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <1274 custom-call.79.0{0} @0> (size=196608,offset=4992): bf16[16,6144]{1,0}
 value: <1276 loop_convert_fusion @0> (size=98304,offset=201600): bf16[16,3072]{1,0}
 value: <1277 custom-call.80.0{} @0> (size=16,offset=4608): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <1278 custom-call.80.0{0} @0> (size=32768,offset=4992): bf16[16,1024]{1,0}
 value: <1280 fusion.232 @0> (size=32768,offset=37760): bf16[16,1024]{1,0}
 value: <1281 custom-call.81.0{} @0> (size=16,offset=4736): (bf16[16,4096]{1,0}, s8[4194304]{0})
 value: <1289 tuple{} @0> (size=32,offset=464000): (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 1346813520 (1.25GiB)

Used values:
<30 x.87 @0>
 positions:
  x.87
 uses:
  add.265, operand 0
 from instruction: %x.87 = f32[] parameter(0)
<31 y.88 @0>
 positions:
  y.88
 uses:
  add.265, operand 1
 from instruction: %y.88 = f32[] parameter(1)
<32 add.265 @0>
 positions:
  add.265
 uses:
 from instruction: %add.265 = f32[] add(f32[] %x.87, f32[] %y.88)
<1113 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.80.0, operand 0
 from instruction: %wrapped_concatenate = bf16[20480,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9, bf16[1024,2048]{1,0} %p.10, bf16[1024,2048]{1,0} %p.11, /*index=10*/bf16[1024,2048]{1,0} %p.12, bf16[1024,2048]{1,0} %p.13, bf16[1024,2048]{1,0} %p.14, bf16[1024,2048]{1,0} %p.15, bf16[1024,2048]{1,0} %p.16, /*index=15*/bf16[1024,2048]{1,0} %p.17, bf16[1024,2048]{1,0} %p.18, bf16[1024,2048]{1,0} %p.19, bf16[1024,2048]{1,0} %p.20, bf16[1024,2048]{1,0} %p.21), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1114 gemm_fusion_dot.80.0 @0>
 positions:
  gemm_fusion_dot.80.0
 uses:
  fusion.272, operand 2
  fusion.270, operand 3
  fusion.268, operand 3
  fusion.266, operand 3
  fusion.264, operand 3
  loop_add_fusion, operand 0
  fusion.260, operand 4
  fusion.258, operand 4
  fusion.256, operand 3
  loop_add_fusion.1, operand 0
  fusion.252, operand 4
  fusion.250, operand 4
  fusion.248, operand 3
  loop_add_fusion.2, operand 0
  fusion.244, operand 4
  fusion.242, operand 4
  fusion.240, operand 3
  loop_add_fusion.3, operand 0
  fusion.236, operand 4
  fusion.234, operand 4
  fusion.232, operand 5
 from instruction: %gemm_fusion_dot.80.0 = bf16[16,20480]{1,0} fusion(bf16[20480,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.80_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"128","block_n":"16","block_k":"128","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<1115 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.272, operand 1
  fusion.270, operand 2
  fusion.268, operand 4
  fusion.266, operand 2
  fusion.264, operand 4
  loop_add_fusion, operand 3
 from instruction: %loop_gather_fusion = bf16[16,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[16]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1116 fusion.272 @0>
 positions:
  fusion.272
 uses:
  custom-call.41.0, operand 0
 from instruction: %fusion.272 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[1024]{0} %p.23), kind=kCustom, calls=%fused_computation.227, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","256"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1117 custom-call.41.0{} @0>
 positions:
  custom-call.41.0 {}
 uses:
  get-tuple-element.41, operand 0 {}
 from instruction: %custom-call.41.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.272, bf16[6144,1024]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1118 custom-call.41.0{0} @0>
 positions:
  custom-call.41.0 {0}
  get-tuple-element.41
 uses:
  loop_convert_fusion.19, operand 0
 from instruction: %custom-call.41.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.272, bf16[6144,1024]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1119 custom-call.41.0{1} @0>
 positions:
  custom-call.41.0 {1}
 uses:
 from instruction: %custom-call.41.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.272, bf16[6144,1024]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1120 loop_convert_fusion.19 @0>
 positions:
  loop_convert_fusion.19
 uses:
  custom-call.42.0, operand 0
 from instruction: %loop_convert_fusion.19 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.41), kind=kLoop, calls=%fused_convert.19, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1121 custom-call.42.0{} @0>
 positions:
  custom-call.42.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.42.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1122 custom-call.42.0{0} @0>
 positions:
  custom-call.42.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.270, operand 4
  fusion.268, operand 5
  fusion.266, operand 4
  fusion.264, operand 5
  loop_add_fusion, operand 4
 from instruction: %custom-call.42.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1123 custom-call.42.0{1} @0>
 positions:
  custom-call.42.0 {1}
 uses:
 from instruction: %custom-call.42.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1124 fusion.270 @0>
 positions:
  fusion.270
 uses:
  custom-call.43.0, operand 0
 from instruction: %fusion.270 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.26, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.225, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1125 custom-call.43.0{} @0>
 positions:
  custom-call.43.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.43.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.270, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1126 custom-call.43.0{0} @0>
 positions:
  custom-call.43.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.18, operand 0
 from instruction: %custom-call.43.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.270, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1127 custom-call.43.0{1} @0>
 positions:
  custom-call.43.0 {1}
 uses:
 from instruction: %custom-call.43.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.270, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1128 loop_convert_fusion.18 @0>
 positions:
  loop_convert_fusion.18
 uses:
  custom-call.44.0, operand 0
 from instruction: %loop_convert_fusion.18 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.18, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1129 custom-call.44.0{} @0>
 positions:
  custom-call.44.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.44.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1130 custom-call.44.0{0} @0>
 positions:
  custom-call.44.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.268, operand 2
  fusion.266, operand 5
  fusion.264, operand 6
  loop_add_fusion, operand 5
 from instruction: %custom-call.44.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1131 custom-call.44.0{1} @0>
 positions:
  custom-call.44.0 {1}
 uses:
 from instruction: %custom-call.44.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1132 fusion.268 @0>
 positions:
  fusion.268
 uses:
  custom-call.45.0, operand 0
 from instruction: %fusion.268 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.29, bf16[16,1024]{1,0} %get-tuple-element.3.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.223, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1133 custom-call.45.0{} @0>
 positions:
  custom-call.45.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.45.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.268, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1134 custom-call.45.0{0} @0>
 positions:
  custom-call.45.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.17, operand 0
 from instruction: %custom-call.45.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.268, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1135 custom-call.45.0{1} @0>
 positions:
  custom-call.45.0 {1}
 uses:
 from instruction: %custom-call.45.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.268, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1136 loop_convert_fusion.17 @0>
 positions:
  loop_convert_fusion.17
 uses:
  custom-call.46.0, operand 0
 from instruction: %loop_convert_fusion.17 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.17, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1137 custom-call.46.0{} @0>
 positions:
  custom-call.46.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.46.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1138 custom-call.46.0{0} @0>
 positions:
  custom-call.46.0 {0}
  get-tuple-element.5.0
 uses:
  fusion.266, operand 6
  fusion.264, operand 7
  loop_add_fusion, operand 6
 from instruction: %custom-call.46.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1139 custom-call.46.0{1} @0>
 positions:
  custom-call.46.0 {1}
 uses:
 from instruction: %custom-call.46.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1140 fusion.266 @0>
 positions:
  fusion.266
 uses:
  custom-call.47.0, operand 0
 from instruction: %fusion.266 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.32, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.3.0, bf16[16,1024]{1,0} %get-tuple-element.5.0), kind=kCustom, calls=%fused_computation.221, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1141 custom-call.47.0{} @0>
 positions:
  custom-call.47.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.47.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.266, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1142 custom-call.47.0{0} @0>
 positions:
  custom-call.47.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.16, operand 0
 from instruction: %custom-call.47.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.266, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1143 custom-call.47.0{1} @0>
 positions:
  custom-call.47.0 {1}
 uses:
 from instruction: %custom-call.47.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.266, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1144 loop_convert_fusion.16 @0>
 positions:
  loop_convert_fusion.16
 uses:
  custom-call.48.0, operand 0
 from instruction: %loop_convert_fusion.16 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.16, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1145 custom-call.48.0{} @0>
 positions:
  custom-call.48.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.48.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1146 custom-call.48.0{0} @0>
 positions:
  custom-call.48.0 {0}
  get-tuple-element.7.0
 uses:
  fusion.264, operand 2
  loop_add_fusion, operand 2
 from instruction: %custom-call.48.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1147 custom-call.48.0{1} @0>
 positions:
  custom-call.48.0 {1}
 uses:
 from instruction: %custom-call.48.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1148 fusion.264 @0>
 positions:
  fusion.264
 uses:
  custom-call.49.0, operand 0
 from instruction: %fusion.264 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.35, bf16[16,1024]{1,0} %get-tuple-element.7.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.1.0, bf16[16,1024]{1,0} %get-tuple-element.3.0, bf16[16,1024]{1,0} %get-tuple-element.5.0), kind=kCustom, calls=%fused_computation.219, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1149 custom-call.49.0{} @0>
 positions:
  custom-call.49.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.49.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.264, bf16[6144,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1150 custom-call.49.0{0} @0>
 positions:
  custom-call.49.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.15, operand 0
 from instruction: %custom-call.49.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.264, bf16[6144,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1151 custom-call.49.0{1} @0>
 positions:
  custom-call.49.0 {1}
 uses:
 from instruction: %custom-call.49.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.264, bf16[6144,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1152 loop_convert_fusion.15 @0>
 positions:
  loop_convert_fusion.15
 uses:
  custom-call.50.0, operand 0
 from instruction: %loop_convert_fusion.15 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.15, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1153 custom-call.50.0{} @0>
 positions:
  custom-call.50.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.50.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1154 custom-call.50.0{0} @0>
 positions:
  custom-call.50.0 {0}
  get-tuple-element.9.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.50.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1155 custom-call.50.0{1} @0>
 positions:
  custom-call.50.0 {1}
 uses:
 from instruction: %custom-call.50.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1156 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.262, operand 0
  fusion.260, operand 2
  fusion.258, operand 2
  fusion.256, operand 4
  loop_add_fusion.1, operand 3
 from instruction: %loop_add_fusion = f32[16,1024]{1,0} fusion(bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1024]{1,0} %get-tuple-element.9.0, bf16[16,1024]{1,0} %get-tuple-element.7.0, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.3.0, bf16[16,1024]{1,0} %get-tuple-element.5.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.103/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1157 fusion.262 @0>
 positions:
  fusion.262
 uses:
  custom-call.51.0, operand 0
 from instruction: %fusion.262 = bf16[16,1024]{1,0} fusion(f32[16,1024]{1,0} %loop_add_fusion, f32[] %p.22, bf16[1024]{0} %p.38), kind=kCustom, calls=%fused_computation.217, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.238"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1158 custom-call.51.0{} @0>
 positions:
  custom-call.51.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.51.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.262, bf16[6144,1024]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1159 custom-call.51.0{0} @0>
 positions:
  custom-call.51.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.14, operand 0
 from instruction: %custom-call.51.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.262, bf16[6144,1024]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1160 custom-call.51.0{1} @0>
 positions:
  custom-call.51.0 {1}
 uses:
 from instruction: %custom-call.51.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.262, bf16[6144,1024]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1161 loop_convert_fusion.14 @0>
 positions:
  loop_convert_fusion.14
 uses:
  custom-call.52.0, operand 0
 from instruction: %loop_convert_fusion.14 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.14, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1162 custom-call.52.0{} @0>
 positions:
  custom-call.52.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.52.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1163 custom-call.52.0{0} @0>
 positions:
  custom-call.52.0 {0}
  get-tuple-element.11.0
 uses:
  fusion.260, operand 3
  fusion.258, operand 3
  fusion.256, operand 5
  loop_add_fusion.1, operand 4
 from instruction: %custom-call.52.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1164 custom-call.52.0{1} @0>
 positions:
  custom-call.52.0 {1}
 uses:
 from instruction: %custom-call.52.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1165 fusion.260 @0>
 positions:
  fusion.260
 uses:
  custom-call.53.0, operand 0
 from instruction: %fusion.260 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.41, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.11.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0), kind=kCustom, calls=%fused_computation.215, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1166 custom-call.53.0{} @0>
 positions:
  custom-call.53.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.53.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.260, bf16[6144,1024]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1167 custom-call.53.0{0} @0>
 positions:
  custom-call.53.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.13, operand 0
 from instruction: %custom-call.53.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.260, bf16[6144,1024]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1168 custom-call.53.0{1} @0>
 positions:
  custom-call.53.0 {1}
 uses:
 from instruction: %custom-call.53.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.260, bf16[6144,1024]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1169 loop_convert_fusion.13 @0>
 positions:
  loop_convert_fusion.13
 uses:
  custom-call.54.0, operand 0
 from instruction: %loop_convert_fusion.13 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.13, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1170 custom-call.54.0{} @0>
 positions:
  custom-call.54.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.54.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1171 custom-call.54.0{0} @0>
 positions:
  custom-call.54.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.258, operand 5
  fusion.256, operand 6
  loop_add_fusion.1, operand 5
 from instruction: %custom-call.54.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1172 custom-call.54.0{1} @0>
 positions:
  custom-call.54.0 {1}
 uses:
 from instruction: %custom-call.54.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1173 fusion.258 @0>
 positions:
  fusion.258
 uses:
  custom-call.55.0, operand 0
 from instruction: %fusion.258 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.44, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.11.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.13.0), kind=kCustom, calls=%fused_computation.213, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1174 custom-call.55.0{} @0>
 positions:
  custom-call.55.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.55.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.258, bf16[6144,1024]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1175 custom-call.55.0{0} @0>
 positions:
  custom-call.55.0 {0}
  get-tuple-element.14.0
 uses:
  loop_convert_fusion.12, operand 0
 from instruction: %custom-call.55.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.258, bf16[6144,1024]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1176 custom-call.55.0{1} @0>
 positions:
  custom-call.55.0 {1}
 uses:
 from instruction: %custom-call.55.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.258, bf16[6144,1024]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1177 loop_convert_fusion.12 @0>
 positions:
  loop_convert_fusion.12
 uses:
  custom-call.56.0, operand 0
 from instruction: %loop_convert_fusion.12 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert.12, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1178 custom-call.56.0{} @0>
 positions:
  custom-call.56.0 {}
 uses:
  get-tuple-element.15.0, operand 0 {}
 from instruction: %custom-call.56.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1179 custom-call.56.0{0} @0>
 positions:
  custom-call.56.0 {0}
  get-tuple-element.15.0
 uses:
  fusion.256, operand 2
  loop_add_fusion.1, operand 2
 from instruction: %custom-call.56.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1180 custom-call.56.0{1} @0>
 positions:
  custom-call.56.0 {1}
 uses:
 from instruction: %custom-call.56.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1181 fusion.256 @0>
 positions:
  fusion.256
 uses:
  custom-call.57.0, operand 0
 from instruction: %fusion.256 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.47, bf16[16,1024]{1,0} %get-tuple-element.15.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, f32[16,1024]{1,0} %loop_add_fusion, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.11.0, bf16[16,1024]{1,0} %get-tuple-element.13.0), kind=kCustom, calls=%fused_computation.211, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1182 custom-call.57.0{} @0>
 positions:
  custom-call.57.0 {}
 uses:
  get-tuple-element.16.0, operand 0 {}
 from instruction: %custom-call.57.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.256, bf16[6144,1024]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1183 custom-call.57.0{0} @0>
 positions:
  custom-call.57.0 {0}
  get-tuple-element.16.0
 uses:
  loop_convert_fusion.11, operand 0
 from instruction: %custom-call.57.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.256, bf16[6144,1024]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1184 custom-call.57.0{1} @0>
 positions:
  custom-call.57.0 {1}
 uses:
 from instruction: %custom-call.57.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.256, bf16[6144,1024]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1185 loop_convert_fusion.11 @0>
 positions:
  loop_convert_fusion.11
 uses:
  custom-call.58.0, operand 0
 from instruction: %loop_convert_fusion.11 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%fused_convert.11, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1186 custom-call.58.0{} @0>
 positions:
  custom-call.58.0 {}
 uses:
  get-tuple-element.17.0, operand 0 {}
 from instruction: %custom-call.58.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1187 custom-call.58.0{0} @0>
 positions:
  custom-call.58.0 {0}
  get-tuple-element.17.0
 uses:
  loop_add_fusion.1, operand 1
 from instruction: %custom-call.58.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1188 custom-call.58.0{1} @0>
 positions:
  custom-call.58.0 {1}
 uses:
 from instruction: %custom-call.58.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1189 loop_add_fusion.1 @0>
 positions:
  loop_add_fusion.1
 uses:
  fusion.254, operand 0
  fusion.252, operand 2
  fusion.250, operand 2
  fusion.248, operand 4
  loop_add_fusion.2, operand 3
 from instruction: %loop_add_fusion.1 = f32[16,1024]{1,0} fusion(bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1024]{1,0} %get-tuple-element.17.0, bf16[16,1024]{1,0} %get-tuple-element.15.0, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.11.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.13.0), kind=kLoop, calls=%fused_add.1, metadata={op_type="aten__add" op_name="aten__add.175/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1190 fusion.254 @0>
 positions:
  fusion.254
 uses:
  custom-call.59.0, operand 0
 from instruction: %fusion.254 = bf16[16,1024]{1,0} fusion(f32[16,1024]{1,0} %loop_add_fusion.1, f32[] %p.22, bf16[1024]{0} %p.50), kind=kCustom, calls=%fused_computation.209, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.238"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1191 custom-call.59.0{} @0>
 positions:
  custom-call.59.0 {}
 uses:
  get-tuple-element.18.0, operand 0 {}
 from instruction: %custom-call.59.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.254, bf16[6144,1024]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1192 custom-call.59.0{0} @0>
 positions:
  custom-call.59.0 {0}
  get-tuple-element.18.0
 uses:
  loop_convert_fusion.10, operand 0
 from instruction: %custom-call.59.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.254, bf16[6144,1024]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1193 custom-call.59.0{1} @0>
 positions:
  custom-call.59.0 {1}
 uses:
 from instruction: %custom-call.59.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.254, bf16[6144,1024]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1194 loop_convert_fusion.10 @0>
 positions:
  loop_convert_fusion.10
 uses:
  custom-call.60.0, operand 0
 from instruction: %loop_convert_fusion.10 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.18.0), kind=kLoop, calls=%fused_convert.10, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1195 custom-call.60.0{} @0>
 positions:
  custom-call.60.0 {}
 uses:
  get-tuple-element.19.0, operand 0 {}
 from instruction: %custom-call.60.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1196 custom-call.60.0{0} @0>
 positions:
  custom-call.60.0 {0}
  get-tuple-element.19.0
 uses:
  fusion.252, operand 3
  fusion.250, operand 3
  fusion.248, operand 5
  loop_add_fusion.2, operand 4
 from instruction: %custom-call.60.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1197 custom-call.60.0{1} @0>
 positions:
  custom-call.60.0 {1}
 uses:
 from instruction: %custom-call.60.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1198 fusion.252 @0>
 positions:
  fusion.252
 uses:
  custom-call.61.0, operand 0
 from instruction: %fusion.252 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.53, f32[16,1024]{1,0} %loop_add_fusion.1, bf16[16,1024]{1,0} %get-tuple-element.19.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0), kind=kCustom, calls=%fused_computation.207, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1199 custom-call.61.0{} @0>
 positions:
  custom-call.61.0 {}
 uses:
  get-tuple-element.20.0, operand 0 {}
 from instruction: %custom-call.61.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.252, bf16[6144,1024]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1200 custom-call.61.0{0} @0>
 positions:
  custom-call.61.0 {0}
  get-tuple-element.20.0
 uses:
  loop_convert_fusion.9, operand 0
 from instruction: %custom-call.61.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.252, bf16[6144,1024]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1201 custom-call.61.0{1} @0>
 positions:
  custom-call.61.0 {1}
 uses:
 from instruction: %custom-call.61.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.252, bf16[6144,1024]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1202 loop_convert_fusion.9 @0>
 positions:
  loop_convert_fusion.9
 uses:
  custom-call.62.0, operand 0
 from instruction: %loop_convert_fusion.9 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.20.0), kind=kLoop, calls=%fused_convert.9, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1203 custom-call.62.0{} @0>
 positions:
  custom-call.62.0 {}
 uses:
  get-tuple-element.21.0, operand 0 {}
 from instruction: %custom-call.62.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1204 custom-call.62.0{0} @0>
 positions:
  custom-call.62.0 {0}
  get-tuple-element.21.0
 uses:
  fusion.250, operand 5
  fusion.248, operand 6
  loop_add_fusion.2, operand 5
 from instruction: %custom-call.62.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1205 custom-call.62.0{1} @0>
 positions:
  custom-call.62.0 {1}
 uses:
 from instruction: %custom-call.62.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1206 fusion.250 @0>
 positions:
  fusion.250
 uses:
  custom-call.63.0, operand 0
 from instruction: %fusion.250 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.56, f32[16,1024]{1,0} %loop_add_fusion.1, bf16[16,1024]{1,0} %get-tuple-element.19.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.21.0), kind=kCustom, calls=%fused_computation.205, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1207 custom-call.63.0{} @0>
 positions:
  custom-call.63.0 {}
 uses:
  get-tuple-element.22.0, operand 0 {}
 from instruction: %custom-call.63.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.250, bf16[6144,1024]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1208 custom-call.63.0{0} @0>
 positions:
  custom-call.63.0 {0}
  get-tuple-element.22.0
 uses:
  loop_convert_fusion.8, operand 0
 from instruction: %custom-call.63.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.250, bf16[6144,1024]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1209 custom-call.63.0{1} @0>
 positions:
  custom-call.63.0 {1}
 uses:
 from instruction: %custom-call.63.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.250, bf16[6144,1024]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1210 loop_convert_fusion.8 @0>
 positions:
  loop_convert_fusion.8
 uses:
  custom-call.64.0, operand 0
 from instruction: %loop_convert_fusion.8 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.22.0), kind=kLoop, calls=%fused_convert.8, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1211 custom-call.64.0{} @0>
 positions:
  custom-call.64.0 {}
 uses:
  get-tuple-element.23.0, operand 0 {}
 from instruction: %custom-call.64.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1212 custom-call.64.0{0} @0>
 positions:
  custom-call.64.0 {0}
  get-tuple-element.23.0
 uses:
  fusion.248, operand 2
  loop_add_fusion.2, operand 2
 from instruction: %custom-call.64.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1213 custom-call.64.0{1} @0>
 positions:
  custom-call.64.0 {1}
 uses:
 from instruction: %custom-call.64.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1214 fusion.248 @0>
 positions:
  fusion.248
 uses:
  custom-call.65.0, operand 0
 from instruction: %fusion.248 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.59, bf16[16,1024]{1,0} %get-tuple-element.23.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, f32[16,1024]{1,0} %loop_add_fusion.1, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.19.0, bf16[16,1024]{1,0} %get-tuple-element.21.0), kind=kCustom, calls=%fused_computation.203, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1215 custom-call.65.0{} @0>
 positions:
  custom-call.65.0 {}
 uses:
  get-tuple-element.24.0, operand 0 {}
 from instruction: %custom-call.65.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.248, bf16[6144,1024]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1216 custom-call.65.0{0} @0>
 positions:
  custom-call.65.0 {0}
  get-tuple-element.24.0
 uses:
  loop_convert_fusion.7, operand 0
 from instruction: %custom-call.65.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.248, bf16[6144,1024]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1217 custom-call.65.0{1} @0>
 positions:
  custom-call.65.0 {1}
 uses:
 from instruction: %custom-call.65.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.248, bf16[6144,1024]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1218 loop_convert_fusion.7 @0>
 positions:
  loop_convert_fusion.7
 uses:
  custom-call.66.0, operand 0
 from instruction: %loop_convert_fusion.7 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.24.0), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1219 custom-call.66.0{} @0>
 positions:
  custom-call.66.0 {}
 uses:
  get-tuple-element.25.0, operand 0 {}
 from instruction: %custom-call.66.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1220 custom-call.66.0{0} @0>
 positions:
  custom-call.66.0 {0}
  get-tuple-element.25.0
 uses:
  loop_add_fusion.2, operand 1
 from instruction: %custom-call.66.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1221 custom-call.66.0{1} @0>
 positions:
  custom-call.66.0 {1}
 uses:
 from instruction: %custom-call.66.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1222 loop_add_fusion.2 @0>
 positions:
  loop_add_fusion.2
 uses:
  fusion.246, operand 0
  fusion.244, operand 2
  fusion.242, operand 2
  fusion.240, operand 4
  loop_add_fusion.3, operand 3
 from instruction: %loop_add_fusion.2 = f32[16,1024]{1,0} fusion(bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1024]{1,0} %get-tuple-element.25.0, bf16[16,1024]{1,0} %get-tuple-element.23.0, f32[16,1024]{1,0} %loop_add_fusion.1, bf16[16,1024]{1,0} %get-tuple-element.19.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.21.0), kind=kLoop, calls=%fused_add.2, metadata={op_type="aten__add" op_name="aten__add.247/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1223 fusion.246 @0>
 positions:
  fusion.246
 uses:
  custom-call.67.0, operand 0
 from instruction: %fusion.246 = bf16[16,1024]{1,0} fusion(f32[16,1024]{1,0} %loop_add_fusion.2, f32[] %p.22, bf16[1024]{0} %p.62), kind=kCustom, calls=%fused_computation.201, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.238"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1224 custom-call.67.0{} @0>
 positions:
  custom-call.67.0 {}
 uses:
  get-tuple-element.26.0, operand 0 {}
 from instruction: %custom-call.67.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.246, bf16[6144,1024]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1225 custom-call.67.0{0} @0>
 positions:
  custom-call.67.0 {0}
  get-tuple-element.26.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.67.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.246, bf16[6144,1024]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1226 custom-call.67.0{1} @0>
 positions:
  custom-call.67.0 {1}
 uses:
 from instruction: %custom-call.67.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.246, bf16[6144,1024]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1227 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.68.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.26.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1228 custom-call.68.0{} @0>
 positions:
  custom-call.68.0 {}
 uses:
  get-tuple-element.27.0, operand 0 {}
 from instruction: %custom-call.68.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1229 custom-call.68.0{0} @0>
 positions:
  custom-call.68.0 {0}
  get-tuple-element.27.0
 uses:
  fusion.244, operand 3
  fusion.242, operand 3
  fusion.240, operand 5
  loop_add_fusion.3, operand 4
 from instruction: %custom-call.68.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1230 custom-call.68.0{1} @0>
 positions:
  custom-call.68.0 {1}
 uses:
 from instruction: %custom-call.68.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1231 fusion.244 @0>
 positions:
  fusion.244
 uses:
  custom-call.69.0, operand 0
 from instruction: %fusion.244 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.65, f32[16,1024]{1,0} %loop_add_fusion.2, bf16[16,1024]{1,0} %get-tuple-element.27.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0), kind=kCustom, calls=%fused_computation.199, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1232 custom-call.69.0{} @0>
 positions:
  custom-call.69.0 {}
 uses:
  get-tuple-element.28.0, operand 0 {}
 from instruction: %custom-call.69.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.244, bf16[6144,1024]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1233 custom-call.69.0{0} @0>
 positions:
  custom-call.69.0 {0}
  get-tuple-element.28.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.69.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.244, bf16[6144,1024]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1234 custom-call.69.0{1} @0>
 positions:
  custom-call.69.0 {1}
 uses:
 from instruction: %custom-call.69.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.244, bf16[6144,1024]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1235 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.70.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.28.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1236 custom-call.70.0{} @0>
 positions:
  custom-call.70.0 {}
 uses:
  get-tuple-element.29.0, operand 0 {}
 from instruction: %custom-call.70.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1237 custom-call.70.0{0} @0>
 positions:
  custom-call.70.0 {0}
  get-tuple-element.29.0
 uses:
  fusion.242, operand 5
  fusion.240, operand 6
  loop_add_fusion.3, operand 5
 from instruction: %custom-call.70.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1238 custom-call.70.0{1} @0>
 positions:
  custom-call.70.0 {1}
 uses:
 from instruction: %custom-call.70.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1239 fusion.242 @0>
 positions:
  fusion.242
 uses:
  custom-call.71.0, operand 0
 from instruction: %fusion.242 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.68, f32[16,1024]{1,0} %loop_add_fusion.2, bf16[16,1024]{1,0} %get-tuple-element.27.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.29.0), kind=kCustom, calls=%fused_computation.197, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1240 custom-call.71.0{} @0>
 positions:
  custom-call.71.0 {}
 uses:
  get-tuple-element.30.0, operand 0 {}
 from instruction: %custom-call.71.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.242, bf16[6144,1024]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1241 custom-call.71.0{0} @0>
 positions:
  custom-call.71.0 {0}
  get-tuple-element.30.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.71.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.242, bf16[6144,1024]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1242 custom-call.71.0{1} @0>
 positions:
  custom-call.71.0 {1}
 uses:
 from instruction: %custom-call.71.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.242, bf16[6144,1024]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1243 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.72.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.30.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1244 custom-call.72.0{} @0>
 positions:
  custom-call.72.0 {}
 uses:
  get-tuple-element.31.0, operand 0 {}
 from instruction: %custom-call.72.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1245 custom-call.72.0{0} @0>
 positions:
  custom-call.72.0 {0}
  get-tuple-element.31.0
 uses:
  fusion.240, operand 2
  loop_add_fusion.3, operand 2
 from instruction: %custom-call.72.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1246 custom-call.72.0{1} @0>
 positions:
  custom-call.72.0 {1}
 uses:
 from instruction: %custom-call.72.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1247 fusion.240 @0>
 positions:
  fusion.240
 uses:
  custom-call.73.0, operand 0
 from instruction: %fusion.240 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.71, bf16[16,1024]{1,0} %get-tuple-element.31.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, f32[16,1024]{1,0} %loop_add_fusion.2, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.27.0, bf16[16,1024]{1,0} %get-tuple-element.29.0), kind=kCustom, calls=%fused_computation.195, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1248 custom-call.73.0{} @0>
 positions:
  custom-call.73.0 {}
 uses:
  get-tuple-element.32.0, operand 0 {}
 from instruction: %custom-call.73.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.240, bf16[6144,1024]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1249 custom-call.73.0{0} @0>
 positions:
  custom-call.73.0 {0}
  get-tuple-element.32.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.73.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.240, bf16[6144,1024]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1250 custom-call.73.0{1} @0>
 positions:
  custom-call.73.0 {1}
 uses:
 from instruction: %custom-call.73.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.240, bf16[6144,1024]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1251 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.74.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.32.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1252 custom-call.74.0{} @0>
 positions:
  custom-call.74.0 {}
 uses:
  get-tuple-element.33.0, operand 0 {}
 from instruction: %custom-call.74.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1253 custom-call.74.0{0} @0>
 positions:
  custom-call.74.0 {0}
  get-tuple-element.33.0
 uses:
  loop_add_fusion.3, operand 1
 from instruction: %custom-call.74.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1254 custom-call.74.0{1} @0>
 positions:
  custom-call.74.0 {1}
 uses:
 from instruction: %custom-call.74.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1255 loop_add_fusion.3 @0>
 positions:
  loop_add_fusion.3
 uses:
  fusion.238, operand 0
  fusion.236, operand 2
  fusion.234, operand 2
  fusion.232, operand 3
 from instruction: %loop_add_fusion.3 = f32[16,1024]{1,0} fusion(bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1024]{1,0} %get-tuple-element.33.0, bf16[16,1024]{1,0} %get-tuple-element.31.0, f32[16,1024]{1,0} %loop_add_fusion.2, bf16[16,1024]{1,0} %get-tuple-element.27.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.29.0), kind=kLoop, calls=%fused_add.3, metadata={op_type="aten__add" op_name="aten__add.319/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1256 fusion.238 @0>
 positions:
  fusion.238
 uses:
  custom-call.75.0, operand 0
 from instruction: %fusion.238 = bf16[16,1024]{1,0} fusion(f32[16,1024]{1,0} %loop_add_fusion.3, f32[] %p.22, bf16[1024]{0} %p.74), kind=kCustom, calls=%fused_computation.193, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.238"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1257 custom-call.75.0{} @0>
 positions:
  custom-call.75.0 {}
 uses:
  get-tuple-element.34.0, operand 0 {}
 from instruction: %custom-call.75.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.238, bf16[6144,1024]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1258 custom-call.75.0{0} @0>
 positions:
  custom-call.75.0 {0}
  get-tuple-element.34.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.75.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.238, bf16[6144,1024]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1259 custom-call.75.0{1} @0>
 positions:
  custom-call.75.0 {1}
 uses:
 from instruction: %custom-call.75.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.238, bf16[6144,1024]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1260 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.76.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.34.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1261 custom-call.76.0{} @0>
 positions:
  custom-call.76.0 {}
 uses:
  get-tuple-element.35.0, operand 0 {}
 from instruction: %custom-call.76.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1262 custom-call.76.0{0} @0>
 positions:
  custom-call.76.0 {0}
  get-tuple-element.35.0
 uses:
  fusion.236, operand 3
  fusion.234, operand 3
  fusion.232, operand 4
 from instruction: %custom-call.76.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1263 custom-call.76.0{1} @0>
 positions:
  custom-call.76.0 {1}
 uses:
 from instruction: %custom-call.76.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1264 fusion.236 @0>
 positions:
  fusion.236
 uses:
  custom-call.77.0, operand 0
 from instruction: %fusion.236 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.77, f32[16,1024]{1,0} %loop_add_fusion.3, bf16[16,1024]{1,0} %get-tuple-element.35.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0), kind=kCustom, calls=%fused_computation.191, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1265 custom-call.77.0{} @0>
 positions:
  custom-call.77.0 {}
 uses:
  get-tuple-element.36.0, operand 0 {}
 from instruction: %custom-call.77.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.236, bf16[6144,1024]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1266 custom-call.77.0{0} @0>
 positions:
  custom-call.77.0 {0}
  get-tuple-element.36.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.77.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.236, bf16[6144,1024]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1267 custom-call.77.0{1} @0>
 positions:
  custom-call.77.0 {1}
 uses:
 from instruction: %custom-call.77.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.236, bf16[6144,1024]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1268 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.78.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.36.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1269 custom-call.78.0{} @0>
 positions:
  custom-call.78.0 {}
 uses:
  get-tuple-element.37.0, operand 0 {}
 from instruction: %custom-call.78.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1270 custom-call.78.0{0} @0>
 positions:
  custom-call.78.0 {0}
  get-tuple-element.37.0
 uses:
  fusion.234, operand 5
  fusion.232, operand 6
 from instruction: %custom-call.78.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1271 custom-call.78.0{1} @0>
 positions:
  custom-call.78.0 {1}
 uses:
 from instruction: %custom-call.78.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1272 fusion.234 @0>
 positions:
  fusion.234
 uses:
  custom-call.79.0, operand 0
 from instruction: %fusion.234 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[1024]{0} %p.80, f32[16,1024]{1,0} %loop_add_fusion.3, bf16[16,1024]{1,0} %get-tuple-element.35.0, bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.37.0), kind=kCustom, calls=%fused_computation.189, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1273 custom-call.79.0{} @0>
 positions:
  custom-call.79.0 {}
 uses:
  get-tuple-element.38.0, operand 0 {}
 from instruction: %custom-call.79.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.234, bf16[6144,1024]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1274 custom-call.79.0{0} @0>
 positions:
  custom-call.79.0 {0}
  get-tuple-element.38.0
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.79.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.234, bf16[6144,1024]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1275 custom-call.79.0{1} @0>
 positions:
  custom-call.79.0 {1}
 uses:
 from instruction: %custom-call.79.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.234, bf16[6144,1024]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1276 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.80.0, operand 0
 from instruction: %loop_convert_fusion = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.38.0), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1277 custom-call.80.0{} @0>
 positions:
  custom-call.80.0 {}
 uses:
  get-tuple-element.39.0, operand 0 {}
 from instruction: %custom-call.80.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1278 custom-call.80.0{0} @0>
 positions:
  custom-call.80.0 {0}
  get-tuple-element.39.0
 uses:
  fusion.232, operand 1
 from instruction: %custom-call.80.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1279 custom-call.80.0{1} @0>
 positions:
  custom-call.80.0 {1}
 uses:
 from instruction: %custom-call.80.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1280 fusion.232 @0>
 positions:
  fusion.232
 uses:
  custom-call.81.0, operand 0
 from instruction: %fusion.232 = bf16[16,1024]{1,0} fusion(f32[] %p.22, bf16[16,1024]{1,0} %get-tuple-element.39.0, bf16[1024]{0} %p.83, f32[16,1024]{1,0} %loop_add_fusion.3, bf16[16,1024]{1,0} %get-tuple-element.35.0, /*index=5*/bf16[16,20480]{1,0} %gemm_fusion_dot.80.0, bf16[16,1024]{1,0} %get-tuple-element.37.0), kind=kCustom, calls=%fused_computation.187, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1281 custom-call.81.0{} @0>
 positions:
  custom-call.81.0 {}
 uses:
  get-tuple-element.40.0, operand 0 {}
 from instruction: %custom-call.81.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.232, bf16[4096,1024]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1282 custom-call.81.0{0} @0>
 positions:
  custom-call.81.0 {0}
  get-tuple-element.40.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.22.0, operand 1
 from instruction: %custom-call.81.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.232, bf16[4096,1024]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1283 custom-call.81.0{1} @0>
 positions:
  custom-call.81.0 {1}
 uses:
 from instruction: %custom-call.81.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.232, bf16[4096,1024]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1284 triton_softmax.22.0 @0>
 positions:
  triton_softmax.22.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.22.0 = f32[16,8,128]{2,1,0} fusion(f32[] %p.22, bf16[16,4096]{1,0} %get-tuple-element.40.0), kind=kCustom, calls=%triton_softmax_computation.22, metadata={op_type="aten__mul" op_name="aten__mul.365/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1","128"]}],"num_warps":"1"}},"force_earliest_schedule":false}
<1285 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.1624.0 {0}
  call {0}
  get-tuple-element.43
  tuple {0}
 uses:
  tuple, operand 0
  tuple.1624.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[16,8,128]{2,1,0} fusion(f32[16,8,128]{2,1,0} %triton_softmax.22.0, bf16[128]{0} %p.85, bf16[40960,128]{1,0} %p.86, s32[16]{0} %p.87), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1286 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.1624.0 {1}
  call {1}
  get-tuple-element.44
  bitcast.4114.0
  tuple {1}
 uses:
  bitcast.4114.0, operand 0
  tuple.1624.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[16,1024]{1,0} fusion(bf16[16,4096]{1,0} %get-tuple-element.40.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1287 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.1624.0 {3}
  call {2}
  get-tuple-element.45
  bitcast.4126.0
  tuple {2}
 uses:
  bitcast.4126.0, operand 0
  tuple.1624.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.88), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<1288 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.1624.0 {2}
  bitcast.4119.0
  call {3}
  get-tuple-element.46
  tuple {3}
 uses:
  tuple, operand 3
  tuple.1624.0, operand 2
  bitcast.4119.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.88), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<1289 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.43, operand 0 {}
  get-tuple-element.44, operand 0 {}
  get-tuple-element.45, operand 0 {}
  get-tuple-element.46, operand 0 {}
 from instruction: %tuple = (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[16,8,128]{2,1,0} %input_concatenate_fusion, bf16[16,8,128]{2,1,0} %bitcast.4114.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.4126.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<1290 p5.54.0 @0>
 positions:
  p5.54.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.54.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1291 p4.52.0 @0>
 positions:
  p4.52.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.52.0 = s32[16]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1292 p82.1426.0 @0>
 positions:
  p82.1426.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p82.1426.0 = bf16[1024,2048]{1,0} parameter(82), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1293 p78.1354.0 @0>
 positions:
  p78.1354.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p78.1354.0 = bf16[1024,2048]{1,0} parameter(78), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1294 p74.1282.0 @0>
 positions:
  p74.1282.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p74.1282.0 = bf16[1024,2048]{1,0} parameter(74), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1295 p70.1210.0 @0>
 positions:
  p70.1210.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p70.1210.0 = bf16[1024,2048]{1,0} parameter(70), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1296 p66.1138.0 @0>
 positions:
  p66.1138.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p66.1138.0 = bf16[1024,2048]{1,0} parameter(66), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1297 p62.1066.0 @0>
 positions:
  p62.1066.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p62.1066.0 = bf16[1024,2048]{1,0} parameter(62), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1298 p58.994.0 @0>
 positions:
  p58.994.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p58.994.0 = bf16[1024,2048]{1,0} parameter(58), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1299 p54.922.0 @0>
 positions:
  p54.922.0
  p.9
 uses:
  call, operand 9
  wrapped_concatenate, operand 7
 from instruction: %p54.922.0 = bf16[1024,2048]{1,0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1300 p50.850.0 @0>
 positions:
  p50.850.0
  p.10
 uses:
  call, operand 10
  wrapped_concatenate, operand 8
 from instruction: %p50.850.0 = bf16[1024,2048]{1,0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1301 p46.778.0 @0>
 positions:
  p46.778.0
  p.11
 uses:
  call, operand 11
  wrapped_concatenate, operand 9
 from instruction: %p46.778.0 = bf16[1024,2048]{1,0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1302 p42.706.0 @0>
 positions:
  p42.706.0
  p.12
 uses:
  call, operand 12
  wrapped_concatenate, operand 10
 from instruction: %p42.706.0 = bf16[1024,2048]{1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1303 p38.634.0 @0>
 positions:
  p38.634.0
  p.13
 uses:
  call, operand 13
  wrapped_concatenate, operand 11
 from instruction: %p38.634.0 = bf16[1024,2048]{1,0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1304 p34.562.0 @0>
 positions:
  p34.562.0
  p.14
 uses:
  call, operand 14
  wrapped_concatenate, operand 12
 from instruction: %p34.562.0 = bf16[1024,2048]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1305 p30.490.0 @0>
 positions:
  p30.490.0
  p.15
 uses:
  call, operand 15
  wrapped_concatenate, operand 13
 from instruction: %p30.490.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1306 p26.418.0 @0>
 positions:
  p26.418.0
  p.16
 uses:
  call, operand 16
  wrapped_concatenate, operand 14
 from instruction: %p26.418.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1307 p22.346.0 @0>
 positions:
  p22.346.0
  p.17
 uses:
  call, operand 17
  wrapped_concatenate, operand 15
 from instruction: %p22.346.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1308 p18.274.0 @0>
 positions:
  p18.274.0
  p.18
 uses:
  call, operand 18
  wrapped_concatenate, operand 16
 from instruction: %p18.274.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1309 p14.202.0 @0>
 positions:
  p14.202.0
  p.19
 uses:
  call, operand 19
  wrapped_concatenate, operand 17
 from instruction: %p14.202.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1310 p10.130.0 @0>
 positions:
  p10.130.0
  p.20
 uses:
  call, operand 20
  wrapped_concatenate, operand 18
 from instruction: %p10.130.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1311 p6.58.0 @0>
 positions:
  p6.58.0
  p.21
 uses:
  call, operand 21
  wrapped_concatenate, operand 19
 from instruction: %p6.58.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1312 p1.4.0 @0>
 positions:
  p1.4.0
  p.22
 uses:
  call, operand 22
  fusion.272, operand 0
  fusion.270, operand 0
  fusion.268, operand 0
  fusion.266, operand 0
  fusion.264, operand 0
  fusion.262, operand 1
  fusion.260, operand 0
  fusion.258, operand 0
  fusion.256, operand 0
  fusion.254, operand 1
  fusion.252, operand 0
  fusion.250, operand 0
  fusion.248, operand 0
  fusion.246, operand 1
  fusion.244, operand 0
  fusion.242, operand 0
  fusion.240, operand 0
  fusion.238, operand 1
  fusion.236, operand 0
  fusion.234, operand 0
  fusion.232, operand 0
  triton_softmax.22.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1313 p9.78.0 @0>
 positions:
  p9.78.0
  p.23
 uses:
  call, operand 23
  fusion.272, operand 3
 from instruction: %p9.78.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1314 p8.76.0 @0>
 positions:
  p8.76.0
  p.24
 uses:
  call, operand 24
  custom-call.41.0, operand 1
 from instruction: %p8.76.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1315 p7.74.0 @0>
 positions:
  p7.74.0
  p.25
 uses:
  call, operand 25
  custom-call.42.0, operand 1
 from instruction: %p7.74.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1316 p13.150.0 @0>
 positions:
  p13.150.0
  p.26
 uses:
  call, operand 26
  fusion.270, operand 1
 from instruction: %p13.150.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1317 p12.148.0 @0>
 positions:
  p12.148.0
  p.27
 uses:
  call, operand 27
  custom-call.43.0, operand 1
 from instruction: %p12.148.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1318 p11.146.0 @0>
 positions:
  p11.146.0
  p.28
 uses:
  call, operand 28
  custom-call.44.0, operand 1
 from instruction: %p11.146.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1319 p17.222.0 @0>
 positions:
  p17.222.0
  p.29
 uses:
  call, operand 29
  fusion.268, operand 1
 from instruction: %p17.222.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1320 p16.220.0 @0>
 positions:
  p16.220.0
  p.30
 uses:
  call, operand 30
  custom-call.45.0, operand 1
 from instruction: %p16.220.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1321 p15.218.0 @0>
 positions:
  p15.218.0
  p.31
 uses:
  call, operand 31
  custom-call.46.0, operand 1
 from instruction: %p15.218.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1322 p21.294.0 @0>
 positions:
  p21.294.0
  p.32
 uses:
  call, operand 32
  fusion.266, operand 1
 from instruction: %p21.294.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1323 p20.292.0 @0>
 positions:
  p20.292.0
  p.33
 uses:
  call, operand 33
  custom-call.47.0, operand 1
 from instruction: %p20.292.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1324 p19.290.0 @0>
 positions:
  p19.290.0
  p.34
 uses:
  call, operand 34
  custom-call.48.0, operand 1
 from instruction: %p19.290.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1325 p25.366.0 @0>
 positions:
  p25.366.0
  p.35
 uses:
  call, operand 35
  fusion.264, operand 1
 from instruction: %p25.366.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1326 p24.364.0 @0>
 positions:
  p24.364.0
  p.36
 uses:
  call, operand 36
  custom-call.49.0, operand 1
 from instruction: %p24.364.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1327 p23.362.0 @0>
 positions:
  p23.362.0
  p.37
 uses:
  call, operand 37
  custom-call.50.0, operand 1
 from instruction: %p23.362.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1328 p29.438.0 @0>
 positions:
  p29.438.0
  p.38
 uses:
  call, operand 38
  fusion.262, operand 2
 from instruction: %p29.438.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1329 p28.436.0 @0>
 positions:
  p28.436.0
  p.39
 uses:
  call, operand 39
  custom-call.51.0, operand 1
 from instruction: %p28.436.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1330 p27.434.0 @0>
 positions:
  p27.434.0
  p.40
 uses:
  call, operand 40
  custom-call.52.0, operand 1
 from instruction: %p27.434.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1331 p33.510.0 @0>
 positions:
  p33.510.0
  p.41
 uses:
  call, operand 41
  fusion.260, operand 1
 from instruction: %p33.510.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1332 p32.508.0 @0>
 positions:
  p32.508.0
  p.42
 uses:
  call, operand 42
  custom-call.53.0, operand 1
 from instruction: %p32.508.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1333 p31.506.0 @0>
 positions:
  p31.506.0
  p.43
 uses:
  call, operand 43
  custom-call.54.0, operand 1
 from instruction: %p31.506.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1334 p37.582.0 @0>
 positions:
  p37.582.0
  p.44
 uses:
  call, operand 44
  fusion.258, operand 1
 from instruction: %p37.582.0 = bf16[1024]{0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1335 p36.580.0 @0>
 positions:
  p36.580.0
  p.45
 uses:
  call, operand 45
  custom-call.55.0, operand 1
 from instruction: %p36.580.0 = bf16[6144,1024]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1336 p35.578.0 @0>
 positions:
  p35.578.0
  p.46
 uses:
  call, operand 46
  custom-call.56.0, operand 1
 from instruction: %p35.578.0 = bf16[1024,3072]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1337 p41.654.0 @0>
 positions:
  p41.654.0
  p.47
 uses:
  call, operand 47
  fusion.256, operand 1
 from instruction: %p41.654.0 = bf16[1024]{0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1338 p40.652.0 @0>
 positions:
  p40.652.0
  p.48
 uses:
  call, operand 48
  custom-call.57.0, operand 1
 from instruction: %p40.652.0 = bf16[6144,1024]{1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1339 p39.650.0 @0>
 positions:
  p39.650.0
  p.49
 uses:
  call, operand 49
  custom-call.58.0, operand 1
 from instruction: %p39.650.0 = bf16[1024,3072]{1,0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1340 p45.726.0 @0>
 positions:
  p45.726.0
  p.50
 uses:
  call, operand 50
  fusion.254, operand 2
 from instruction: %p45.726.0 = bf16[1024]{0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1341 p44.724.0 @0>
 positions:
  p44.724.0
  p.51
 uses:
  call, operand 51
  custom-call.59.0, operand 1
 from instruction: %p44.724.0 = bf16[6144,1024]{1,0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1342 p43.722.0 @0>
 positions:
  p43.722.0
  p.52
 uses:
  call, operand 52
  custom-call.60.0, operand 1
 from instruction: %p43.722.0 = bf16[1024,3072]{1,0} parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1343 p49.798.0 @0>
 positions:
  p49.798.0
  p.53
 uses:
  call, operand 53
  fusion.252, operand 1
 from instruction: %p49.798.0 = bf16[1024]{0} parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1344 p48.796.0 @0>
 positions:
  p48.796.0
  p.54
 uses:
  call, operand 54
  custom-call.61.0, operand 1
 from instruction: %p48.796.0 = bf16[6144,1024]{1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1345 p47.794.0 @0>
 positions:
  p47.794.0
  p.55
 uses:
  call, operand 55
  custom-call.62.0, operand 1
 from instruction: %p47.794.0 = bf16[1024,3072]{1,0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1346 p53.870.0 @0>
 positions:
  p53.870.0
  p.56
 uses:
  call, operand 56
  fusion.250, operand 1
 from instruction: %p53.870.0 = bf16[1024]{0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1347 p52.868.0 @0>
 positions:
  p52.868.0
  p.57
 uses:
  call, operand 57
  custom-call.63.0, operand 1
 from instruction: %p52.868.0 = bf16[6144,1024]{1,0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1348 p51.866.0 @0>
 positions:
  p51.866.0
  p.58
 uses:
  call, operand 58
  custom-call.64.0, operand 1
 from instruction: %p51.866.0 = bf16[1024,3072]{1,0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1349 p57.942.0 @0>
 positions:
  p57.942.0
  p.59
 uses:
  call, operand 59
  fusion.248, operand 1
 from instruction: %p57.942.0 = bf16[1024]{0} parameter(57), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1350 p56.940.0 @0>
 positions:
  p56.940.0
  p.60
 uses:
  call, operand 60
  custom-call.65.0, operand 1
 from instruction: %p56.940.0 = bf16[6144,1024]{1,0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1351 p55.938.0 @0>
 positions:
  p55.938.0
  p.61
 uses:
  call, operand 61
  custom-call.66.0, operand 1
 from instruction: %p55.938.0 = bf16[1024,3072]{1,0} parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1352 p61.1014.0 @0>
 positions:
  p61.1014.0
  p.62
 uses:
  call, operand 62
  fusion.246, operand 2
 from instruction: %p61.1014.0 = bf16[1024]{0} parameter(61), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1353 p60.1012.0 @0>
 positions:
  p60.1012.0
  p.63
 uses:
  call, operand 63
  custom-call.67.0, operand 1
 from instruction: %p60.1012.0 = bf16[6144,1024]{1,0} parameter(60), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1354 p59.1010.0 @0>
 positions:
  p59.1010.0
  p.64
 uses:
  call, operand 64
  custom-call.68.0, operand 1
 from instruction: %p59.1010.0 = bf16[1024,3072]{1,0} parameter(59), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1355 p65.1086.0 @0>
 positions:
  p65.1086.0
  p.65
 uses:
  call, operand 65
  fusion.244, operand 1
 from instruction: %p65.1086.0 = bf16[1024]{0} parameter(65), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1356 p64.1084.0 @0>
 positions:
  p64.1084.0
  p.66
 uses:
  call, operand 66
  custom-call.69.0, operand 1
 from instruction: %p64.1084.0 = bf16[6144,1024]{1,0} parameter(64), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1357 p63.1082.0 @0>
 positions:
  p63.1082.0
  p.67
 uses:
  call, operand 67
  custom-call.70.0, operand 1
 from instruction: %p63.1082.0 = bf16[1024,3072]{1,0} parameter(63), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1358 p69.1158.0 @0>
 positions:
  p69.1158.0
  p.68
 uses:
  call, operand 68
  fusion.242, operand 1
 from instruction: %p69.1158.0 = bf16[1024]{0} parameter(69), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1359 p68.1156.0 @0>
 positions:
  p68.1156.0
  p.69
 uses:
  call, operand 69
  custom-call.71.0, operand 1
 from instruction: %p68.1156.0 = bf16[6144,1024]{1,0} parameter(68), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1360 p67.1154.0 @0>
 positions:
  p67.1154.0
  p.70
 uses:
  call, operand 70
  custom-call.72.0, operand 1
 from instruction: %p67.1154.0 = bf16[1024,3072]{1,0} parameter(67), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1361 p73.1230.0 @0>
 positions:
  p73.1230.0
  p.71
 uses:
  call, operand 71
  fusion.240, operand 1
 from instruction: %p73.1230.0 = bf16[1024]{0} parameter(73), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1362 p72.1228.0 @0>
 positions:
  p72.1228.0
  p.72
 uses:
  call, operand 72
  custom-call.73.0, operand 1
 from instruction: %p72.1228.0 = bf16[6144,1024]{1,0} parameter(72), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1363 p71.1226.0 @0>
 positions:
  p71.1226.0
  p.73
 uses:
  call, operand 73
  custom-call.74.0, operand 1
 from instruction: %p71.1226.0 = bf16[1024,3072]{1,0} parameter(71), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1364 p77.1302.0 @0>
 positions:
  p77.1302.0
  p.74
 uses:
  call, operand 74
  fusion.238, operand 2
 from instruction: %p77.1302.0 = bf16[1024]{0} parameter(77), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1365 p76.1300.0 @0>
 positions:
  p76.1300.0
  p.75
 uses:
  call, operand 75
  custom-call.75.0, operand 1
 from instruction: %p76.1300.0 = bf16[6144,1024]{1,0} parameter(76), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1366 p75.1298.0 @0>
 positions:
  p75.1298.0
  p.76
 uses:
  call, operand 76
  custom-call.76.0, operand 1
 from instruction: %p75.1298.0 = bf16[1024,3072]{1,0} parameter(75), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1367 p81.1374.0 @0>
 positions:
  p81.1374.0
  p.77
 uses:
  call, operand 77
  fusion.236, operand 1
 from instruction: %p81.1374.0 = bf16[1024]{0} parameter(81), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1368 p80.1372.0 @0>
 positions:
  p80.1372.0
  p.78
 uses:
  call, operand 78
  custom-call.77.0, operand 1
 from instruction: %p80.1372.0 = bf16[6144,1024]{1,0} parameter(80), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1369 p79.1370.0 @0>
 positions:
  p79.1370.0
  p.79
 uses:
  call, operand 79
  custom-call.78.0, operand 1
 from instruction: %p79.1370.0 = bf16[1024,3072]{1,0} parameter(79), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1370 p85.1446.0 @0>
 positions:
  p85.1446.0
  p.80
 uses:
  call, operand 80
  fusion.234, operand 1
 from instruction: %p85.1446.0 = bf16[1024]{0} parameter(85), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1371 p84.1444.0 @0>
 positions:
  p84.1444.0
  p.81
 uses:
  call, operand 81
  custom-call.79.0, operand 1
 from instruction: %p84.1444.0 = bf16[6144,1024]{1,0} parameter(84), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1372 p83.1442.0 @0>
 positions:
  p83.1442.0
  p.82
 uses:
  call, operand 82
  custom-call.80.0, operand 1
 from instruction: %p83.1442.0 = bf16[1024,3072]{1,0} parameter(83), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1373 p3.8.0 @0>
 positions:
  p3.8.0
  p.83
 uses:
  call, operand 83
  fusion.232, operand 2
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1374 p2.6.0 @0>
 positions:
  p2.6.0
  p.84
 uses:
  call, operand 84
  custom-call.81.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1375 p0.1.0 @0>
 positions:
  p0.1.0
  p.85
 uses:
  call, operand 85
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1376 p87.1571.0 @0>
 positions:
  p87.1571.0
  p.86
 uses:
  call, operand 86
  input_concatenate_fusion, operand 2
 from instruction: %p87.1571.0 = bf16[40960,128]{1,0} parameter(87), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1377 p86.1570.0 @0>
 positions:
  p86.1570.0
  p.87
 uses:
  call, operand 87
  input_concatenate_fusion, operand 3
 from instruction: %p86.1570.0 = s32[16]{0} parameter(86), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1378 p88.1615.0 @0>
 positions:
  p88.1615.0
  p.88
 uses:
  call, operand 88
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p88.1615.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(88), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1379 tuple.1624.0{} @0>
 positions:
  tuple.1624.0 {}
 uses:
 from instruction: %tuple.1624.0 = (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[16,8,128]{2,1,0} %get-tuple-element.43, bf16[16,8,128]{2,1,0} %get-tuple-element.44, bf16[4233,16,8,128]{3,2,1,0} %bitcast.4119.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.45)


HloLiveRange (max 323):
  InstructionSequence:
    0:p1.4.0
    1:p88.1615.0
    2:p87.1571.0
    3:p86.1570.0
    4:p85.1446.0
    5:p84.1444.0
    6:p83.1442.0
    7:p82.1426.0
    8:p81.1374.0
    9:p80.1372.0
    10:p79.1370.0
    11:p78.1354.0
    12:p77.1302.0
    13:p76.1300.0
    14:p75.1298.0
    15:p74.1282.0
    16:p73.1230.0
    17:p72.1228.0
    18:p71.1226.0
    19:p70.1210.0
    20:p69.1158.0
    21:p68.1156.0
    22:p67.1154.0
    23:p66.1138.0
    24:p65.1086.0
    25:p64.1084.0
    26:p63.1082.0
    27:p62.1066.0
    28:p61.1014.0
    29:p60.1012.0
    30:p59.1010.0
    31:p58.994.0
    32:p57.942.0
    33:p56.940.0
    34:p55.938.0
    35:p54.922.0
    36:p53.870.0
    37:p52.868.0
    38:p51.866.0
    39:p50.850.0
    40:p49.798.0
    41:p48.796.0
    42:p47.794.0
    43:p46.778.0
    44:p45.726.0
    45:p44.724.0
    46:p43.722.0
    47:p42.706.0
    48:p41.654.0
    49:p40.652.0
    50:p39.650.0
    51:p38.634.0
    52:p37.582.0
    53:p36.580.0
    54:p35.578.0
    55:p34.562.0
    56:p33.510.0
    57:p32.508.0
    58:p31.506.0
    59:p30.490.0
    60:p29.438.0
    61:p28.436.0
    62:p27.434.0
    63:p26.418.0
    64:p25.366.0
    65:p24.364.0
    66:p23.362.0
    67:p22.346.0
    68:p21.294.0
    69:p20.292.0
    70:p19.290.0
    71:p18.274.0
    72:p17.222.0
    73:p16.220.0
    74:p15.218.0
    75:p14.202.0
    76:p13.150.0
    77:p12.148.0
    78:p11.146.0
    79:p10.130.0
    80:p9.78.0
    81:p8.76.0
    82:p7.74.0
    83:p6.58.0
    84:p5.54.0
    85:p4.52.0
    86:p3.8.0
    87:p2.6.0
    88:p0.1.0
    89:p
    90:p.1
    91:p.2
    92:p.3
    93:p.4
    94:p.5
    95:p.6
    96:p.7
    97:p.8
    98:p.9
    99:p.10
    100:p.11
    101:p.12
    102:p.13
    103:p.14
    104:p.15
    105:p.16
    106:p.17
    107:p.18
    108:p.19
    109:p.20
    110:p.21
    111:p.22
    112:p.23
    113:p.24
    114:p.25
    115:p.26
    116:p.27
    117:p.28
    118:p.29
    119:p.30
    120:p.31
    121:p.32
    122:p.33
    123:p.34
    124:p.35
    125:p.36
    126:p.37
    127:p.38
    128:p.39
    129:p.40
    130:p.41
    131:p.42
    132:p.43
    133:p.44
    134:p.45
    135:p.46
    136:p.47
    137:p.48
    138:p.49
    139:p.50
    140:p.51
    141:p.52
    142:p.53
    143:p.54
    144:p.55
    145:p.56
    146:p.57
    147:p.58
    148:p.59
    149:p.60
    150:p.61
    151:p.62
    152:p.63
    153:p.64
    154:p.65
    155:p.66
    156:p.67
    157:p.68
    158:p.69
    159:p.70
    160:p.71
    161:p.72
    162:p.73
    163:p.74
    164:p.75
    165:p.76
    166:p.77
    167:p.78
    168:p.79
    169:p.80
    170:p.81
    171:p.82
    172:p.83
    173:p.84
    174:p.85
    175:p.86
    176:p.87
    177:p.88
    178:loop_gather_fusion
    179:wrapped_concatenate
    180:gemm_fusion_dot.80.0
    181:fusion.272
    182:custom-call.41.0
    183:get-tuple-element.41
    184:loop_convert_fusion.19
    185:custom-call.42.0
    186:get-tuple-element.1.0
    187:fusion.270
    188:custom-call.43.0
    189:get-tuple-element.2.0
    190:loop_convert_fusion.18
    191:custom-call.44.0
    192:get-tuple-element.3.0
    193:fusion.268
    194:custom-call.45.0
    195:get-tuple-element.4.0
    196:loop_convert_fusion.17
    197:custom-call.46.0
    198:get-tuple-element.5.0
    199:fusion.266
    200:custom-call.47.0
    201:get-tuple-element.6.0
    202:loop_convert_fusion.16
    203:custom-call.48.0
    204:get-tuple-element.7.0
    205:fusion.264
    206:custom-call.49.0
    207:get-tuple-element.8.0
    208:loop_convert_fusion.15
    209:custom-call.50.0
    210:get-tuple-element.9.0
    211:loop_add_fusion
    212:fusion.262
    213:custom-call.51.0
    214:get-tuple-element.10.0
    215:loop_convert_fusion.14
    216:custom-call.52.0
    217:get-tuple-element.11.0
    218:fusion.260
    219:custom-call.53.0
    220:get-tuple-element.12.0
    221:loop_convert_fusion.13
    222:custom-call.54.0
    223:get-tuple-element.13.0
    224:fusion.258
    225:custom-call.55.0
    226:get-tuple-element.14.0
    227:loop_convert_fusion.12
    228:custom-call.56.0
    229:get-tuple-element.15.0
    230:fusion.256
    231:custom-call.57.0
    232:get-tuple-element.16.0
    233:loop_convert_fusion.11
    234:custom-call.58.0
    235:get-tuple-element.17.0
    236:loop_add_fusion.1
    237:fusion.254
    238:custom-call.59.0
    239:get-tuple-element.18.0
    240:loop_convert_fusion.10
    241:custom-call.60.0
    242:get-tuple-element.19.0
    243:fusion.252
    244:custom-call.61.0
    245:get-tuple-element.20.0
    246:loop_convert_fusion.9
    247:custom-call.62.0
    248:get-tuple-element.21.0
    249:fusion.250
    250:custom-call.63.0
    251:get-tuple-element.22.0
    252:loop_convert_fusion.8
    253:custom-call.64.0
    254:get-tuple-element.23.0
    255:fusion.248
    256:custom-call.65.0
    257:get-tuple-element.24.0
    258:loop_convert_fusion.7
    259:custom-call.66.0
    260:get-tuple-element.25.0
    261:loop_add_fusion.2
    262:fusion.246
    263:custom-call.67.0
    264:get-tuple-element.26.0
    265:loop_convert_fusion.6
    266:custom-call.68.0
    267:get-tuple-element.27.0
    268:fusion.244
    269:custom-call.69.0
    270:get-tuple-element.28.0
    271:loop_convert_fusion.5
    272:custom-call.70.0
    273:get-tuple-element.29.0
    274:fusion.242
    275:custom-call.71.0
    276:get-tuple-element.30.0
    277:loop_convert_fusion.4
    278:custom-call.72.0
    279:get-tuple-element.31.0
    280:fusion.240
    281:custom-call.73.0
    282:get-tuple-element.32.0
    283:loop_convert_fusion.3
    284:custom-call.74.0
    285:get-tuple-element.33.0
    286:loop_add_fusion.3
    287:fusion.238
    288:custom-call.75.0
    289:get-tuple-element.34.0
    290:loop_convert_fusion.2
    291:custom-call.76.0
    292:get-tuple-element.35.0
    293:fusion.236
    294:custom-call.77.0
    295:get-tuple-element.36.0
    296:loop_convert_fusion.1
    297:custom-call.78.0
    298:get-tuple-element.37.0
    299:fusion.234
    300:custom-call.79.0
    301:get-tuple-element.38.0
    302:loop_convert_fusion
    303:custom-call.80.0
    304:get-tuple-element.39.0
    305:fusion.232
    306:custom-call.81.0
    307:get-tuple-element.40.0
    308:wrapped_slice
    309:triton_softmax.22.0
    310:input_concatenate_fusion
    311:bitcast.4114.0
    312:loop_slice_fusion
    313:bitcast.4126.0
    314:wrapped_slice.1
    315:tuple
    316:call
    317:get-tuple-element.43
    318:get-tuple-element.44
    319:get-tuple-element.45
    320:get-tuple-element.46
    321:bitcast.4119.0
    322:tuple.1624.0
  BufferLiveRange:
    wrapped_concatenate{}:179-180
    gemm_fusion_dot.80.0{}:180-305
    loop_gather_fusion{}:178-211
    fusion.272{}:181-182
    custom-call.41.0{}:182-183
    custom-call.41.0{0}:182-184
    custom-call.41.0{1}:182-182
    loop_convert_fusion.19{}:184-185
    custom-call.42.0{}:185-186
    custom-call.42.0{0}:185-211
    custom-call.42.0{1}:185-185
    fusion.270{}:187-188
    custom-call.43.0{}:188-189
    custom-call.43.0{0}:188-190
    custom-call.43.0{1}:188-188
    loop_convert_fusion.18{}:190-191
    custom-call.44.0{}:191-192
    custom-call.44.0{0}:191-211
    custom-call.44.0{1}:191-191
    fusion.268{}:193-194
    custom-call.45.0{}:194-195
    custom-call.45.0{0}:194-196
    custom-call.45.0{1}:194-194
    loop_convert_fusion.17{}:196-197
    custom-call.46.0{}:197-198
    custom-call.46.0{0}:197-211
    custom-call.46.0{1}:197-197
    fusion.266{}:199-200
    custom-call.47.0{}:200-201
    custom-call.47.0{0}:200-202
    custom-call.47.0{1}:200-200
    loop_convert_fusion.16{}:202-203
    custom-call.48.0{}:203-204
    custom-call.48.0{0}:203-211
    custom-call.48.0{1}:203-203
    fusion.264{}:205-206
    custom-call.49.0{}:206-207
    custom-call.49.0{0}:206-208
    custom-call.49.0{1}:206-206
    loop_convert_fusion.15{}:208-209
    custom-call.50.0{}:209-210
    custom-call.50.0{0}:209-211
    custom-call.50.0{1}:209-209
    loop_add_fusion{}:211-236
    fusion.262{}:212-213
    custom-call.51.0{}:213-214
    custom-call.51.0{0}:213-215
    custom-call.51.0{1}:213-213
    loop_convert_fusion.14{}:215-216
    custom-call.52.0{}:216-217
    custom-call.52.0{0}:216-236
    custom-call.52.0{1}:216-216
    fusion.260{}:218-219
    custom-call.53.0{}:219-220
    custom-call.53.0{0}:219-221
    custom-call.53.0{1}:219-219
    loop_convert_fusion.13{}:221-222
    custom-call.54.0{}:222-223
    custom-call.54.0{0}:222-236
    custom-call.54.0{1}:222-222
    fusion.258{}:224-225
    custom-call.55.0{}:225-226
    custom-call.55.0{0}:225-227
    custom-call.55.0{1}:225-225
    loop_convert_fusion.12{}:227-228
    custom-call.56.0{}:228-229
    custom-call.56.0{0}:228-236
    custom-call.56.0{1}:228-228
    fusion.256{}:230-231
    custom-call.57.0{}:231-232
    custom-call.57.0{0}:231-233
    custom-call.57.0{1}:231-231
    loop_convert_fusion.11{}:233-234
    custom-call.58.0{}:234-235
    custom-call.58.0{0}:234-236
    custom-call.58.0{1}:234-234
    loop_add_fusion.1{}:236-261
    fusion.254{}:237-238
    custom-call.59.0{}:238-239
    custom-call.59.0{0}:238-240
    custom-call.59.0{1}:238-238
    loop_convert_fusion.10{}:240-241
    custom-call.60.0{}:241-242
    custom-call.60.0{0}:241-261
    custom-call.60.0{1}:241-241
    fusion.252{}:243-244
    custom-call.61.0{}:244-245
    custom-call.61.0{0}:244-246
    custom-call.61.0{1}:244-244
    loop_convert_fusion.9{}:246-247
    custom-call.62.0{}:247-248
    custom-call.62.0{0}:247-261
    custom-call.62.0{1}:247-247
    fusion.250{}:249-250
    custom-call.63.0{}:250-251
    custom-call.63.0{0}:250-252
    custom-call.63.0{1}:250-250
    loop_convert_fusion.8{}:252-253
    custom-call.64.0{}:253-254
    custom-call.64.0{0}:253-261
    custom-call.64.0{1}:253-253
    fusion.248{}:255-256
    custom-call.65.0{}:256-257
    custom-call.65.0{0}:256-258
    custom-call.65.0{1}:256-256
    loop_convert_fusion.7{}:258-259
    custom-call.66.0{}:259-260
    custom-call.66.0{0}:259-261
    custom-call.66.0{1}:259-259
    loop_add_fusion.2{}:261-286
    fusion.246{}:262-263
    custom-call.67.0{}:263-264
    custom-call.67.0{0}:263-265
    custom-call.67.0{1}:263-263
    loop_convert_fusion.6{}:265-266
    custom-call.68.0{}:266-267
    custom-call.68.0{0}:266-286
    custom-call.68.0{1}:266-266
    fusion.244{}:268-269
    custom-call.69.0{}:269-270
    custom-call.69.0{0}:269-271
    custom-call.69.0{1}:269-269
    loop_convert_fusion.5{}:271-272
    custom-call.70.0{}:272-273
    custom-call.70.0{0}:272-286
    custom-call.70.0{1}:272-272
    fusion.242{}:274-275
    custom-call.71.0{}:275-276
    custom-call.71.0{0}:275-277
    custom-call.71.0{1}:275-275
    loop_convert_fusion.4{}:277-278
    custom-call.72.0{}:278-279
    custom-call.72.0{0}:278-286
    custom-call.72.0{1}:278-278
    fusion.240{}:280-281
    custom-call.73.0{}:281-282
    custom-call.73.0{0}:281-283
    custom-call.73.0{1}:281-281
    loop_convert_fusion.3{}:283-284
    custom-call.74.0{}:284-285
    custom-call.74.0{0}:284-286
    custom-call.74.0{1}:284-284
    loop_add_fusion.3{}:286-305
    fusion.238{}:287-288
    custom-call.75.0{}:288-289
    custom-call.75.0{0}:288-290
    custom-call.75.0{1}:288-288
    loop_convert_fusion.2{}:290-291
    custom-call.76.0{}:291-292
    custom-call.76.0{0}:291-305
    custom-call.76.0{1}:291-291
    fusion.236{}:293-294
    custom-call.77.0{}:294-295
    custom-call.77.0{0}:294-296
    custom-call.77.0{1}:294-294
    loop_convert_fusion.1{}:296-297
    custom-call.78.0{}:297-298
    custom-call.78.0{0}:297-305
    custom-call.78.0{1}:297-297
    fusion.234{}:299-300
    custom-call.79.0{}:300-301
    custom-call.79.0{0}:300-302
    custom-call.79.0{1}:300-300
    loop_convert_fusion{}:302-303
    custom-call.80.0{}:303-304
    custom-call.80.0{0}:303-305
    custom-call.80.0{1}:303-303
    fusion.232{}:305-306
    custom-call.81.0{}:306-307
    custom-call.81.0{0}:306-309
    custom-call.81.0{1}:306-306
    triton_softmax.22.0{}:309-310
    input_concatenate_fusion{}:310-323
    wrapped_slice{}:308-323
    loop_slice_fusion{}:312-323
    wrapped_slice.1{}:314-323
    tuple{}:315-320
    p5.54.0{}:0-323
    p4.52.0{}:0-323
    p82.1426.0{}:0-323
    p78.1354.0{}:0-323
    p74.1282.0{}:0-323
    p70.1210.0{}:0-323
    p66.1138.0{}:0-323
    p62.1066.0{}:0-323
    p58.994.0{}:0-323
    p54.922.0{}:0-323
    p50.850.0{}:0-323
    p46.778.0{}:0-323
    p42.706.0{}:0-323
    p38.634.0{}:0-323
    p34.562.0{}:0-323
    p30.490.0{}:0-323
    p26.418.0{}:0-323
    p22.346.0{}:0-323
    p18.274.0{}:0-323
    p14.202.0{}:0-323
    p10.130.0{}:0-323
    p6.58.0{}:0-323
    p1.4.0{}:0-323
    p9.78.0{}:0-323
    p8.76.0{}:0-323
    p7.74.0{}:0-323
    p13.150.0{}:0-323
    p12.148.0{}:0-323
    p11.146.0{}:0-323
    p17.222.0{}:0-323
    p16.220.0{}:0-323
    p15.218.0{}:0-323
    p21.294.0{}:0-323
    p20.292.0{}:0-323
    p19.290.0{}:0-323
    p25.366.0{}:0-323
    p24.364.0{}:0-323
    p23.362.0{}:0-323
    p29.438.0{}:0-323
    p28.436.0{}:0-323
    p27.434.0{}:0-323
    p33.510.0{}:0-323
    p32.508.0{}:0-323
    p31.506.0{}:0-323
    p37.582.0{}:0-323
    p36.580.0{}:0-323
    p35.578.0{}:0-323
    p41.654.0{}:0-323
    p40.652.0{}:0-323
    p39.650.0{}:0-323
    p45.726.0{}:0-323
    p44.724.0{}:0-323
    p43.722.0{}:0-323
    p49.798.0{}:0-323
    p48.796.0{}:0-323
    p47.794.0{}:0-323
    p53.870.0{}:0-323
    p52.868.0{}:0-323
    p51.866.0{}:0-323
    p57.942.0{}:0-323
    p56.940.0{}:0-323
    p55.938.0{}:0-323
    p61.1014.0{}:0-323
    p60.1012.0{}:0-323
    p59.1010.0{}:0-323
    p65.1086.0{}:0-323
    p64.1084.0{}:0-323
    p63.1082.0{}:0-323
    p69.1158.0{}:0-323
    p68.1156.0{}:0-323
    p67.1154.0{}:0-323
    p73.1230.0{}:0-323
    p72.1228.0{}:0-323
    p71.1226.0{}:0-323
    p77.1302.0{}:0-323
    p76.1300.0{}:0-323
    p75.1298.0{}:0-323
    p81.1374.0{}:0-323
    p80.1372.0{}:0-323
    p79.1370.0{}:0-323
    p85.1446.0{}:0-323
    p84.1444.0{}:0-323
    p83.1442.0{}:0-323
    p3.8.0{}:0-323
    p2.6.0{}:0-323
    p0.1.0{}:0-323
    p87.1571.0{}:0-323
    p86.1570.0{}:0-323
    p88.1615.0{}:0-323
    tuple.1624.0{}:322-323
  Live ranges at 315 (peak):
    input_concatenate_fusion: 32768 bytes
    wrapped_slice: 32768 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.54.0: 311164928 bytes
    p4.52.0: 64 bytes
    p82.1426.0: 4194304 bytes
    p78.1354.0: 4194304 bytes
    p74.1282.0: 4194304 bytes
    p70.1210.0: 4194304 bytes
    p66.1138.0: 4194304 bytes
    p62.1066.0: 4194304 bytes
    p58.994.0: 4194304 bytes
    p54.922.0: 4194304 bytes
    p50.850.0: 4194304 bytes
    p46.778.0: 4194304 bytes
    p42.706.0: 4194304 bytes
    p38.634.0: 4194304 bytes
    p34.562.0: 4194304 bytes
    p30.490.0: 4194304 bytes
    p26.418.0: 4194304 bytes
    p22.346.0: 4194304 bytes
    p18.274.0: 4194304 bytes
    p14.202.0: 4194304 bytes
    p10.130.0: 4194304 bytes
    p6.58.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.78.0: 2048 bytes
    p8.76.0: 12582912 bytes
    p7.74.0: 6291456 bytes
    p13.150.0: 2048 bytes
    p12.148.0: 12582912 bytes
    p11.146.0: 6291456 bytes
    p17.222.0: 2048 bytes
    p16.220.0: 12582912 bytes
    p15.218.0: 6291456 bytes
    p21.294.0: 2048 bytes
    p20.292.0: 12582912 bytes
    p19.290.0: 6291456 bytes
    p25.366.0: 2048 bytes
    p24.364.0: 12582912 bytes
    p23.362.0: 6291456 bytes
    p29.438.0: 2048 bytes
    p28.436.0: 12582912 bytes
    p27.434.0: 6291456 bytes
    p33.510.0: 2048 bytes
    p32.508.0: 12582912 bytes
    p31.506.0: 6291456 bytes
    p37.582.0: 2048 bytes
    p36.580.0: 12582912 bytes
    p35.578.0: 6291456 bytes
    p41.654.0: 2048 bytes
    p40.652.0: 12582912 bytes
    p39.650.0: 6291456 bytes
    p45.726.0: 2048 bytes
    p44.724.0: 12582912 bytes
    p43.722.0: 6291456 bytes
    p49.798.0: 2048 bytes
    p48.796.0: 12582912 bytes
    p47.794.0: 6291456 bytes
    p53.870.0: 2048 bytes
    p52.868.0: 12582912 bytes
    p51.866.0: 6291456 bytes
    p57.942.0: 2048 bytes
    p56.940.0: 12582912 bytes
    p55.938.0: 6291456 bytes
    p61.1014.0: 2048 bytes
    p60.1012.0: 12582912 bytes
    p59.1010.0: 6291456 bytes
    p65.1086.0: 2048 bytes
    p64.1084.0: 12582912 bytes
    p63.1082.0: 6291456 bytes
    p69.1158.0: 2048 bytes
    p68.1156.0: 12582912 bytes
    p67.1154.0: 6291456 bytes
    p73.1230.0: 2048 bytes
    p72.1228.0: 12582912 bytes
    p71.1226.0: 6291456 bytes
    p77.1302.0: 2048 bytes
    p76.1300.0: 12582912 bytes
    p75.1298.0: 6291456 bytes
    p81.1374.0: 2048 bytes
    p80.1372.0: 12582912 bytes
    p79.1370.0: 6291456 bytes
    p85.1446.0: 2048 bytes
    p84.1444.0: 12582912 bytes
    p83.1442.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p87.1571.0: 10485760 bytes
    p86.1570.0: 64 bytes
    p88.1615.0: 277413888 bytes
