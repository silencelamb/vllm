BufferAssignment:
allocation 0: size 311164928, parameter 31, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <1738 p31.148.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, maybe-live-out:
 value: <1709 copy.28 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 277413888, maybe-live-out:
 value: <1710 copy.29 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 3: size 277413888, maybe-live-out:
 value: <1711 copy.30 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 277413888, maybe-live-out:
 value: <1712 copy.31 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 5: size 277413888, maybe-live-out:
 value: <1713 copy.32 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 6: size 277413888, maybe-live-out:
 value: <1714 copy.33 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 7: size 277413888, maybe-live-out:
 value: <1715 copy.34 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 8: size 277413888, maybe-live-out:
 value: <1716 copy.35 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 9: size 277413888, maybe-live-out:
 value: <1717 copy.36 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 10: size 277413888, maybe-live-out:
 value: <1718 copy.37 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 11: size 277413888, maybe-live-out:
 value: <1719 copy.38 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 12: size 277413888, maybe-live-out:
 value: <1720 copy.39 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 13: size 277413888, maybe-live-out:
 value: <1721 copy.40 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 14: size 277413888, maybe-live-out:
 value: <1722 copy.41 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 15: size 277413888, maybe-live-out:
 value: <1723 copy.42 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 16: size 277413888, maybe-live-out:
 value: <1724 copy.43 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 17: size 277413888, maybe-live-out:
 value: <1725 copy.44 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 18: size 277413888, maybe-live-out:
 value: <1726 copy.45 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 19: size 277413888, maybe-live-out:
 value: <1727 copy.46 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 20: size 277413888, maybe-live-out:
 value: <1507 custom-call.63.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1728 copy.47 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 21: size 277413888, maybe-live-out:
 value: <1499 custom-call.61.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1729 copy.48 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 22: size 277413888, maybe-live-out:
 value: <1491 custom-call.59.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1540 custom-call.71.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1573 custom-call.79.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1606 custom-call.87.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1639 custom-call.95.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1672 custom-call.103.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1697 custom-call.109.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1730 copy.49 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 23: size 277413888, maybe-live-out:
 value: <1483 custom-call.57.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1532 custom-call.69.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1565 custom-call.77.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1598 custom-call.85.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1631 custom-call.93.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1664 custom-call.101.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1689 custom-call.107.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1731 copy.50 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 24: size 277413888, maybe-live-out:
 value: <1517 loop_add_fusion @0> (size=131072,offset=0): f32[32,1024]{1,0}
 value: <1550 loop_add_fusion.1 @0> (size=131072,offset=0): f32[32,1024]{1,0}
 value: <1583 loop_add_fusion.2 @0> (size=131072,offset=0): f32[32,1024]{1,0}
 value: <1616 loop_add_fusion.3 @0> (size=131072,offset=0): f32[32,1024]{1,0}
 value: <1649 loop_add_fusion.4 @0> (size=131072,offset=0): f32[32,1024]{1,0}
 value: <1682 loop_add_fusion.5 @0> (size=131072,offset=0): f32[32,1024]{1,0}
 value: <1732 copy.51 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 25: size 277413888, maybe-live-out:
 value: <1477 fusion.343 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1481 loop_convert_fusion @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1485 fusion.344 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1489 loop_convert_fusion.1 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1493 fusion.345 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1497 loop_convert_fusion.2 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1501 fusion.346 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1505 loop_convert_fusion.3 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1509 fusion.347 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1513 loop_convert_fusion.4 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1522 loop_convert_fusion.5 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1526 fusion.349 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1530 loop_convert_fusion.6 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1534 fusion.350 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1538 loop_convert_fusion.7 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1542 fusion.351 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1546 loop_convert_fusion.8 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1555 loop_convert_fusion.9 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1559 fusion.353 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1563 loop_convert_fusion.10 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1567 fusion.354 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1571 loop_convert_fusion.11 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1575 fusion.355 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1579 loop_convert_fusion.12 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1588 loop_convert_fusion.13 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1592 fusion.357 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1596 loop_convert_fusion.14 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1600 fusion.358 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1604 loop_convert_fusion.15 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1608 fusion.359 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1612 loop_convert_fusion.16 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1621 loop_convert_fusion.17 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1625 fusion.361 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1629 loop_convert_fusion.18 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1633 fusion.362 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1637 loop_convert_fusion.19 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1641 fusion.363 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1645 loop_convert_fusion.20 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1654 loop_convert_fusion.21 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1658 fusion.365 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1662 loop_convert_fusion.22 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1666 fusion.366 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1670 loop_convert_fusion.23 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1674 fusion.367 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1678 loop_convert_fusion.24 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1687 loop_convert_fusion.25 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1695 loop_convert_fusion.26 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1703 loop_convert_fusion.27 @0> (size=196608,offset=0): bf16[32,3072]{1,0}
 value: <1733 copy.52 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 26: size 277413888, maybe-live-out:
 value: <1479 custom-call.56.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1487 custom-call.58.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1495 custom-call.60.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1503 custom-call.62.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1511 custom-call.64.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1515 custom-call.65.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1520 custom-call.66.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1528 custom-call.68.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1536 custom-call.70.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1544 custom-call.72.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1548 custom-call.73.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1553 custom-call.74.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1561 custom-call.76.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1569 custom-call.78.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1577 custom-call.80.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1581 custom-call.81.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1586 custom-call.82.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1594 custom-call.84.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1602 custom-call.86.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1610 custom-call.88.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1614 custom-call.89.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1619 custom-call.90.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1627 custom-call.92.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1635 custom-call.94.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1643 custom-call.96.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1647 custom-call.97.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1652 custom-call.98.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1660 custom-call.100.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1668 custom-call.102.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1676 custom-call.104.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1680 custom-call.105.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1685 custom-call.106.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1693 custom-call.108.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1701 custom-call.110.0{0} @0> (size=393216,offset=0): bf16[32,6144]{1,0}
 value: <1705 custom-call.111.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1734 copy.53 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 27: size 277413888, maybe-live-out:
 value: <1475 gemm_fusion_dot.110.0 @0> (size=1835008,offset=0): bf16[32,28672]{1,0}
 value: <1735 copy.54 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 28: size 277413888, maybe-live-out:
 value: <1474 wrapped_concatenate @0> (size=117440512,offset=0): bf16[28672,2048]{1,0}
 value: <1480 custom-call.56.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1484 custom-call.57.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1488 custom-call.58.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1492 custom-call.59.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1496 custom-call.60.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1500 custom-call.61.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1504 custom-call.62.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1508 custom-call.63.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1512 custom-call.64.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1516 custom-call.65.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1521 custom-call.66.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1525 custom-call.67.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1529 custom-call.68.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1533 custom-call.69.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1537 custom-call.70.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1541 custom-call.71.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1545 custom-call.72.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1549 custom-call.73.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1554 custom-call.74.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1558 custom-call.75.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1562 custom-call.76.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1566 custom-call.77.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1570 custom-call.78.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1574 custom-call.79.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1578 custom-call.80.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1582 custom-call.81.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1587 custom-call.82.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1591 custom-call.83.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1595 custom-call.84.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1599 custom-call.85.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1603 custom-call.86.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1607 custom-call.87.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1611 custom-call.88.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1615 custom-call.89.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1620 custom-call.90.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1624 custom-call.91.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1628 custom-call.92.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1632 custom-call.93.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1636 custom-call.94.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1640 custom-call.95.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1644 custom-call.96.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1648 custom-call.97.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1653 custom-call.98.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1657 custom-call.99.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1661 custom-call.100.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1665 custom-call.101.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1669 custom-call.102.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1673 custom-call.103.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1677 custom-call.104.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1681 custom-call.105.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1686 custom-call.106.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1690 custom-call.107.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1694 custom-call.108.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1698 custom-call.109.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1702 custom-call.110.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1706 custom-call.111.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1736 copy.55 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 29: size 277413888, parameter 0, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1854 p0.1.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 30: size 277413888, parameter 1, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1855 p1.4.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 31: size 277413888, parameter 2, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1856 p2.7.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 32: size 277413888, parameter 3, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1857 p3.10.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 33: size 277413888, parameter 4, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1858 p4.13.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 34: size 277413888, parameter 5, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1859 p5.16.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 35: size 277413888, parameter 6, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1860 p6.19.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 36: size 277413888, parameter 7, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1861 p7.22.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 37: size 277413888, parameter 8, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1862 p8.25.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 38: size 277413888, parameter 9, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1863 p9.28.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 39: size 277413888, parameter 10, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1864 p10.31.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 40: size 277413888, parameter 11, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1865 p11.34.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 41: size 277413888, parameter 12, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1866 p12.37.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 42: size 277413888, parameter 13, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1867 p13.40.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 43: size 277413888, parameter 14, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1868 p14.43.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 44: size 277413888, parameter 15, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1869 p15.46.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 45: size 277413888, parameter 16, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1870 p16.49.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 46: size 277413888, parameter 17, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1871 p17.52.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 47: size 277413888, parameter 18, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1872 p18.55.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 48: size 277413888, parameter 19, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1873 p19.58.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 49: size 277413888, parameter 20, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1874 p20.61.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 50: size 277413888, parameter 21, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1875 p21.64.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 51: size 277413888, parameter 22, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1876 p22.67.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 52: size 277413888, parameter 23, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1877 p23.70.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 53: size 277413888, parameter 24, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1878 p24.73.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 54: size 277413888, parameter 25, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1879 p25.76.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 55: size 277413888, parameter 26, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1880 p26.79.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 56: size 277413888, parameter 27, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1881 p27.82.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 57: size 12582912, parameter 34, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1770 p34.170.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 58: size 12582912, parameter 38, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1773 p38.242.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 59: size 12582912, parameter 42, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1776 p42.314.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 60: size 12582912, parameter 46, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1779 p46.386.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 61: size 12582912, parameter 50, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1782 p50.458.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 62: size 12582912, parameter 54, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1785 p54.530.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 63: size 12582912, parameter 58, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1788 p58.602.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 64: size 12582912, parameter 62, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1791 p62.674.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 65: size 12582912, parameter 66, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1794 p66.746.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 66: size 12582912, parameter 70, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1797 p70.818.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 67: size 12582912, parameter 74, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1800 p74.890.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 68: size 12582912, parameter 78, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1803 p78.962.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 69: size 12582912, parameter 82, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1806 p82.1034.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 70: size 12582912, parameter 86, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1809 p86.1106.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 71: size 12582912, parameter 90, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1812 p90.1178.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 72: size 12582912, parameter 94, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1815 p94.1250.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 73: size 12582912, parameter 98, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1818 p98.1322.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 74: size 12582912, parameter 102, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1821 p102.1394.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 75: size 12582912, parameter 106, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1824 p106.1466.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 76: size 12582912, parameter 110, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1827 p110.1538.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 77: size 12582912, parameter 114, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1830 p114.1610.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 78: size 12582912, parameter 118, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1833 p118.1682.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 79: size 12582912, parameter 122, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1836 p122.1754.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 80: size 12582912, parameter 126, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1839 p126.1826.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 81: size 12582912, parameter 130, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1842 p130.1898.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 82: size 12582912, parameter 134, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1845 p134.1970.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 83: size 12582912, parameter 138, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1848 p138.2042.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 84: size 12582912, parameter 142, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1851 p142.2114.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 85: size 6291456, parameter 33, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1771 p33.168.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 86: size 6291456, parameter 37, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1774 p37.240.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 87: size 6291456, parameter 41, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1777 p41.312.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 88: size 6291456, parameter 45, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1780 p45.384.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 89: size 6291456, parameter 49, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1783 p49.456.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 90: size 6291456, parameter 53, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1786 p53.528.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 91: size 6291456, parameter 57, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1789 p57.600.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 92: size 6291456, parameter 61, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1792 p61.672.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 93: size 6291456, parameter 65, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1795 p65.744.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 94: size 6291456, parameter 69, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1798 p69.816.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 95: size 6291456, parameter 73, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1801 p73.888.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 96: size 6291456, parameter 77, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1804 p77.960.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 97: size 6291456, parameter 81, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1807 p81.1032.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 98: size 6291456, parameter 85, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1810 p85.1104.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 99: size 6291456, parameter 89, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1813 p89.1176.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 100: size 6291456, parameter 93, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1816 p93.1248.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 101: size 6291456, parameter 97, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1819 p97.1320.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 102: size 6291456, parameter 101, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1822 p101.1392.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 103: size 6291456, parameter 105, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1825 p105.1464.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 104: size 6291456, parameter 109, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1828 p109.1536.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 105: size 6291456, parameter 113, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1831 p113.1608.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 106: size 6291456, parameter 117, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1834 p117.1680.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 107: size 6291456, parameter 121, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1837 p121.1752.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 108: size 6291456, parameter 125, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1840 p125.1824.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 109: size 6291456, parameter 129, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1843 p129.1896.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 110: size 6291456, parameter 133, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1846 p133.1968.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 111: size 6291456, parameter 137, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1849 p137.2040.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 112: size 6291456, parameter 141, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1852 p141.2112.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 113: size 4194304, parameter 140, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1740 p140.2096.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 114: size 4194304, parameter 136, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1741 p136.2024.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 115: size 4194304, parameter 132, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1742 p132.1952.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 116: size 4194304, parameter 128, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1743 p128.1880.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 117: size 4194304, parameter 124, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1744 p124.1808.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 118: size 4194304, parameter 120, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1745 p120.1736.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 119: size 4194304, parameter 116, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1746 p116.1664.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 120: size 4194304, parameter 112, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1747 p112.1592.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 121: size 4194304, parameter 108, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1748 p108.1520.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 122: size 4194304, parameter 104, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1749 p104.1448.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 123: size 4194304, parameter 100, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1750 p100.1376.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 124: size 4194304, parameter 96, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1751 p96.1304.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 125: size 4194304, parameter 92, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1752 p92.1232.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 126: size 4194304, parameter 88, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1753 p88.1160.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 127: size 4194304, parameter 84, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1754 p84.1088.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 128: size 4194304, parameter 80, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1755 p80.1016.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 129: size 4194304, parameter 76, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1756 p76.944.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 130: size 4194304, parameter 72, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1757 p72.872.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 131: size 4194304, parameter 68, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1758 p68.800.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 132: size 4194304, parameter 64, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1759 p64.728.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 133: size 4194304, parameter 60, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1760 p60.656.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 134: size 4194304, parameter 56, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1761 p56.584.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 135: size 4194304, parameter 52, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1762 p52.512.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 136: size 4194304, parameter 48, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1763 p48.440.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 137: size 4194304, parameter 44, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1764 p44.368.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 138: size 4194304, parameter 40, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1765 p40.296.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 139: size 4194304, parameter 36, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1766 p36.224.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 140: size 4194304, parameter 32, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1767 p32.152.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 141: size 65536, maybe-live-out:
 value: <1476 loop_gather_fusion @0> (size=65536,offset=0): bf16[32,1,1024]{2,0,1}
 value: <1518 fusion.348 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1524 custom-call.67.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1551 fusion.352 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1557 custom-call.75.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1584 fusion.356 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1590 custom-call.83.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1617 fusion.360 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1623 custom-call.91.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1650 fusion.364 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1656 custom-call.99.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1683 fusion.368 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1691 fusion.369 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1699 fusion.370 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <1707 fusion.371 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
allocation 142: size 2048, parameter 35, shape |bf16[1024]| at ShapeIndex {}:
 value: <1769 p35.172.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 143: size 2048, parameter 39, shape |bf16[1024]| at ShapeIndex {}:
 value: <1772 p39.244.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 144: size 2048, parameter 43, shape |bf16[1024]| at ShapeIndex {}:
 value: <1775 p43.316.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 145: size 2048, parameter 47, shape |bf16[1024]| at ShapeIndex {}:
 value: <1778 p47.388.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 146: size 2048, parameter 51, shape |bf16[1024]| at ShapeIndex {}:
 value: <1781 p51.460.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 147: size 2048, parameter 55, shape |bf16[1024]| at ShapeIndex {}:
 value: <1784 p55.532.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 148: size 2048, parameter 59, shape |bf16[1024]| at ShapeIndex {}:
 value: <1787 p59.604.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 149: size 2048, parameter 63, shape |bf16[1024]| at ShapeIndex {}:
 value: <1790 p63.676.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 150: size 2048, parameter 67, shape |bf16[1024]| at ShapeIndex {}:
 value: <1793 p67.748.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 151: size 2048, parameter 71, shape |bf16[1024]| at ShapeIndex {}:
 value: <1796 p71.820.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 152: size 2048, parameter 75, shape |bf16[1024]| at ShapeIndex {}:
 value: <1799 p75.892.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 153: size 2048, parameter 79, shape |bf16[1024]| at ShapeIndex {}:
 value: <1802 p79.964.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 154: size 2048, parameter 83, shape |bf16[1024]| at ShapeIndex {}:
 value: <1805 p83.1036.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 155: size 2048, parameter 87, shape |bf16[1024]| at ShapeIndex {}:
 value: <1808 p87.1108.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 156: size 2048, parameter 91, shape |bf16[1024]| at ShapeIndex {}:
 value: <1811 p91.1180.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 157: size 2048, parameter 95, shape |bf16[1024]| at ShapeIndex {}:
 value: <1814 p95.1252.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 158: size 2048, parameter 99, shape |bf16[1024]| at ShapeIndex {}:
 value: <1817 p99.1324.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 159: size 2048, parameter 103, shape |bf16[1024]| at ShapeIndex {}:
 value: <1820 p103.1396.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 160: size 2048, parameter 107, shape |bf16[1024]| at ShapeIndex {}:
 value: <1823 p107.1468.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 161: size 2048, parameter 111, shape |bf16[1024]| at ShapeIndex {}:
 value: <1826 p111.1540.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 162: size 2048, parameter 115, shape |bf16[1024]| at ShapeIndex {}:
 value: <1829 p115.1612.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 163: size 2048, parameter 119, shape |bf16[1024]| at ShapeIndex {}:
 value: <1832 p119.1684.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 164: size 2048, parameter 123, shape |bf16[1024]| at ShapeIndex {}:
 value: <1835 p123.1756.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 165: size 2048, parameter 127, shape |bf16[1024]| at ShapeIndex {}:
 value: <1838 p127.1828.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 166: size 2048, parameter 131, shape |bf16[1024]| at ShapeIndex {}:
 value: <1841 p131.1900.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 167: size 2048, parameter 135, shape |bf16[1024]| at ShapeIndex {}:
 value: <1844 p135.1972.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 168: size 2048, parameter 139, shape |bf16[1024]| at ShapeIndex {}:
 value: <1847 p139.2044.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 169: size 2048, parameter 143, shape |bf16[1024]| at ShapeIndex {}:
 value: <1850 p143.2116.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 170: size 2048, parameter 28, shape |bf16[1024]| at ShapeIndex {}:
 value: <1853 p28.85.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 171: size 232, output shape is |(bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], /*index=5*/bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], /*index=10*/bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], /*index=15*/bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], /*index=20*/bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], /*index=25*/bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[2,4233,16,8,128], bf16[32,1024])|, maybe-live-out:
 value: <1882 tuple.1{} @0> (size=232,offset=0): (bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[32,1024]{1,0})
allocation 172: size 4, thread-local:
 value: <40 add.354 @0> (size=4,offset=0): f32[]
allocation 173: size 4, thread-local:
 value: <39 y.182 @0> (size=4,offset=0): f32[]
allocation 174: size 128, parameter 30, shape |s32[32]| at ShapeIndex {}:
 value: <1739 p30.146.0 @0> (size=128,offset=0): s32[32]{0}
allocation 175: size 4, thread-local:
 value: <38 x.181 @0> (size=4,offset=0): f32[]
allocation 176: size 4, parameter 29, shape |f32[]| at ShapeIndex {}:
 value: <1768 p29.88.0 @0> (size=4,offset=0): f32[]
allocation 177: size 7656, preallocated-temp:
 value: <1478 custom-call.56.0{} @0> (size=16,offset=7040): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1482 custom-call.57.0{} @0> (size=16,offset=6912): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1486 custom-call.58.0{} @0> (size=16,offset=0): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1490 custom-call.59.0{} @0> (size=16,offset=128): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1494 custom-call.60.0{} @0> (size=16,offset=256): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1498 custom-call.61.0{} @0> (size=16,offset=384): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1502 custom-call.62.0{} @0> (size=16,offset=512): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1506 custom-call.63.0{} @0> (size=16,offset=640): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1510 custom-call.64.0{} @0> (size=16,offset=768): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1514 custom-call.65.0{} @0> (size=16,offset=896): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1519 custom-call.66.0{} @0> (size=16,offset=1024): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1523 custom-call.67.0{} @0> (size=16,offset=1152): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1527 custom-call.68.0{} @0> (size=16,offset=1280): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1531 custom-call.69.0{} @0> (size=16,offset=1408): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1535 custom-call.70.0{} @0> (size=16,offset=1536): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1539 custom-call.71.0{} @0> (size=16,offset=1664): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1543 custom-call.72.0{} @0> (size=16,offset=1792): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1547 custom-call.73.0{} @0> (size=16,offset=1920): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1552 custom-call.74.0{} @0> (size=16,offset=2048): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1556 custom-call.75.0{} @0> (size=16,offset=2176): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1560 custom-call.76.0{} @0> (size=16,offset=2304): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1564 custom-call.77.0{} @0> (size=16,offset=2432): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1568 custom-call.78.0{} @0> (size=16,offset=2560): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1572 custom-call.79.0{} @0> (size=16,offset=2688): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1576 custom-call.80.0{} @0> (size=16,offset=2816): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1580 custom-call.81.0{} @0> (size=16,offset=2944): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1585 custom-call.82.0{} @0> (size=16,offset=3072): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1589 custom-call.83.0{} @0> (size=16,offset=3200): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1593 custom-call.84.0{} @0> (size=16,offset=3328): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1597 custom-call.85.0{} @0> (size=16,offset=3456): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1601 custom-call.86.0{} @0> (size=16,offset=3584): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1605 custom-call.87.0{} @0> (size=16,offset=3712): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1609 custom-call.88.0{} @0> (size=16,offset=3840): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1613 custom-call.89.0{} @0> (size=16,offset=3968): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1618 custom-call.90.0{} @0> (size=16,offset=4096): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1622 custom-call.91.0{} @0> (size=16,offset=4224): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1626 custom-call.92.0{} @0> (size=16,offset=4352): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1630 custom-call.93.0{} @0> (size=16,offset=4480): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1634 custom-call.94.0{} @0> (size=16,offset=4608): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1638 custom-call.95.0{} @0> (size=16,offset=4736): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1642 custom-call.96.0{} @0> (size=16,offset=4864): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1646 custom-call.97.0{} @0> (size=16,offset=4992): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1651 custom-call.98.0{} @0> (size=16,offset=5120): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1655 custom-call.99.0{} @0> (size=16,offset=5248): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1659 custom-call.100.0{} @0> (size=16,offset=5376): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1663 custom-call.101.0{} @0> (size=16,offset=5504): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1667 custom-call.102.0{} @0> (size=16,offset=5632): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1671 custom-call.103.0{} @0> (size=16,offset=5760): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1675 custom-call.104.0{} @0> (size=16,offset=5888): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1679 custom-call.105.0{} @0> (size=16,offset=6016): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1684 custom-call.106.0{} @0> (size=16,offset=6144): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1688 custom-call.107.0{} @0> (size=16,offset=6272): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1692 custom-call.108.0{} @0> (size=16,offset=6400): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1696 custom-call.109.0{} @0> (size=16,offset=6528): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1700 custom-call.110.0{} @0> (size=16,offset=6656): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <1704 custom-call.111.0{} @0> (size=16,offset=6784): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <1708 tuple{} @0> (size=232,offset=7424): (bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[32,1024]{1,0})
 value: <1737 tuple.2{} @0> (size=232,offset=7168): (bf16[32,1024]{1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 16492398432 (15.36GiB)

Used values:
<38 x.181 @0>
 positions:
  x.181
 uses:
  add.354, operand 0
 from instruction: %x.181 = f32[] parameter(0)
<39 y.182 @0>
 positions:
  y.182
 uses:
  add.354, operand 1
 from instruction: %y.182 = f32[] parameter(1)
<40 add.354 @0>
 positions:
  add.354
 uses:
 from instruction: %add.354 = f32[] add(f32[] %x.181, f32[] %y.182)
<1474 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.110.0, operand 0
 from instruction: %wrapped_concatenate = bf16[28672,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9, bf16[1024,2048]{1,0} %p.10, bf16[1024,2048]{1,0} %p.11, /*index=10*/bf16[1024,2048]{1,0} %p.12, bf16[1024,2048]{1,0} %p.13, bf16[1024,2048]{1,0} %p.14, bf16[1024,2048]{1,0} %p.15, bf16[1024,2048]{1,0} %p.16, /*index=15*/bf16[1024,2048]{1,0} %p.17, bf16[1024,2048]{1,0} %p.18, bf16[1024,2048]{1,0} %p.19, bf16[1024,2048]{1,0} %p.20, bf16[1024,2048]{1,0} %p.21, /*index=20*/bf16[1024,2048]{1,0} %p.22, bf16[1024,2048]{1,0} %p.23, bf16[1024,2048]{1,0} %p.24, bf16[1024,2048]{1,0} %p.25, bf16[1024,2048]{1,0} %p.26, /*index=25*/bf16[1024,2048]{1,0} %p.27, bf16[1024,2048]{1,0} %p.28, bf16[1024,2048]{1,0} %p.29), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1475 gemm_fusion_dot.110.0 @0>
 positions:
  gemm_fusion_dot.110.0
 uses:
  fusion.343, operand 2
  fusion.344, operand 3
  fusion.345, operand 3
  fusion.346, operand 3
  fusion.347, operand 3
  loop_add_fusion, operand 0
  fusion.349, operand 4
  fusion.350, operand 4
  fusion.351, operand 3
  loop_add_fusion.1, operand 0
  fusion.353, operand 4
  fusion.354, operand 4
  fusion.355, operand 3
  loop_add_fusion.2, operand 0
  fusion.357, operand 4
  fusion.358, operand 4
  fusion.359, operand 3
  loop_add_fusion.3, operand 0
  fusion.361, operand 4
  fusion.362, operand 4
  fusion.363, operand 3
  loop_add_fusion.4, operand 0
  fusion.365, operand 4
  fusion.366, operand 4
  fusion.367, operand 3
  loop_add_fusion.5, operand 0
  fusion.369, operand 4
  fusion.370, operand 4
  fusion.371, operand 5
 from instruction: %gemm_fusion_dot.110.0 = bf16[32,28672]{1,0} fusion(bf16[28672,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.110_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"64","block_n":"32","block_k":"128","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<1476 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.343, operand 1
  fusion.344, operand 2
  fusion.345, operand 4
  fusion.346, operand 2
  fusion.347, operand 4
  loop_add_fusion, operand 3
 from instruction: %loop_gather_fusion = bf16[32,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[32]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1477 fusion.343 @0>
 positions:
  fusion.343
 uses:
  custom-call.56.0, operand 0
 from instruction: %fusion.343 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[1024]{0} %p.31), kind=kCustom, calls=%fused_computation.284, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1478 custom-call.56.0{} @0>
 positions:
  custom-call.56.0 {}
 uses:
  get-tuple-element.56, operand 0 {}
 from instruction: %custom-call.56.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.343, bf16[6144,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1479 custom-call.56.0{0} @0>
 positions:
  custom-call.56.0 {0}
  get-tuple-element.56
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.56.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.343, bf16[6144,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1480 custom-call.56.0{1} @0>
 positions:
  custom-call.56.0 {1}
 uses:
 from instruction: %custom-call.56.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.343, bf16[6144,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1481 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.57.0, operand 0
 from instruction: %loop_convert_fusion = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.56), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1482 custom-call.57.0{} @0>
 positions:
  custom-call.57.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.57.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1483 custom-call.57.0{0} @0>
 positions:
  custom-call.57.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.344, operand 4
  fusion.345, operand 5
  fusion.346, operand 4
  fusion.347, operand 5
  loop_add_fusion, operand 4
 from instruction: %custom-call.57.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1484 custom-call.57.0{1} @0>
 positions:
  custom-call.57.0 {1}
 uses:
 from instruction: %custom-call.57.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1485 fusion.344 @0>
 positions:
  fusion.344
 uses:
  custom-call.58.0, operand 0
 from instruction: %fusion.344 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.34, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.285, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1486 custom-call.58.0{} @0>
 positions:
  custom-call.58.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.58.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.344, bf16[6144,1024]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1487 custom-call.58.0{0} @0>
 positions:
  custom-call.58.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.58.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.344, bf16[6144,1024]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1488 custom-call.58.0{1} @0>
 positions:
  custom-call.58.0 {1}
 uses:
 from instruction: %custom-call.58.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.344, bf16[6144,1024]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1489 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.59.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1490 custom-call.59.0{} @0>
 positions:
  custom-call.59.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.59.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1491 custom-call.59.0{0} @0>
 positions:
  custom-call.59.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.345, operand 2
  fusion.346, operand 5
  fusion.347, operand 6
  loop_add_fusion, operand 5
 from instruction: %custom-call.59.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1492 custom-call.59.0{1} @0>
 positions:
  custom-call.59.0 {1}
 uses:
 from instruction: %custom-call.59.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1493 fusion.345 @0>
 positions:
  fusion.345
 uses:
  custom-call.60.0, operand 0
 from instruction: %fusion.345 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.37, bf16[32,1024]{1,0} %get-tuple-element.3.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.286, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1494 custom-call.60.0{} @0>
 positions:
  custom-call.60.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.60.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.345, bf16[6144,1024]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1495 custom-call.60.0{0} @0>
 positions:
  custom-call.60.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.60.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.345, bf16[6144,1024]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1496 custom-call.60.0{1} @0>
 positions:
  custom-call.60.0 {1}
 uses:
 from instruction: %custom-call.60.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.345, bf16[6144,1024]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1497 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.61.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1498 custom-call.61.0{} @0>
 positions:
  custom-call.61.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.61.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1499 custom-call.61.0{0} @0>
 positions:
  custom-call.61.0 {0}
  get-tuple-element.5.0
 uses:
  fusion.346, operand 6
  fusion.347, operand 7
  loop_add_fusion, operand 6
 from instruction: %custom-call.61.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1500 custom-call.61.0{1} @0>
 positions:
  custom-call.61.0 {1}
 uses:
 from instruction: %custom-call.61.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1501 fusion.346 @0>
 positions:
  fusion.346
 uses:
  custom-call.62.0, operand 0
 from instruction: %fusion.346 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.40, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.3.0, bf16[32,1024]{1,0} %get-tuple-element.5.0), kind=kCustom, calls=%fused_computation.287, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1502 custom-call.62.0{} @0>
 positions:
  custom-call.62.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.62.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.346, bf16[6144,1024]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1503 custom-call.62.0{0} @0>
 positions:
  custom-call.62.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.62.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.346, bf16[6144,1024]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1504 custom-call.62.0{1} @0>
 positions:
  custom-call.62.0 {1}
 uses:
 from instruction: %custom-call.62.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.346, bf16[6144,1024]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1505 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.63.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1506 custom-call.63.0{} @0>
 positions:
  custom-call.63.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.63.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1507 custom-call.63.0{0} @0>
 positions:
  custom-call.63.0 {0}
  get-tuple-element.7.0
 uses:
  fusion.347, operand 2
  loop_add_fusion, operand 2
 from instruction: %custom-call.63.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1508 custom-call.63.0{1} @0>
 positions:
  custom-call.63.0 {1}
 uses:
 from instruction: %custom-call.63.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1509 fusion.347 @0>
 positions:
  fusion.347
 uses:
  custom-call.64.0, operand 0
 from instruction: %fusion.347 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.43, bf16[32,1024]{1,0} %get-tuple-element.7.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.1.0, bf16[32,1024]{1,0} %get-tuple-element.3.0, bf16[32,1024]{1,0} %get-tuple-element.5.0), kind=kCustom, calls=%fused_computation.288, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1510 custom-call.64.0{} @0>
 positions:
  custom-call.64.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.64.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.347, bf16[6144,1024]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1511 custom-call.64.0{0} @0>
 positions:
  custom-call.64.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.64.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.347, bf16[6144,1024]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1512 custom-call.64.0{1} @0>
 positions:
  custom-call.64.0 {1}
 uses:
 from instruction: %custom-call.64.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.347, bf16[6144,1024]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1513 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.65.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1514 custom-call.65.0{} @0>
 positions:
  custom-call.65.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.65.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1515 custom-call.65.0{0} @0>
 positions:
  custom-call.65.0 {0}
  get-tuple-element.9.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.65.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1516 custom-call.65.0{1} @0>
 positions:
  custom-call.65.0 {1}
 uses:
 from instruction: %custom-call.65.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1517 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.348, operand 0
  fusion.349, operand 2
  fusion.350, operand 2
  fusion.351, operand 4
  loop_add_fusion.1, operand 3
 from instruction: %loop_add_fusion = f32[32,1024]{1,0} fusion(bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.9.0, bf16[32,1024]{1,0} %get-tuple-element.7.0, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.3.0, bf16[32,1024]{1,0} %get-tuple-element.5.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.610/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1518 fusion.348 @0>
 positions:
  fusion.348
 uses:
  custom-call.66.0, operand 0
 from instruction: %fusion.348 = bf16[32,1024]{1,0} fusion(f32[32,1024]{1,0} %loop_add_fusion, f32[] %p.30, bf16[1024]{0} %p.46), kind=kCustom, calls=%fused_computation.289, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.348"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1519 custom-call.66.0{} @0>
 positions:
  custom-call.66.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.66.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.348, bf16[6144,1024]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1520 custom-call.66.0{0} @0>
 positions:
  custom-call.66.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.66.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.348, bf16[6144,1024]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1521 custom-call.66.0{1} @0>
 positions:
  custom-call.66.0 {1}
 uses:
 from instruction: %custom-call.66.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.348, bf16[6144,1024]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1522 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.67.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1523 custom-call.67.0{} @0>
 positions:
  custom-call.67.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.67.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1524 custom-call.67.0{0} @0>
 positions:
  custom-call.67.0 {0}
  get-tuple-element.11.0
 uses:
  fusion.349, operand 3
  fusion.350, operand 3
  fusion.351, operand 5
  loop_add_fusion.1, operand 4
 from instruction: %custom-call.67.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1525 custom-call.67.0{1} @0>
 positions:
  custom-call.67.0 {1}
 uses:
 from instruction: %custom-call.67.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1526 fusion.349 @0>
 positions:
  fusion.349
 uses:
  custom-call.68.0, operand 0
 from instruction: %fusion.349 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.49, f32[32,1024]{1,0} %loop_add_fusion, bf16[32,1024]{1,0} %get-tuple-element.11.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0), kind=kCustom, calls=%fused_computation.290, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1527 custom-call.68.0{} @0>
 positions:
  custom-call.68.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.68.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.349, bf16[6144,1024]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1528 custom-call.68.0{0} @0>
 positions:
  custom-call.68.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.68.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.349, bf16[6144,1024]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1529 custom-call.68.0{1} @0>
 positions:
  custom-call.68.0 {1}
 uses:
 from instruction: %custom-call.68.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.349, bf16[6144,1024]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1530 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.69.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1531 custom-call.69.0{} @0>
 positions:
  custom-call.69.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.69.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1532 custom-call.69.0{0} @0>
 positions:
  custom-call.69.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.350, operand 5
  fusion.351, operand 6
  loop_add_fusion.1, operand 5
 from instruction: %custom-call.69.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1533 custom-call.69.0{1} @0>
 positions:
  custom-call.69.0 {1}
 uses:
 from instruction: %custom-call.69.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1534 fusion.350 @0>
 positions:
  fusion.350
 uses:
  custom-call.70.0, operand 0
 from instruction: %fusion.350 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.52, f32[32,1024]{1,0} %loop_add_fusion, bf16[32,1024]{1,0} %get-tuple-element.11.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.13.0), kind=kCustom, calls=%fused_computation.291, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1535 custom-call.70.0{} @0>
 positions:
  custom-call.70.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.70.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.350, bf16[6144,1024]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1536 custom-call.70.0{0} @0>
 positions:
  custom-call.70.0 {0}
  get-tuple-element.14.0
 uses:
  loop_convert_fusion.7, operand 0
 from instruction: %custom-call.70.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.350, bf16[6144,1024]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1537 custom-call.70.0{1} @0>
 positions:
  custom-call.70.0 {1}
 uses:
 from instruction: %custom-call.70.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.350, bf16[6144,1024]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1538 loop_convert_fusion.7 @0>
 positions:
  loop_convert_fusion.7
 uses:
  custom-call.71.0, operand 0
 from instruction: %loop_convert_fusion.7 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1539 custom-call.71.0{} @0>
 positions:
  custom-call.71.0 {}
 uses:
  get-tuple-element.15.0, operand 0 {}
 from instruction: %custom-call.71.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1540 custom-call.71.0{0} @0>
 positions:
  custom-call.71.0 {0}
  get-tuple-element.15.0
 uses:
  fusion.351, operand 2
  loop_add_fusion.1, operand 2
 from instruction: %custom-call.71.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1541 custom-call.71.0{1} @0>
 positions:
  custom-call.71.0 {1}
 uses:
 from instruction: %custom-call.71.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1542 fusion.351 @0>
 positions:
  fusion.351
 uses:
  custom-call.72.0, operand 0
 from instruction: %fusion.351 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.55, bf16[32,1024]{1,0} %get-tuple-element.15.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, f32[32,1024]{1,0} %loop_add_fusion, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.11.0, bf16[32,1024]{1,0} %get-tuple-element.13.0), kind=kCustom, calls=%fused_computation.292, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1543 custom-call.72.0{} @0>
 positions:
  custom-call.72.0 {}
 uses:
  get-tuple-element.16.0, operand 0 {}
 from instruction: %custom-call.72.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.351, bf16[6144,1024]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1544 custom-call.72.0{0} @0>
 positions:
  custom-call.72.0 {0}
  get-tuple-element.16.0
 uses:
  loop_convert_fusion.8, operand 0
 from instruction: %custom-call.72.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.351, bf16[6144,1024]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1545 custom-call.72.0{1} @0>
 positions:
  custom-call.72.0 {1}
 uses:
 from instruction: %custom-call.72.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.351, bf16[6144,1024]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1546 loop_convert_fusion.8 @0>
 positions:
  loop_convert_fusion.8
 uses:
  custom-call.73.0, operand 0
 from instruction: %loop_convert_fusion.8 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%fused_convert.8, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1547 custom-call.73.0{} @0>
 positions:
  custom-call.73.0 {}
 uses:
  get-tuple-element.17.0, operand 0 {}
 from instruction: %custom-call.73.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1548 custom-call.73.0{0} @0>
 positions:
  custom-call.73.0 {0}
  get-tuple-element.17.0
 uses:
  loop_add_fusion.1, operand 1
 from instruction: %custom-call.73.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1549 custom-call.73.0{1} @0>
 positions:
  custom-call.73.0 {1}
 uses:
 from instruction: %custom-call.73.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1550 loop_add_fusion.1 @0>
 positions:
  loop_add_fusion.1
 uses:
  fusion.352, operand 0
  fusion.353, operand 2
  fusion.354, operand 2
  fusion.355, operand 4
  loop_add_fusion.2, operand 3
 from instruction: %loop_add_fusion.1 = f32[32,1024]{1,0} fusion(bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.17.0, bf16[32,1024]{1,0} %get-tuple-element.15.0, f32[32,1024]{1,0} %loop_add_fusion, bf16[32,1024]{1,0} %get-tuple-element.11.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.13.0), kind=kLoop, calls=%fused_add.1, metadata={op_type="aten__add" op_name="aten__add.682/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1551 fusion.352 @0>
 positions:
  fusion.352
 uses:
  custom-call.74.0, operand 0
 from instruction: %fusion.352 = bf16[32,1024]{1,0} fusion(f32[32,1024]{1,0} %loop_add_fusion.1, f32[] %p.30, bf16[1024]{0} %p.58), kind=kCustom, calls=%fused_computation.293, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.348"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1552 custom-call.74.0{} @0>
 positions:
  custom-call.74.0 {}
 uses:
  get-tuple-element.18.0, operand 0 {}
 from instruction: %custom-call.74.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.352, bf16[6144,1024]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1553 custom-call.74.0{0} @0>
 positions:
  custom-call.74.0 {0}
  get-tuple-element.18.0
 uses:
  loop_convert_fusion.9, operand 0
 from instruction: %custom-call.74.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.352, bf16[6144,1024]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1554 custom-call.74.0{1} @0>
 positions:
  custom-call.74.0 {1}
 uses:
 from instruction: %custom-call.74.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.352, bf16[6144,1024]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1555 loop_convert_fusion.9 @0>
 positions:
  loop_convert_fusion.9
 uses:
  custom-call.75.0, operand 0
 from instruction: %loop_convert_fusion.9 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.18.0), kind=kLoop, calls=%fused_convert.9, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1556 custom-call.75.0{} @0>
 positions:
  custom-call.75.0 {}
 uses:
  get-tuple-element.19.0, operand 0 {}
 from instruction: %custom-call.75.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1557 custom-call.75.0{0} @0>
 positions:
  custom-call.75.0 {0}
  get-tuple-element.19.0
 uses:
  fusion.353, operand 3
  fusion.354, operand 3
  fusion.355, operand 5
  loop_add_fusion.2, operand 4
 from instruction: %custom-call.75.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1558 custom-call.75.0{1} @0>
 positions:
  custom-call.75.0 {1}
 uses:
 from instruction: %custom-call.75.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1559 fusion.353 @0>
 positions:
  fusion.353
 uses:
  custom-call.76.0, operand 0
 from instruction: %fusion.353 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.61, f32[32,1024]{1,0} %loop_add_fusion.1, bf16[32,1024]{1,0} %get-tuple-element.19.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0), kind=kCustom, calls=%fused_computation.294, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1560 custom-call.76.0{} @0>
 positions:
  custom-call.76.0 {}
 uses:
  get-tuple-element.20.0, operand 0 {}
 from instruction: %custom-call.76.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.353, bf16[6144,1024]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1561 custom-call.76.0{0} @0>
 positions:
  custom-call.76.0 {0}
  get-tuple-element.20.0
 uses:
  loop_convert_fusion.10, operand 0
 from instruction: %custom-call.76.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.353, bf16[6144,1024]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1562 custom-call.76.0{1} @0>
 positions:
  custom-call.76.0 {1}
 uses:
 from instruction: %custom-call.76.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.353, bf16[6144,1024]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1563 loop_convert_fusion.10 @0>
 positions:
  loop_convert_fusion.10
 uses:
  custom-call.77.0, operand 0
 from instruction: %loop_convert_fusion.10 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.20.0), kind=kLoop, calls=%fused_convert.10, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1564 custom-call.77.0{} @0>
 positions:
  custom-call.77.0 {}
 uses:
  get-tuple-element.21.0, operand 0 {}
 from instruction: %custom-call.77.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1565 custom-call.77.0{0} @0>
 positions:
  custom-call.77.0 {0}
  get-tuple-element.21.0
 uses:
  fusion.354, operand 5
  fusion.355, operand 6
  loop_add_fusion.2, operand 5
 from instruction: %custom-call.77.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1566 custom-call.77.0{1} @0>
 positions:
  custom-call.77.0 {1}
 uses:
 from instruction: %custom-call.77.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1567 fusion.354 @0>
 positions:
  fusion.354
 uses:
  custom-call.78.0, operand 0
 from instruction: %fusion.354 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.64, f32[32,1024]{1,0} %loop_add_fusion.1, bf16[32,1024]{1,0} %get-tuple-element.19.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.21.0), kind=kCustom, calls=%fused_computation.295, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1568 custom-call.78.0{} @0>
 positions:
  custom-call.78.0 {}
 uses:
  get-tuple-element.22.0, operand 0 {}
 from instruction: %custom-call.78.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.354, bf16[6144,1024]{1,0} %p.65), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1569 custom-call.78.0{0} @0>
 positions:
  custom-call.78.0 {0}
  get-tuple-element.22.0
 uses:
  loop_convert_fusion.11, operand 0
 from instruction: %custom-call.78.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.354, bf16[6144,1024]{1,0} %p.65), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1570 custom-call.78.0{1} @0>
 positions:
  custom-call.78.0 {1}
 uses:
 from instruction: %custom-call.78.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.354, bf16[6144,1024]{1,0} %p.65), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1571 loop_convert_fusion.11 @0>
 positions:
  loop_convert_fusion.11
 uses:
  custom-call.79.0, operand 0
 from instruction: %loop_convert_fusion.11 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.22.0), kind=kLoop, calls=%fused_convert.11, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1572 custom-call.79.0{} @0>
 positions:
  custom-call.79.0 {}
 uses:
  get-tuple-element.23.0, operand 0 {}
 from instruction: %custom-call.79.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1573 custom-call.79.0{0} @0>
 positions:
  custom-call.79.0 {0}
  get-tuple-element.23.0
 uses:
  fusion.355, operand 2
  loop_add_fusion.2, operand 2
 from instruction: %custom-call.79.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1574 custom-call.79.0{1} @0>
 positions:
  custom-call.79.0 {1}
 uses:
 from instruction: %custom-call.79.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1575 fusion.355 @0>
 positions:
  fusion.355
 uses:
  custom-call.80.0, operand 0
 from instruction: %fusion.355 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.67, bf16[32,1024]{1,0} %get-tuple-element.23.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, f32[32,1024]{1,0} %loop_add_fusion.1, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.19.0, bf16[32,1024]{1,0} %get-tuple-element.21.0), kind=kCustom, calls=%fused_computation.296, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1576 custom-call.80.0{} @0>
 positions:
  custom-call.80.0 {}
 uses:
  get-tuple-element.24.0, operand 0 {}
 from instruction: %custom-call.80.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.355, bf16[6144,1024]{1,0} %p.68), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1577 custom-call.80.0{0} @0>
 positions:
  custom-call.80.0 {0}
  get-tuple-element.24.0
 uses:
  loop_convert_fusion.12, operand 0
 from instruction: %custom-call.80.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.355, bf16[6144,1024]{1,0} %p.68), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1578 custom-call.80.0{1} @0>
 positions:
  custom-call.80.0 {1}
 uses:
 from instruction: %custom-call.80.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.355, bf16[6144,1024]{1,0} %p.68), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1579 loop_convert_fusion.12 @0>
 positions:
  loop_convert_fusion.12
 uses:
  custom-call.81.0, operand 0
 from instruction: %loop_convert_fusion.12 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.24.0), kind=kLoop, calls=%fused_convert.12, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1580 custom-call.81.0{} @0>
 positions:
  custom-call.81.0 {}
 uses:
  get-tuple-element.25.0, operand 0 {}
 from instruction: %custom-call.81.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1581 custom-call.81.0{0} @0>
 positions:
  custom-call.81.0 {0}
  get-tuple-element.25.0
 uses:
  loop_add_fusion.2, operand 1
 from instruction: %custom-call.81.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1582 custom-call.81.0{1} @0>
 positions:
  custom-call.81.0 {1}
 uses:
 from instruction: %custom-call.81.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1583 loop_add_fusion.2 @0>
 positions:
  loop_add_fusion.2
 uses:
  fusion.356, operand 0
  fusion.357, operand 2
  fusion.358, operand 2
  fusion.359, operand 4
  loop_add_fusion.3, operand 3
 from instruction: %loop_add_fusion.2 = f32[32,1024]{1,0} fusion(bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.25.0, bf16[32,1024]{1,0} %get-tuple-element.23.0, f32[32,1024]{1,0} %loop_add_fusion.1, bf16[32,1024]{1,0} %get-tuple-element.19.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.21.0), kind=kLoop, calls=%fused_add.2, metadata={op_type="aten__add" op_name="aten__add.754/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1584 fusion.356 @0>
 positions:
  fusion.356
 uses:
  custom-call.82.0, operand 0
 from instruction: %fusion.356 = bf16[32,1024]{1,0} fusion(f32[32,1024]{1,0} %loop_add_fusion.2, f32[] %p.30, bf16[1024]{0} %p.70), kind=kCustom, calls=%fused_computation.297, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.348"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1585 custom-call.82.0{} @0>
 positions:
  custom-call.82.0 {}
 uses:
  get-tuple-element.26.0, operand 0 {}
 from instruction: %custom-call.82.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.356, bf16[6144,1024]{1,0} %p.71), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1586 custom-call.82.0{0} @0>
 positions:
  custom-call.82.0 {0}
  get-tuple-element.26.0
 uses:
  loop_convert_fusion.13, operand 0
 from instruction: %custom-call.82.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.356, bf16[6144,1024]{1,0} %p.71), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1587 custom-call.82.0{1} @0>
 positions:
  custom-call.82.0 {1}
 uses:
 from instruction: %custom-call.82.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.356, bf16[6144,1024]{1,0} %p.71), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1588 loop_convert_fusion.13 @0>
 positions:
  loop_convert_fusion.13
 uses:
  custom-call.83.0, operand 0
 from instruction: %loop_convert_fusion.13 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.26.0), kind=kLoop, calls=%fused_convert.13, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1589 custom-call.83.0{} @0>
 positions:
  custom-call.83.0 {}
 uses:
  get-tuple-element.27.0, operand 0 {}
 from instruction: %custom-call.83.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1590 custom-call.83.0{0} @0>
 positions:
  custom-call.83.0 {0}
  get-tuple-element.27.0
 uses:
  fusion.357, operand 3
  fusion.358, operand 3
  fusion.359, operand 5
  loop_add_fusion.3, operand 4
 from instruction: %custom-call.83.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1591 custom-call.83.0{1} @0>
 positions:
  custom-call.83.0 {1}
 uses:
 from instruction: %custom-call.83.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1592 fusion.357 @0>
 positions:
  fusion.357
 uses:
  custom-call.84.0, operand 0
 from instruction: %fusion.357 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.73, f32[32,1024]{1,0} %loop_add_fusion.2, bf16[32,1024]{1,0} %get-tuple-element.27.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0), kind=kCustom, calls=%fused_computation.298, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1593 custom-call.84.0{} @0>
 positions:
  custom-call.84.0 {}
 uses:
  get-tuple-element.28.0, operand 0 {}
 from instruction: %custom-call.84.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.357, bf16[6144,1024]{1,0} %p.74), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1594 custom-call.84.0{0} @0>
 positions:
  custom-call.84.0 {0}
  get-tuple-element.28.0
 uses:
  loop_convert_fusion.14, operand 0
 from instruction: %custom-call.84.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.357, bf16[6144,1024]{1,0} %p.74), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1595 custom-call.84.0{1} @0>
 positions:
  custom-call.84.0 {1}
 uses:
 from instruction: %custom-call.84.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.357, bf16[6144,1024]{1,0} %p.74), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1596 loop_convert_fusion.14 @0>
 positions:
  loop_convert_fusion.14
 uses:
  custom-call.85.0, operand 0
 from instruction: %loop_convert_fusion.14 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.28.0), kind=kLoop, calls=%fused_convert.14, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1597 custom-call.85.0{} @0>
 positions:
  custom-call.85.0 {}
 uses:
  get-tuple-element.29.0, operand 0 {}
 from instruction: %custom-call.85.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1598 custom-call.85.0{0} @0>
 positions:
  custom-call.85.0 {0}
  get-tuple-element.29.0
 uses:
  fusion.358, operand 5
  fusion.359, operand 6
  loop_add_fusion.3, operand 5
 from instruction: %custom-call.85.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1599 custom-call.85.0{1} @0>
 positions:
  custom-call.85.0 {1}
 uses:
 from instruction: %custom-call.85.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1600 fusion.358 @0>
 positions:
  fusion.358
 uses:
  custom-call.86.0, operand 0
 from instruction: %fusion.358 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.76, f32[32,1024]{1,0} %loop_add_fusion.2, bf16[32,1024]{1,0} %get-tuple-element.27.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.29.0), kind=kCustom, calls=%fused_computation.299, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1601 custom-call.86.0{} @0>
 positions:
  custom-call.86.0 {}
 uses:
  get-tuple-element.30.0, operand 0 {}
 from instruction: %custom-call.86.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.358, bf16[6144,1024]{1,0} %p.77), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1602 custom-call.86.0{0} @0>
 positions:
  custom-call.86.0 {0}
  get-tuple-element.30.0
 uses:
  loop_convert_fusion.15, operand 0
 from instruction: %custom-call.86.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.358, bf16[6144,1024]{1,0} %p.77), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1603 custom-call.86.0{1} @0>
 positions:
  custom-call.86.0 {1}
 uses:
 from instruction: %custom-call.86.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.358, bf16[6144,1024]{1,0} %p.77), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1604 loop_convert_fusion.15 @0>
 positions:
  loop_convert_fusion.15
 uses:
  custom-call.87.0, operand 0
 from instruction: %loop_convert_fusion.15 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.30.0), kind=kLoop, calls=%fused_convert.15, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1605 custom-call.87.0{} @0>
 positions:
  custom-call.87.0 {}
 uses:
  get-tuple-element.31.0, operand 0 {}
 from instruction: %custom-call.87.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1606 custom-call.87.0{0} @0>
 positions:
  custom-call.87.0 {0}
  get-tuple-element.31.0
 uses:
  fusion.359, operand 2
  loop_add_fusion.3, operand 2
 from instruction: %custom-call.87.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1607 custom-call.87.0{1} @0>
 positions:
  custom-call.87.0 {1}
 uses:
 from instruction: %custom-call.87.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1608 fusion.359 @0>
 positions:
  fusion.359
 uses:
  custom-call.88.0, operand 0
 from instruction: %fusion.359 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.79, bf16[32,1024]{1,0} %get-tuple-element.31.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, f32[32,1024]{1,0} %loop_add_fusion.2, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.27.0, bf16[32,1024]{1,0} %get-tuple-element.29.0), kind=kCustom, calls=%fused_computation.300, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1609 custom-call.88.0{} @0>
 positions:
  custom-call.88.0 {}
 uses:
  get-tuple-element.32.0, operand 0 {}
 from instruction: %custom-call.88.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.359, bf16[6144,1024]{1,0} %p.80), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1610 custom-call.88.0{0} @0>
 positions:
  custom-call.88.0 {0}
  get-tuple-element.32.0
 uses:
  loop_convert_fusion.16, operand 0
 from instruction: %custom-call.88.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.359, bf16[6144,1024]{1,0} %p.80), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1611 custom-call.88.0{1} @0>
 positions:
  custom-call.88.0 {1}
 uses:
 from instruction: %custom-call.88.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.359, bf16[6144,1024]{1,0} %p.80), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1612 loop_convert_fusion.16 @0>
 positions:
  loop_convert_fusion.16
 uses:
  custom-call.89.0, operand 0
 from instruction: %loop_convert_fusion.16 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.32.0), kind=kLoop, calls=%fused_convert.16, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1613 custom-call.89.0{} @0>
 positions:
  custom-call.89.0 {}
 uses:
  get-tuple-element.33.0, operand 0 {}
 from instruction: %custom-call.89.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1614 custom-call.89.0{0} @0>
 positions:
  custom-call.89.0 {0}
  get-tuple-element.33.0
 uses:
  loop_add_fusion.3, operand 1
 from instruction: %custom-call.89.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1615 custom-call.89.0{1} @0>
 positions:
  custom-call.89.0 {1}
 uses:
 from instruction: %custom-call.89.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1616 loop_add_fusion.3 @0>
 positions:
  loop_add_fusion.3
 uses:
  fusion.360, operand 0
  fusion.361, operand 2
  fusion.362, operand 2
  fusion.363, operand 4
  loop_add_fusion.4, operand 3
 from instruction: %loop_add_fusion.3 = f32[32,1024]{1,0} fusion(bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.33.0, bf16[32,1024]{1,0} %get-tuple-element.31.0, f32[32,1024]{1,0} %loop_add_fusion.2, bf16[32,1024]{1,0} %get-tuple-element.27.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.29.0), kind=kLoop, calls=%fused_add.3, metadata={op_type="aten__add" op_name="aten__add.826/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1617 fusion.360 @0>
 positions:
  fusion.360
 uses:
  custom-call.90.0, operand 0
 from instruction: %fusion.360 = bf16[32,1024]{1,0} fusion(f32[32,1024]{1,0} %loop_add_fusion.3, f32[] %p.30, bf16[1024]{0} %p.82), kind=kCustom, calls=%fused_computation.301, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.348"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1618 custom-call.90.0{} @0>
 positions:
  custom-call.90.0 {}
 uses:
  get-tuple-element.34.0, operand 0 {}
 from instruction: %custom-call.90.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.360, bf16[6144,1024]{1,0} %p.83), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1619 custom-call.90.0{0} @0>
 positions:
  custom-call.90.0 {0}
  get-tuple-element.34.0
 uses:
  loop_convert_fusion.17, operand 0
 from instruction: %custom-call.90.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.360, bf16[6144,1024]{1,0} %p.83), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1620 custom-call.90.0{1} @0>
 positions:
  custom-call.90.0 {1}
 uses:
 from instruction: %custom-call.90.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.360, bf16[6144,1024]{1,0} %p.83), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1621 loop_convert_fusion.17 @0>
 positions:
  loop_convert_fusion.17
 uses:
  custom-call.91.0, operand 0
 from instruction: %loop_convert_fusion.17 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.34.0), kind=kLoop, calls=%fused_convert.17, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1622 custom-call.91.0{} @0>
 positions:
  custom-call.91.0 {}
 uses:
  get-tuple-element.35.0, operand 0 {}
 from instruction: %custom-call.91.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1623 custom-call.91.0{0} @0>
 positions:
  custom-call.91.0 {0}
  get-tuple-element.35.0
 uses:
  fusion.361, operand 3
  fusion.362, operand 3
  fusion.363, operand 5
  loop_add_fusion.4, operand 4
 from instruction: %custom-call.91.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1624 custom-call.91.0{1} @0>
 positions:
  custom-call.91.0 {1}
 uses:
 from instruction: %custom-call.91.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1625 fusion.361 @0>
 positions:
  fusion.361
 uses:
  custom-call.92.0, operand 0
 from instruction: %fusion.361 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.85, f32[32,1024]{1,0} %loop_add_fusion.3, bf16[32,1024]{1,0} %get-tuple-element.35.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0), kind=kCustom, calls=%fused_computation.302, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1626 custom-call.92.0{} @0>
 positions:
  custom-call.92.0 {}
 uses:
  get-tuple-element.36.0, operand 0 {}
 from instruction: %custom-call.92.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.361, bf16[6144,1024]{1,0} %p.86), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1627 custom-call.92.0{0} @0>
 positions:
  custom-call.92.0 {0}
  get-tuple-element.36.0
 uses:
  loop_convert_fusion.18, operand 0
 from instruction: %custom-call.92.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.361, bf16[6144,1024]{1,0} %p.86), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1628 custom-call.92.0{1} @0>
 positions:
  custom-call.92.0 {1}
 uses:
 from instruction: %custom-call.92.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.361, bf16[6144,1024]{1,0} %p.86), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1629 loop_convert_fusion.18 @0>
 positions:
  loop_convert_fusion.18
 uses:
  custom-call.93.0, operand 0
 from instruction: %loop_convert_fusion.18 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.36.0), kind=kLoop, calls=%fused_convert.18, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1630 custom-call.93.0{} @0>
 positions:
  custom-call.93.0 {}
 uses:
  get-tuple-element.37.0, operand 0 {}
 from instruction: %custom-call.93.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.87), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1631 custom-call.93.0{0} @0>
 positions:
  custom-call.93.0 {0}
  get-tuple-element.37.0
 uses:
  fusion.362, operand 5
  fusion.363, operand 6
  loop_add_fusion.4, operand 5
 from instruction: %custom-call.93.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.87), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1632 custom-call.93.0{1} @0>
 positions:
  custom-call.93.0 {1}
 uses:
 from instruction: %custom-call.93.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.87), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1633 fusion.362 @0>
 positions:
  fusion.362
 uses:
  custom-call.94.0, operand 0
 from instruction: %fusion.362 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.88, f32[32,1024]{1,0} %loop_add_fusion.3, bf16[32,1024]{1,0} %get-tuple-element.35.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.37.0), kind=kCustom, calls=%fused_computation.303, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1634 custom-call.94.0{} @0>
 positions:
  custom-call.94.0 {}
 uses:
  get-tuple-element.38.0, operand 0 {}
 from instruction: %custom-call.94.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.362, bf16[6144,1024]{1,0} %p.89), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1635 custom-call.94.0{0} @0>
 positions:
  custom-call.94.0 {0}
  get-tuple-element.38.0
 uses:
  loop_convert_fusion.19, operand 0
 from instruction: %custom-call.94.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.362, bf16[6144,1024]{1,0} %p.89), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1636 custom-call.94.0{1} @0>
 positions:
  custom-call.94.0 {1}
 uses:
 from instruction: %custom-call.94.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.362, bf16[6144,1024]{1,0} %p.89), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1637 loop_convert_fusion.19 @0>
 positions:
  loop_convert_fusion.19
 uses:
  custom-call.95.0, operand 0
 from instruction: %loop_convert_fusion.19 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.38.0), kind=kLoop, calls=%fused_convert.19, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1638 custom-call.95.0{} @0>
 positions:
  custom-call.95.0 {}
 uses:
  get-tuple-element.39.0, operand 0 {}
 from instruction: %custom-call.95.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.90), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1639 custom-call.95.0{0} @0>
 positions:
  custom-call.95.0 {0}
  get-tuple-element.39.0
 uses:
  fusion.363, operand 2
  loop_add_fusion.4, operand 2
 from instruction: %custom-call.95.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.90), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1640 custom-call.95.0{1} @0>
 positions:
  custom-call.95.0 {1}
 uses:
 from instruction: %custom-call.95.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.90), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1641 fusion.363 @0>
 positions:
  fusion.363
 uses:
  custom-call.96.0, operand 0
 from instruction: %fusion.363 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.91, bf16[32,1024]{1,0} %get-tuple-element.39.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, f32[32,1024]{1,0} %loop_add_fusion.3, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.35.0, bf16[32,1024]{1,0} %get-tuple-element.37.0), kind=kCustom, calls=%fused_computation.304, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1642 custom-call.96.0{} @0>
 positions:
  custom-call.96.0 {}
 uses:
  get-tuple-element.40.0, operand 0 {}
 from instruction: %custom-call.96.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.363, bf16[6144,1024]{1,0} %p.92), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1643 custom-call.96.0{0} @0>
 positions:
  custom-call.96.0 {0}
  get-tuple-element.40.0
 uses:
  loop_convert_fusion.20, operand 0
 from instruction: %custom-call.96.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.363, bf16[6144,1024]{1,0} %p.92), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1644 custom-call.96.0{1} @0>
 positions:
  custom-call.96.0 {1}
 uses:
 from instruction: %custom-call.96.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.363, bf16[6144,1024]{1,0} %p.92), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1645 loop_convert_fusion.20 @0>
 positions:
  loop_convert_fusion.20
 uses:
  custom-call.97.0, operand 0
 from instruction: %loop_convert_fusion.20 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.40.0), kind=kLoop, calls=%fused_convert.20, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1646 custom-call.97.0{} @0>
 positions:
  custom-call.97.0 {}
 uses:
  get-tuple-element.41.0, operand 0 {}
 from instruction: %custom-call.97.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.93), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1647 custom-call.97.0{0} @0>
 positions:
  custom-call.97.0 {0}
  get-tuple-element.41.0
 uses:
  loop_add_fusion.4, operand 1
 from instruction: %custom-call.97.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.93), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1648 custom-call.97.0{1} @0>
 positions:
  custom-call.97.0 {1}
 uses:
 from instruction: %custom-call.97.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.93), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1649 loop_add_fusion.4 @0>
 positions:
  loop_add_fusion.4
 uses:
  fusion.364, operand 0
  fusion.365, operand 2
  fusion.366, operand 2
  fusion.367, operand 4
  loop_add_fusion.5, operand 3
 from instruction: %loop_add_fusion.4 = f32[32,1024]{1,0} fusion(bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.41.0, bf16[32,1024]{1,0} %get-tuple-element.39.0, f32[32,1024]{1,0} %loop_add_fusion.3, bf16[32,1024]{1,0} %get-tuple-element.35.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.37.0), kind=kLoop, calls=%fused_add.4, metadata={op_type="aten__add" op_name="aten__add.898/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1650 fusion.364 @0>
 positions:
  fusion.364
 uses:
  custom-call.98.0, operand 0
 from instruction: %fusion.364 = bf16[32,1024]{1,0} fusion(f32[32,1024]{1,0} %loop_add_fusion.4, f32[] %p.30, bf16[1024]{0} %p.94), kind=kCustom, calls=%fused_computation.305, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.348"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1651 custom-call.98.0{} @0>
 positions:
  custom-call.98.0 {}
 uses:
  get-tuple-element.42.0, operand 0 {}
 from instruction: %custom-call.98.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.364, bf16[6144,1024]{1,0} %p.95), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1652 custom-call.98.0{0} @0>
 positions:
  custom-call.98.0 {0}
  get-tuple-element.42.0
 uses:
  loop_convert_fusion.21, operand 0
 from instruction: %custom-call.98.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.364, bf16[6144,1024]{1,0} %p.95), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1653 custom-call.98.0{1} @0>
 positions:
  custom-call.98.0 {1}
 uses:
 from instruction: %custom-call.98.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.364, bf16[6144,1024]{1,0} %p.95), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1654 loop_convert_fusion.21 @0>
 positions:
  loop_convert_fusion.21
 uses:
  custom-call.99.0, operand 0
 from instruction: %loop_convert_fusion.21 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.42.0), kind=kLoop, calls=%fused_convert.21, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1655 custom-call.99.0{} @0>
 positions:
  custom-call.99.0 {}
 uses:
  get-tuple-element.43.0, operand 0 {}
 from instruction: %custom-call.99.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.96), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1656 custom-call.99.0{0} @0>
 positions:
  custom-call.99.0 {0}
  get-tuple-element.43.0
 uses:
  fusion.365, operand 3
  fusion.366, operand 3
  fusion.367, operand 5
  loop_add_fusion.5, operand 4
 from instruction: %custom-call.99.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.96), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1657 custom-call.99.0{1} @0>
 positions:
  custom-call.99.0 {1}
 uses:
 from instruction: %custom-call.99.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.96), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1658 fusion.365 @0>
 positions:
  fusion.365
 uses:
  custom-call.100.0, operand 0
 from instruction: %fusion.365 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.97, f32[32,1024]{1,0} %loop_add_fusion.4, bf16[32,1024]{1,0} %get-tuple-element.43.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0), kind=kCustom, calls=%fused_computation.306, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1659 custom-call.100.0{} @0>
 positions:
  custom-call.100.0 {}
 uses:
  get-tuple-element.44.0, operand 0 {}
 from instruction: %custom-call.100.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.365, bf16[6144,1024]{1,0} %p.98), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1660 custom-call.100.0{0} @0>
 positions:
  custom-call.100.0 {0}
  get-tuple-element.44.0
 uses:
  loop_convert_fusion.22, operand 0
 from instruction: %custom-call.100.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.365, bf16[6144,1024]{1,0} %p.98), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1661 custom-call.100.0{1} @0>
 positions:
  custom-call.100.0 {1}
 uses:
 from instruction: %custom-call.100.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.365, bf16[6144,1024]{1,0} %p.98), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1662 loop_convert_fusion.22 @0>
 positions:
  loop_convert_fusion.22
 uses:
  custom-call.101.0, operand 0
 from instruction: %loop_convert_fusion.22 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.44.0), kind=kLoop, calls=%fused_convert.22, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1663 custom-call.101.0{} @0>
 positions:
  custom-call.101.0 {}
 uses:
  get-tuple-element.45.0, operand 0 {}
 from instruction: %custom-call.101.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.99), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1664 custom-call.101.0{0} @0>
 positions:
  custom-call.101.0 {0}
  get-tuple-element.45.0
 uses:
  fusion.366, operand 5
  fusion.367, operand 6
  loop_add_fusion.5, operand 5
 from instruction: %custom-call.101.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.99), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1665 custom-call.101.0{1} @0>
 positions:
  custom-call.101.0 {1}
 uses:
 from instruction: %custom-call.101.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.99), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1666 fusion.366 @0>
 positions:
  fusion.366
 uses:
  custom-call.102.0, operand 0
 from instruction: %fusion.366 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.100, f32[32,1024]{1,0} %loop_add_fusion.4, bf16[32,1024]{1,0} %get-tuple-element.43.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.45.0), kind=kCustom, calls=%fused_computation.307, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1667 custom-call.102.0{} @0>
 positions:
  custom-call.102.0 {}
 uses:
  get-tuple-element.46.0, operand 0 {}
 from instruction: %custom-call.102.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.366, bf16[6144,1024]{1,0} %p.101), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1668 custom-call.102.0{0} @0>
 positions:
  custom-call.102.0 {0}
  get-tuple-element.46.0
 uses:
  loop_convert_fusion.23, operand 0
 from instruction: %custom-call.102.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.366, bf16[6144,1024]{1,0} %p.101), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1669 custom-call.102.0{1} @0>
 positions:
  custom-call.102.0 {1}
 uses:
 from instruction: %custom-call.102.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.366, bf16[6144,1024]{1,0} %p.101), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1670 loop_convert_fusion.23 @0>
 positions:
  loop_convert_fusion.23
 uses:
  custom-call.103.0, operand 0
 from instruction: %loop_convert_fusion.23 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.46.0), kind=kLoop, calls=%fused_convert.23, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1671 custom-call.103.0{} @0>
 positions:
  custom-call.103.0 {}
 uses:
  get-tuple-element.47.0, operand 0 {}
 from instruction: %custom-call.103.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.23, bf16[1024,3072]{1,0} %p.102), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1672 custom-call.103.0{0} @0>
 positions:
  custom-call.103.0 {0}
  get-tuple-element.47.0
 uses:
  fusion.367, operand 2
  loop_add_fusion.5, operand 2
 from instruction: %custom-call.103.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.23, bf16[1024,3072]{1,0} %p.102), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1673 custom-call.103.0{1} @0>
 positions:
  custom-call.103.0 {1}
 uses:
 from instruction: %custom-call.103.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.23, bf16[1024,3072]{1,0} %p.102), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1674 fusion.367 @0>
 positions:
  fusion.367
 uses:
  custom-call.104.0, operand 0
 from instruction: %fusion.367 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.103, bf16[32,1024]{1,0} %get-tuple-element.47.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, f32[32,1024]{1,0} %loop_add_fusion.4, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.43.0, bf16[32,1024]{1,0} %get-tuple-element.45.0), kind=kCustom, calls=%fused_computation.308, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1675 custom-call.104.0{} @0>
 positions:
  custom-call.104.0 {}
 uses:
  get-tuple-element.48.0, operand 0 {}
 from instruction: %custom-call.104.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.367, bf16[6144,1024]{1,0} %p.104), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1676 custom-call.104.0{0} @0>
 positions:
  custom-call.104.0 {0}
  get-tuple-element.48.0
 uses:
  loop_convert_fusion.24, operand 0
 from instruction: %custom-call.104.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.367, bf16[6144,1024]{1,0} %p.104), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1677 custom-call.104.0{1} @0>
 positions:
  custom-call.104.0 {1}
 uses:
 from instruction: %custom-call.104.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.367, bf16[6144,1024]{1,0} %p.104), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1678 loop_convert_fusion.24 @0>
 positions:
  loop_convert_fusion.24
 uses:
  custom-call.105.0, operand 0
 from instruction: %loop_convert_fusion.24 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.48.0), kind=kLoop, calls=%fused_convert.24, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1679 custom-call.105.0{} @0>
 positions:
  custom-call.105.0 {}
 uses:
  get-tuple-element.49.0, operand 0 {}
 from instruction: %custom-call.105.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.24, bf16[1024,3072]{1,0} %p.105), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1680 custom-call.105.0{0} @0>
 positions:
  custom-call.105.0 {0}
  get-tuple-element.49.0
 uses:
  loop_add_fusion.5, operand 1
 from instruction: %custom-call.105.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.24, bf16[1024,3072]{1,0} %p.105), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1681 custom-call.105.0{1} @0>
 positions:
  custom-call.105.0 {1}
 uses:
 from instruction: %custom-call.105.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.24, bf16[1024,3072]{1,0} %p.105), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1682 loop_add_fusion.5 @0>
 positions:
  loop_add_fusion.5
 uses:
  fusion.368, operand 0
  fusion.369, operand 2
  fusion.370, operand 2
  fusion.371, operand 3
 from instruction: %loop_add_fusion.5 = f32[32,1024]{1,0} fusion(bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.49.0, bf16[32,1024]{1,0} %get-tuple-element.47.0, f32[32,1024]{1,0} %loop_add_fusion.4, bf16[32,1024]{1,0} %get-tuple-element.43.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.45.0), kind=kLoop, calls=%fused_add.5, metadata={op_type="aten__add" op_name="aten__add.970/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1683 fusion.368 @0>
 positions:
  fusion.368
 uses:
  custom-call.106.0, operand 0
 from instruction: %fusion.368 = bf16[32,1024]{1,0} fusion(f32[32,1024]{1,0} %loop_add_fusion.5, f32[] %p.30, bf16[1024]{0} %p.106), kind=kCustom, calls=%fused_computation.309, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.348"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1684 custom-call.106.0{} @0>
 positions:
  custom-call.106.0 {}
 uses:
  get-tuple-element.50.0, operand 0 {}
 from instruction: %custom-call.106.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.368, bf16[6144,1024]{1,0} %p.107), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1685 custom-call.106.0{0} @0>
 positions:
  custom-call.106.0 {0}
  get-tuple-element.50.0
 uses:
  loop_convert_fusion.25, operand 0
 from instruction: %custom-call.106.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.368, bf16[6144,1024]{1,0} %p.107), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1686 custom-call.106.0{1} @0>
 positions:
  custom-call.106.0 {1}
 uses:
 from instruction: %custom-call.106.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.368, bf16[6144,1024]{1,0} %p.107), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1687 loop_convert_fusion.25 @0>
 positions:
  loop_convert_fusion.25
 uses:
  custom-call.107.0, operand 0
 from instruction: %loop_convert_fusion.25 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.50.0), kind=kLoop, calls=%fused_convert.25, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1688 custom-call.107.0{} @0>
 positions:
  custom-call.107.0 {}
 uses:
  get-tuple-element.51.0, operand 0 {}
 from instruction: %custom-call.107.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.25, bf16[1024,3072]{1,0} %p.108), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1689 custom-call.107.0{0} @0>
 positions:
  custom-call.107.0 {0}
  get-tuple-element.51.0
 uses:
  fusion.369, operand 3
  fusion.370, operand 3
  fusion.371, operand 4
 from instruction: %custom-call.107.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.25, bf16[1024,3072]{1,0} %p.108), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1690 custom-call.107.0{1} @0>
 positions:
  custom-call.107.0 {1}
 uses:
 from instruction: %custom-call.107.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.25, bf16[1024,3072]{1,0} %p.108), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1691 fusion.369 @0>
 positions:
  fusion.369
 uses:
  custom-call.108.0, operand 0
 from instruction: %fusion.369 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.109, f32[32,1024]{1,0} %loop_add_fusion.5, bf16[32,1024]{1,0} %get-tuple-element.51.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0), kind=kCustom, calls=%fused_computation.310, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1692 custom-call.108.0{} @0>
 positions:
  custom-call.108.0 {}
 uses:
  get-tuple-element.52.0, operand 0 {}
 from instruction: %custom-call.108.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.369, bf16[6144,1024]{1,0} %p.110), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1693 custom-call.108.0{0} @0>
 positions:
  custom-call.108.0 {0}
  get-tuple-element.52.0
 uses:
  loop_convert_fusion.26, operand 0
 from instruction: %custom-call.108.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.369, bf16[6144,1024]{1,0} %p.110), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1694 custom-call.108.0{1} @0>
 positions:
  custom-call.108.0 {1}
 uses:
 from instruction: %custom-call.108.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.369, bf16[6144,1024]{1,0} %p.110), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1695 loop_convert_fusion.26 @0>
 positions:
  loop_convert_fusion.26
 uses:
  custom-call.109.0, operand 0
 from instruction: %loop_convert_fusion.26 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.52.0), kind=kLoop, calls=%fused_convert.26, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1696 custom-call.109.0{} @0>
 positions:
  custom-call.109.0 {}
 uses:
  get-tuple-element.53.0, operand 0 {}
 from instruction: %custom-call.109.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.26, bf16[1024,3072]{1,0} %p.111), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1697 custom-call.109.0{0} @0>
 positions:
  custom-call.109.0 {0}
  get-tuple-element.53.0
 uses:
  fusion.370, operand 5
  fusion.371, operand 6
 from instruction: %custom-call.109.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.26, bf16[1024,3072]{1,0} %p.111), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1698 custom-call.109.0{1} @0>
 positions:
  custom-call.109.0 {1}
 uses:
 from instruction: %custom-call.109.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.26, bf16[1024,3072]{1,0} %p.111), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1699 fusion.370 @0>
 positions:
  fusion.370
 uses:
  custom-call.110.0, operand 0
 from instruction: %fusion.370 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[1024]{0} %p.112, f32[32,1024]{1,0} %loop_add_fusion.5, bf16[32,1024]{1,0} %get-tuple-element.51.0, bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.53.0), kind=kCustom, calls=%fused_computation.311, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1700 custom-call.110.0{} @0>
 positions:
  custom-call.110.0 {}
 uses:
  get-tuple-element.54.0, operand 0 {}
 from instruction: %custom-call.110.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.370, bf16[6144,1024]{1,0} %p.113), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1701 custom-call.110.0{0} @0>
 positions:
  custom-call.110.0 {0}
  get-tuple-element.54.0
 uses:
  loop_convert_fusion.27, operand 0
 from instruction: %custom-call.110.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.370, bf16[6144,1024]{1,0} %p.113), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1702 custom-call.110.0{1} @0>
 positions:
  custom-call.110.0 {1}
 uses:
 from instruction: %custom-call.110.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.370, bf16[6144,1024]{1,0} %p.113), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1703 loop_convert_fusion.27 @0>
 positions:
  loop_convert_fusion.27
 uses:
  custom-call.111.0, operand 0
 from instruction: %loop_convert_fusion.27 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.54.0), kind=kLoop, calls=%fused_convert.27, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1704 custom-call.111.0{} @0>
 positions:
  custom-call.111.0 {}
 uses:
  get-tuple-element.55.0, operand 0 {}
 from instruction: %custom-call.111.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.27, bf16[1024,3072]{1,0} %p.114), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1705 custom-call.111.0{0} @0>
 positions:
  custom-call.111.0 {0}
  get-tuple-element.55.0
 uses:
  fusion.371, operand 1
 from instruction: %custom-call.111.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.27, bf16[1024,3072]{1,0} %p.114), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1706 custom-call.111.0{1} @0>
 positions:
  custom-call.111.0 {1}
 uses:
 from instruction: %custom-call.111.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.27, bf16[1024,3072]{1,0} %p.114), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1707 fusion.371 @0>
 positions:
  fusion.371
  tuple.1 {28}
  call {0}
  get-tuple-element.145
  tuple {28}
  get-tuple-element.114
  tuple.2 {0}
 uses:
  tuple.1, operand 28
  tuple.2, operand 0
 from instruction: %fusion.371 = bf16[32,1024]{1,0} fusion(f32[] %p.30, bf16[32,1024]{1,0} %get-tuple-element.55.0, bf16[1024]{0} %p.115, f32[32,1024]{1,0} %loop_add_fusion.5, bf16[32,1024]{1,0} %get-tuple-element.51.0, /*index=5*/bf16[32,28672]{1,0} %gemm_fusion_dot.110.0, bf16[32,1024]{1,0} %get-tuple-element.53.0), kind=kCustom, calls=%fused_computation.312, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1708 tuple{} @0>
 positions:
  tuple {}
 uses:
  get-tuple-element.86, operand 0 {}
  get-tuple-element.87, operand 0 {}
  get-tuple-element.88, operand 0 {}
  get-tuple-element.89, operand 0 {}
  get-tuple-element.90, operand 0 {}
  get-tuple-element.91, operand 0 {}
  get-tuple-element.92, operand 0 {}
  get-tuple-element.93, operand 0 {}
  get-tuple-element.94, operand 0 {}
  get-tuple-element.95, operand 0 {}
  get-tuple-element.96, operand 0 {}
  get-tuple-element.97, operand 0 {}
  get-tuple-element.98, operand 0 {}
  get-tuple-element.99, operand 0 {}
  get-tuple-element.100, operand 0 {}
  get-tuple-element.101, operand 0 {}
  get-tuple-element.102, operand 0 {}
  get-tuple-element.103, operand 0 {}
  get-tuple-element.104, operand 0 {}
  get-tuple-element.105, operand 0 {}
  get-tuple-element.106, operand 0 {}
  get-tuple-element.107, operand 0 {}
  get-tuple-element.108, operand 0 {}
  get-tuple-element.109, operand 0 {}
  get-tuple-element.110, operand 0 {}
  get-tuple-element.111, operand 0 {}
  get-tuple-element.112, operand 0 {}
  get-tuple-element.113, operand 0 {}
  get-tuple-element.114, operand 0 {}
 from instruction: %tuple = (bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[32,1024]{1,0}) tuple(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.116, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.117, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.118, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.119, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.120, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0} %p.121, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.122, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.123, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.124, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.125, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0} %p.126, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.127, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.128, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.129, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.130, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0} %p.131, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.132, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.133, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.134, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.135, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0} %p.136, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.137, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.138, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.139, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.140, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0} %p.141, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.142, bf16[2,4233,16,8,128]{4,3,2,1,0} %p.143, bf16[32,1024]{1,0} %fusion.371)
<1709 copy.28 @0>
 positions:
  copy.28
  tuple.1 {0}
  call {1}
  get-tuple-element.146
  tuple.2 {1}
 uses:
  tuple.2, operand 1
  tuple.1, operand 0
 from instruction: %copy.28 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.86)
<1710 copy.29 @0>
 positions:
  copy.29
  tuple.1 {1}
  call {2}
  get-tuple-element.147
  tuple.2 {2}
 uses:
  tuple.2, operand 2
  tuple.1, operand 1
 from instruction: %copy.29 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.87)
<1711 copy.30 @0>
 positions:
  copy.30
  tuple.1 {2}
  call {3}
  get-tuple-element.148
  tuple.2 {3}
 uses:
  tuple.2, operand 3
  tuple.1, operand 2
 from instruction: %copy.30 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.88)
<1712 copy.31 @0>
 positions:
  copy.31
  tuple.1 {3}
  call {4}
  get-tuple-element.149
  tuple.2 {4}
 uses:
  tuple.2, operand 4
  tuple.1, operand 3
 from instruction: %copy.31 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.89)
<1713 copy.32 @0>
 positions:
  copy.32
  tuple.1 {4}
  call {5}
  get-tuple-element.150
  tuple.2 {5}
 uses:
  tuple.2, operand 5
  tuple.1, operand 4
 from instruction: %copy.32 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.90)
<1714 copy.33 @0>
 positions:
  copy.33
  tuple.1 {5}
  call {6}
  get-tuple-element.151
  tuple.2 {6}
 uses:
  tuple.2, operand 6
  tuple.1, operand 5
 from instruction: %copy.33 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.91)
<1715 copy.34 @0>
 positions:
  copy.34
  tuple.1 {6}
  call {7}
  get-tuple-element.152
  tuple.2 {7}
 uses:
  tuple.2, operand 7
  tuple.1, operand 6
 from instruction: %copy.34 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.92)
<1716 copy.35 @0>
 positions:
  copy.35
  tuple.1 {7}
  call {8}
  get-tuple-element.153
  tuple.2 {8}
 uses:
  tuple.2, operand 8
  tuple.1, operand 7
 from instruction: %copy.35 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.93)
<1717 copy.36 @0>
 positions:
  copy.36
  tuple.1 {8}
  call {9}
  get-tuple-element.154
  tuple.2 {9}
 uses:
  tuple.2, operand 9
  tuple.1, operand 8
 from instruction: %copy.36 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.94)
<1718 copy.37 @0>
 positions:
  copy.37
  tuple.1 {9}
  call {10}
  get-tuple-element.155
  tuple.2 {10}
 uses:
  tuple.2, operand 10
  tuple.1, operand 9
 from instruction: %copy.37 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.95)
<1719 copy.38 @0>
 positions:
  copy.38
  tuple.1 {10}
  call {11}
  get-tuple-element.156
  tuple.2 {11}
 uses:
  tuple.2, operand 11
  tuple.1, operand 10
 from instruction: %copy.38 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.96)
<1720 copy.39 @0>
 positions:
  copy.39
  tuple.1 {11}
  call {12}
  get-tuple-element.157
  tuple.2 {12}
 uses:
  tuple.2, operand 12
  tuple.1, operand 11
 from instruction: %copy.39 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.97)
<1721 copy.40 @0>
 positions:
  copy.40
  tuple.1 {12}
  call {13}
  get-tuple-element.158
  tuple.2 {13}
 uses:
  tuple.2, operand 13
  tuple.1, operand 12
 from instruction: %copy.40 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.98)
<1722 copy.41 @0>
 positions:
  copy.41
  tuple.1 {13}
  call {14}
  get-tuple-element.159
  tuple.2 {14}
 uses:
  tuple.2, operand 14
  tuple.1, operand 13
 from instruction: %copy.41 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.99)
<1723 copy.42 @0>
 positions:
  copy.42
  tuple.1 {14}
  call {15}
  get-tuple-element.160
  tuple.2 {15}
 uses:
  tuple.2, operand 15
  tuple.1, operand 14
 from instruction: %copy.42 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.100)
<1724 copy.43 @0>
 positions:
  copy.43
  tuple.1 {15}
  call {16}
  get-tuple-element.161
  tuple.2 {16}
 uses:
  tuple.2, operand 16
  tuple.1, operand 15
 from instruction: %copy.43 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.101)
<1725 copy.44 @0>
 positions:
  copy.44
  tuple.1 {16}
  call {17}
  get-tuple-element.162
  tuple.2 {17}
 uses:
  tuple.2, operand 17
  tuple.1, operand 16
 from instruction: %copy.44 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.102)
<1726 copy.45 @0>
 positions:
  copy.45
  tuple.1 {17}
  call {18}
  get-tuple-element.163
  tuple.2 {18}
 uses:
  tuple.2, operand 18
  tuple.1, operand 17
 from instruction: %copy.45 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.103)
<1727 copy.46 @0>
 positions:
  copy.46
  tuple.1 {18}
  call {19}
  get-tuple-element.164
  tuple.2 {19}
 uses:
  tuple.2, operand 19
  tuple.1, operand 18
 from instruction: %copy.46 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.104)
<1728 copy.47 @0>
 positions:
  copy.47
  tuple.1 {19}
  call {20}
  get-tuple-element.165
  tuple.2 {20}
 uses:
  tuple.2, operand 20
  tuple.1, operand 19
 from instruction: %copy.47 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.105)
<1729 copy.48 @0>
 positions:
  copy.48
  tuple.1 {20}
  call {21}
  get-tuple-element.166
  tuple.2 {21}
 uses:
  tuple.2, operand 21
  tuple.1, operand 20
 from instruction: %copy.48 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.106)
<1730 copy.49 @0>
 positions:
  copy.49
  tuple.1 {21}
  call {22}
  get-tuple-element.167
  tuple.2 {22}
 uses:
  tuple.2, operand 22
  tuple.1, operand 21
 from instruction: %copy.49 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.107)
<1731 copy.50 @0>
 positions:
  copy.50
  tuple.1 {22}
  call {23}
  get-tuple-element.168
  tuple.2 {23}
 uses:
  tuple.2, operand 23
  tuple.1, operand 22
 from instruction: %copy.50 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.108)
<1732 copy.51 @0>
 positions:
  copy.51
  tuple.1 {23}
  call {24}
  get-tuple-element.169
  tuple.2 {24}
 uses:
  tuple.2, operand 24
  tuple.1, operand 23
 from instruction: %copy.51 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.109)
<1733 copy.52 @0>
 positions:
  copy.52
  tuple.1 {24}
  call {25}
  get-tuple-element.170
  tuple.2 {25}
 uses:
  tuple.2, operand 25
  tuple.1, operand 24
 from instruction: %copy.52 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.110)
<1734 copy.53 @0>
 positions:
  copy.53
  tuple.1 {25}
  call {26}
  get-tuple-element.171
  tuple.2 {26}
 uses:
  tuple.2, operand 26
  tuple.1, operand 25
 from instruction: %copy.53 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.111)
<1735 copy.54 @0>
 positions:
  copy.54
  tuple.1 {26}
  call {27}
  get-tuple-element.172
  tuple.2 {27}
 uses:
  tuple.2, operand 27
  tuple.1, operand 26
 from instruction: %copy.54 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.112)
<1736 copy.55 @0>
 positions:
  copy.55
  tuple.1 {27}
  call {28}
  get-tuple-element.173
  tuple.2 {28}
 uses:
  tuple.2, operand 28
  tuple.1, operand 27
 from instruction: %copy.55 = bf16[2,4233,16,8,128]{4,3,2,1,0} copy(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.113)
<1737 tuple.2{} @0>
 positions:
  tuple.2 {}
  call {}
 uses:
  get-tuple-element.145, operand 0 {}
  get-tuple-element.146, operand 0 {}
  get-tuple-element.147, operand 0 {}
  get-tuple-element.148, operand 0 {}
  get-tuple-element.149, operand 0 {}
  get-tuple-element.150, operand 0 {}
  get-tuple-element.151, operand 0 {}
  get-tuple-element.152, operand 0 {}
  get-tuple-element.153, operand 0 {}
  get-tuple-element.154, operand 0 {}
  get-tuple-element.155, operand 0 {}
  get-tuple-element.156, operand 0 {}
  get-tuple-element.157, operand 0 {}
  get-tuple-element.158, operand 0 {}
  get-tuple-element.159, operand 0 {}
  get-tuple-element.160, operand 0 {}
  get-tuple-element.161, operand 0 {}
  get-tuple-element.162, operand 0 {}
  get-tuple-element.163, operand 0 {}
  get-tuple-element.164, operand 0 {}
  get-tuple-element.165, operand 0 {}
  get-tuple-element.166, operand 0 {}
  get-tuple-element.167, operand 0 {}
  get-tuple-element.168, operand 0 {}
  get-tuple-element.169, operand 0 {}
  get-tuple-element.170, operand 0 {}
  get-tuple-element.171, operand 0 {}
  get-tuple-element.172, operand 0 {}
  get-tuple-element.173, operand 0 {}
 from instruction: %tuple.2 = (bf16[32,1024]{1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[32,1024]{1,0} %get-tuple-element.114, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.28, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.29, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.30, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.31, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.32, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.33, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.34, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.35, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.36, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.37, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.38, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.39, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.40, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.41, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.42, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.43, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.44, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.45, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.46, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.47, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.48, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.49, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.50, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.51, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.52, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.53, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.54, bf16[2,4233,16,8,128]{4,3,2,1,0} %copy.55)
<1738 p31.148.0 @0>
 positions:
  p31.148.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p31.148.0 = bf16[151936,1024]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1739 p30.146.0 @0>
 positions:
  p30.146.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p30.146.0 = s32[32]{0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1740 p140.2096.0 @0>
 positions:
  p140.2096.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p140.2096.0 = bf16[1024,2048]{1,0} parameter(140), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1741 p136.2024.0 @0>
 positions:
  p136.2024.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p136.2024.0 = bf16[1024,2048]{1,0} parameter(136), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1742 p132.1952.0 @0>
 positions:
  p132.1952.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p132.1952.0 = bf16[1024,2048]{1,0} parameter(132), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1743 p128.1880.0 @0>
 positions:
  p128.1880.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p128.1880.0 = bf16[1024,2048]{1,0} parameter(128), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1744 p124.1808.0 @0>
 positions:
  p124.1808.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p124.1808.0 = bf16[1024,2048]{1,0} parameter(124), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1745 p120.1736.0 @0>
 positions:
  p120.1736.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p120.1736.0 = bf16[1024,2048]{1,0} parameter(120), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1746 p116.1664.0 @0>
 positions:
  p116.1664.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p116.1664.0 = bf16[1024,2048]{1,0} parameter(116), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1747 p112.1592.0 @0>
 positions:
  p112.1592.0
  p.9
 uses:
  call, operand 9
  wrapped_concatenate, operand 7
 from instruction: %p112.1592.0 = bf16[1024,2048]{1,0} parameter(112), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1748 p108.1520.0 @0>
 positions:
  p108.1520.0
  p.10
 uses:
  call, operand 10
  wrapped_concatenate, operand 8
 from instruction: %p108.1520.0 = bf16[1024,2048]{1,0} parameter(108), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1749 p104.1448.0 @0>
 positions:
  p104.1448.0
  p.11
 uses:
  call, operand 11
  wrapped_concatenate, operand 9
 from instruction: %p104.1448.0 = bf16[1024,2048]{1,0} parameter(104), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1750 p100.1376.0 @0>
 positions:
  p100.1376.0
  p.12
 uses:
  call, operand 12
  wrapped_concatenate, operand 10
 from instruction: %p100.1376.0 = bf16[1024,2048]{1,0} parameter(100), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1751 p96.1304.0 @0>
 positions:
  p96.1304.0
  p.13
 uses:
  call, operand 13
  wrapped_concatenate, operand 11
 from instruction: %p96.1304.0 = bf16[1024,2048]{1,0} parameter(96), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1752 p92.1232.0 @0>
 positions:
  p92.1232.0
  p.14
 uses:
  call, operand 14
  wrapped_concatenate, operand 12
 from instruction: %p92.1232.0 = bf16[1024,2048]{1,0} parameter(92), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1753 p88.1160.0 @0>
 positions:
  p88.1160.0
  p.15
 uses:
  call, operand 15
  wrapped_concatenate, operand 13
 from instruction: %p88.1160.0 = bf16[1024,2048]{1,0} parameter(88), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1754 p84.1088.0 @0>
 positions:
  p84.1088.0
  p.16
 uses:
  call, operand 16
  wrapped_concatenate, operand 14
 from instruction: %p84.1088.0 = bf16[1024,2048]{1,0} parameter(84), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1755 p80.1016.0 @0>
 positions:
  p80.1016.0
  p.17
 uses:
  call, operand 17
  wrapped_concatenate, operand 15
 from instruction: %p80.1016.0 = bf16[1024,2048]{1,0} parameter(80), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1756 p76.944.0 @0>
 positions:
  p76.944.0
  p.18
 uses:
  call, operand 18
  wrapped_concatenate, operand 16
 from instruction: %p76.944.0 = bf16[1024,2048]{1,0} parameter(76), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1757 p72.872.0 @0>
 positions:
  p72.872.0
  p.19
 uses:
  call, operand 19
  wrapped_concatenate, operand 17
 from instruction: %p72.872.0 = bf16[1024,2048]{1,0} parameter(72), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1758 p68.800.0 @0>
 positions:
  p68.800.0
  p.20
 uses:
  call, operand 20
  wrapped_concatenate, operand 18
 from instruction: %p68.800.0 = bf16[1024,2048]{1,0} parameter(68), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1759 p64.728.0 @0>
 positions:
  p64.728.0
  p.21
 uses:
  call, operand 21
  wrapped_concatenate, operand 19
 from instruction: %p64.728.0 = bf16[1024,2048]{1,0} parameter(64), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1760 p60.656.0 @0>
 positions:
  p60.656.0
  p.22
 uses:
  call, operand 22
  wrapped_concatenate, operand 20
 from instruction: %p60.656.0 = bf16[1024,2048]{1,0} parameter(60), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1761 p56.584.0 @0>
 positions:
  p56.584.0
  p.23
 uses:
  call, operand 23
  wrapped_concatenate, operand 21
 from instruction: %p56.584.0 = bf16[1024,2048]{1,0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1762 p52.512.0 @0>
 positions:
  p52.512.0
  p.24
 uses:
  call, operand 24
  wrapped_concatenate, operand 22
 from instruction: %p52.512.0 = bf16[1024,2048]{1,0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1763 p48.440.0 @0>
 positions:
  p48.440.0
  p.25
 uses:
  call, operand 25
  wrapped_concatenate, operand 23
 from instruction: %p48.440.0 = bf16[1024,2048]{1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1764 p44.368.0 @0>
 positions:
  p44.368.0
  p.26
 uses:
  call, operand 26
  wrapped_concatenate, operand 24
 from instruction: %p44.368.0 = bf16[1024,2048]{1,0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1765 p40.296.0 @0>
 positions:
  p40.296.0
  p.27
 uses:
  call, operand 27
  wrapped_concatenate, operand 25
 from instruction: %p40.296.0 = bf16[1024,2048]{1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1766 p36.224.0 @0>
 positions:
  p36.224.0
  p.28
 uses:
  call, operand 28
  wrapped_concatenate, operand 26
 from instruction: %p36.224.0 = bf16[1024,2048]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1767 p32.152.0 @0>
 positions:
  p32.152.0
  p.29
 uses:
  call, operand 29
  wrapped_concatenate, operand 27
 from instruction: %p32.152.0 = bf16[1024,2048]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1768 p29.88.0 @0>
 positions:
  p29.88.0
  p.30
 uses:
  call, operand 30
  fusion.343, operand 0
  fusion.344, operand 0
  fusion.345, operand 0
  fusion.346, operand 0
  fusion.347, operand 0
  fusion.348, operand 1
  fusion.349, operand 0
  fusion.350, operand 0
  fusion.351, operand 0
  fusion.352, operand 1
  fusion.353, operand 0
  fusion.354, operand 0
  fusion.355, operand 0
  fusion.356, operand 1
  fusion.357, operand 0
  fusion.358, operand 0
  fusion.359, operand 0
  fusion.360, operand 1
  fusion.361, operand 0
  fusion.362, operand 0
  fusion.363, operand 0
  fusion.364, operand 1
  fusion.365, operand 0
  fusion.366, operand 0
  fusion.367, operand 0
  fusion.368, operand 1
  fusion.369, operand 0
  fusion.370, operand 0
  fusion.371, operand 0
 from instruction: %p29.88.0 = f32[] parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1769 p35.172.0 @0>
 positions:
  p35.172.0
  p.31
 uses:
  call, operand 31
  fusion.343, operand 3
 from instruction: %p35.172.0 = bf16[1024]{0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1770 p34.170.0 @0>
 positions:
  p34.170.0
  p.32
 uses:
  call, operand 32
  custom-call.56.0, operand 1
 from instruction: %p34.170.0 = bf16[6144,1024]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1771 p33.168.0 @0>
 positions:
  p33.168.0
  p.33
 uses:
  call, operand 33
  custom-call.57.0, operand 1
 from instruction: %p33.168.0 = bf16[1024,3072]{1,0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1772 p39.244.0 @0>
 positions:
  p39.244.0
  p.34
 uses:
  call, operand 34
  fusion.344, operand 1
 from instruction: %p39.244.0 = bf16[1024]{0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1773 p38.242.0 @0>
 positions:
  p38.242.0
  p.35
 uses:
  call, operand 35
  custom-call.58.0, operand 1
 from instruction: %p38.242.0 = bf16[6144,1024]{1,0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1774 p37.240.0 @0>
 positions:
  p37.240.0
  p.36
 uses:
  call, operand 36
  custom-call.59.0, operand 1
 from instruction: %p37.240.0 = bf16[1024,3072]{1,0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1775 p43.316.0 @0>
 positions:
  p43.316.0
  p.37
 uses:
  call, operand 37
  fusion.345, operand 1
 from instruction: %p43.316.0 = bf16[1024]{0} parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1776 p42.314.0 @0>
 positions:
  p42.314.0
  p.38
 uses:
  call, operand 38
  custom-call.60.0, operand 1
 from instruction: %p42.314.0 = bf16[6144,1024]{1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1777 p41.312.0 @0>
 positions:
  p41.312.0
  p.39
 uses:
  call, operand 39
  custom-call.61.0, operand 1
 from instruction: %p41.312.0 = bf16[1024,3072]{1,0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1778 p47.388.0 @0>
 positions:
  p47.388.0
  p.40
 uses:
  call, operand 40
  fusion.346, operand 1
 from instruction: %p47.388.0 = bf16[1024]{0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1779 p46.386.0 @0>
 positions:
  p46.386.0
  p.41
 uses:
  call, operand 41
  custom-call.62.0, operand 1
 from instruction: %p46.386.0 = bf16[6144,1024]{1,0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1780 p45.384.0 @0>
 positions:
  p45.384.0
  p.42
 uses:
  call, operand 42
  custom-call.63.0, operand 1
 from instruction: %p45.384.0 = bf16[1024,3072]{1,0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1781 p51.460.0 @0>
 positions:
  p51.460.0
  p.43
 uses:
  call, operand 43
  fusion.347, operand 1
 from instruction: %p51.460.0 = bf16[1024]{0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1782 p50.458.0 @0>
 positions:
  p50.458.0
  p.44
 uses:
  call, operand 44
  custom-call.64.0, operand 1
 from instruction: %p50.458.0 = bf16[6144,1024]{1,0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1783 p49.456.0 @0>
 positions:
  p49.456.0
  p.45
 uses:
  call, operand 45
  custom-call.65.0, operand 1
 from instruction: %p49.456.0 = bf16[1024,3072]{1,0} parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1784 p55.532.0 @0>
 positions:
  p55.532.0
  p.46
 uses:
  call, operand 46
  fusion.348, operand 2
 from instruction: %p55.532.0 = bf16[1024]{0} parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1785 p54.530.0 @0>
 positions:
  p54.530.0
  p.47
 uses:
  call, operand 47
  custom-call.66.0, operand 1
 from instruction: %p54.530.0 = bf16[6144,1024]{1,0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1786 p53.528.0 @0>
 positions:
  p53.528.0
  p.48
 uses:
  call, operand 48
  custom-call.67.0, operand 1
 from instruction: %p53.528.0 = bf16[1024,3072]{1,0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1787 p59.604.0 @0>
 positions:
  p59.604.0
  p.49
 uses:
  call, operand 49
  fusion.349, operand 1
 from instruction: %p59.604.0 = bf16[1024]{0} parameter(59), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1788 p58.602.0 @0>
 positions:
  p58.602.0
  p.50
 uses:
  call, operand 50
  custom-call.68.0, operand 1
 from instruction: %p58.602.0 = bf16[6144,1024]{1,0} parameter(58), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1789 p57.600.0 @0>
 positions:
  p57.600.0
  p.51
 uses:
  call, operand 51
  custom-call.69.0, operand 1
 from instruction: %p57.600.0 = bf16[1024,3072]{1,0} parameter(57), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1790 p63.676.0 @0>
 positions:
  p63.676.0
  p.52
 uses:
  call, operand 52
  fusion.350, operand 1
 from instruction: %p63.676.0 = bf16[1024]{0} parameter(63), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1791 p62.674.0 @0>
 positions:
  p62.674.0
  p.53
 uses:
  call, operand 53
  custom-call.70.0, operand 1
 from instruction: %p62.674.0 = bf16[6144,1024]{1,0} parameter(62), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1792 p61.672.0 @0>
 positions:
  p61.672.0
  p.54
 uses:
  call, operand 54
  custom-call.71.0, operand 1
 from instruction: %p61.672.0 = bf16[1024,3072]{1,0} parameter(61), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1793 p67.748.0 @0>
 positions:
  p67.748.0
  p.55
 uses:
  call, operand 55
  fusion.351, operand 1
 from instruction: %p67.748.0 = bf16[1024]{0} parameter(67), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1794 p66.746.0 @0>
 positions:
  p66.746.0
  p.56
 uses:
  call, operand 56
  custom-call.72.0, operand 1
 from instruction: %p66.746.0 = bf16[6144,1024]{1,0} parameter(66), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1795 p65.744.0 @0>
 positions:
  p65.744.0
  p.57
 uses:
  call, operand 57
  custom-call.73.0, operand 1
 from instruction: %p65.744.0 = bf16[1024,3072]{1,0} parameter(65), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1796 p71.820.0 @0>
 positions:
  p71.820.0
  p.58
 uses:
  call, operand 58
  fusion.352, operand 2
 from instruction: %p71.820.0 = bf16[1024]{0} parameter(71), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1797 p70.818.0 @0>
 positions:
  p70.818.0
  p.59
 uses:
  call, operand 59
  custom-call.74.0, operand 1
 from instruction: %p70.818.0 = bf16[6144,1024]{1,0} parameter(70), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1798 p69.816.0 @0>
 positions:
  p69.816.0
  p.60
 uses:
  call, operand 60
  custom-call.75.0, operand 1
 from instruction: %p69.816.0 = bf16[1024,3072]{1,0} parameter(69), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1799 p75.892.0 @0>
 positions:
  p75.892.0
  p.61
 uses:
  call, operand 61
  fusion.353, operand 1
 from instruction: %p75.892.0 = bf16[1024]{0} parameter(75), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1800 p74.890.0 @0>
 positions:
  p74.890.0
  p.62
 uses:
  call, operand 62
  custom-call.76.0, operand 1
 from instruction: %p74.890.0 = bf16[6144,1024]{1,0} parameter(74), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1801 p73.888.0 @0>
 positions:
  p73.888.0
  p.63
 uses:
  call, operand 63
  custom-call.77.0, operand 1
 from instruction: %p73.888.0 = bf16[1024,3072]{1,0} parameter(73), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1802 p79.964.0 @0>
 positions:
  p79.964.0
  p.64
 uses:
  call, operand 64
  fusion.354, operand 1
 from instruction: %p79.964.0 = bf16[1024]{0} parameter(79), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1803 p78.962.0 @0>
 positions:
  p78.962.0
  p.65
 uses:
  call, operand 65
  custom-call.78.0, operand 1
 from instruction: %p78.962.0 = bf16[6144,1024]{1,0} parameter(78), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1804 p77.960.0 @0>
 positions:
  p77.960.0
  p.66
 uses:
  call, operand 66
  custom-call.79.0, operand 1
 from instruction: %p77.960.0 = bf16[1024,3072]{1,0} parameter(77), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1805 p83.1036.0 @0>
 positions:
  p83.1036.0
  p.67
 uses:
  call, operand 67
  fusion.355, operand 1
 from instruction: %p83.1036.0 = bf16[1024]{0} parameter(83), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1806 p82.1034.0 @0>
 positions:
  p82.1034.0
  p.68
 uses:
  call, operand 68
  custom-call.80.0, operand 1
 from instruction: %p82.1034.0 = bf16[6144,1024]{1,0} parameter(82), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1807 p81.1032.0 @0>
 positions:
  p81.1032.0
  p.69
 uses:
  call, operand 69
  custom-call.81.0, operand 1
 from instruction: %p81.1032.0 = bf16[1024,3072]{1,0} parameter(81), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1808 p87.1108.0 @0>
 positions:
  p87.1108.0
  p.70
 uses:
  call, operand 70
  fusion.356, operand 2
 from instruction: %p87.1108.0 = bf16[1024]{0} parameter(87), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1809 p86.1106.0 @0>
 positions:
  p86.1106.0
  p.71
 uses:
  call, operand 71
  custom-call.82.0, operand 1
 from instruction: %p86.1106.0 = bf16[6144,1024]{1,0} parameter(86), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1810 p85.1104.0 @0>
 positions:
  p85.1104.0
  p.72
 uses:
  call, operand 72
  custom-call.83.0, operand 1
 from instruction: %p85.1104.0 = bf16[1024,3072]{1,0} parameter(85), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1811 p91.1180.0 @0>
 positions:
  p91.1180.0
  p.73
 uses:
  call, operand 73
  fusion.357, operand 1
 from instruction: %p91.1180.0 = bf16[1024]{0} parameter(91), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1812 p90.1178.0 @0>
 positions:
  p90.1178.0
  p.74
 uses:
  call, operand 74
  custom-call.84.0, operand 1
 from instruction: %p90.1178.0 = bf16[6144,1024]{1,0} parameter(90), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1813 p89.1176.0 @0>
 positions:
  p89.1176.0
  p.75
 uses:
  call, operand 75
  custom-call.85.0, operand 1
 from instruction: %p89.1176.0 = bf16[1024,3072]{1,0} parameter(89), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1814 p95.1252.0 @0>
 positions:
  p95.1252.0
  p.76
 uses:
  call, operand 76
  fusion.358, operand 1
 from instruction: %p95.1252.0 = bf16[1024]{0} parameter(95), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1815 p94.1250.0 @0>
 positions:
  p94.1250.0
  p.77
 uses:
  call, operand 77
  custom-call.86.0, operand 1
 from instruction: %p94.1250.0 = bf16[6144,1024]{1,0} parameter(94), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1816 p93.1248.0 @0>
 positions:
  p93.1248.0
  p.78
 uses:
  call, operand 78
  custom-call.87.0, operand 1
 from instruction: %p93.1248.0 = bf16[1024,3072]{1,0} parameter(93), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1817 p99.1324.0 @0>
 positions:
  p99.1324.0
  p.79
 uses:
  call, operand 79
  fusion.359, operand 1
 from instruction: %p99.1324.0 = bf16[1024]{0} parameter(99), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1818 p98.1322.0 @0>
 positions:
  p98.1322.0
  p.80
 uses:
  call, operand 80
  custom-call.88.0, operand 1
 from instruction: %p98.1322.0 = bf16[6144,1024]{1,0} parameter(98), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1819 p97.1320.0 @0>
 positions:
  p97.1320.0
  p.81
 uses:
  call, operand 81
  custom-call.89.0, operand 1
 from instruction: %p97.1320.0 = bf16[1024,3072]{1,0} parameter(97), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1820 p103.1396.0 @0>
 positions:
  p103.1396.0
  p.82
 uses:
  call, operand 82
  fusion.360, operand 2
 from instruction: %p103.1396.0 = bf16[1024]{0} parameter(103), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1821 p102.1394.0 @0>
 positions:
  p102.1394.0
  p.83
 uses:
  call, operand 83
  custom-call.90.0, operand 1
 from instruction: %p102.1394.0 = bf16[6144,1024]{1,0} parameter(102), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1822 p101.1392.0 @0>
 positions:
  p101.1392.0
  p.84
 uses:
  call, operand 84
  custom-call.91.0, operand 1
 from instruction: %p101.1392.0 = bf16[1024,3072]{1,0} parameter(101), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1823 p107.1468.0 @0>
 positions:
  p107.1468.0
  p.85
 uses:
  call, operand 85
  fusion.361, operand 1
 from instruction: %p107.1468.0 = bf16[1024]{0} parameter(107), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1824 p106.1466.0 @0>
 positions:
  p106.1466.0
  p.86
 uses:
  call, operand 86
  custom-call.92.0, operand 1
 from instruction: %p106.1466.0 = bf16[6144,1024]{1,0} parameter(106), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1825 p105.1464.0 @0>
 positions:
  p105.1464.0
  p.87
 uses:
  call, operand 87
  custom-call.93.0, operand 1
 from instruction: %p105.1464.0 = bf16[1024,3072]{1,0} parameter(105), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1826 p111.1540.0 @0>
 positions:
  p111.1540.0
  p.88
 uses:
  call, operand 88
  fusion.362, operand 1
 from instruction: %p111.1540.0 = bf16[1024]{0} parameter(111), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1827 p110.1538.0 @0>
 positions:
  p110.1538.0
  p.89
 uses:
  call, operand 89
  custom-call.94.0, operand 1
 from instruction: %p110.1538.0 = bf16[6144,1024]{1,0} parameter(110), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1828 p109.1536.0 @0>
 positions:
  p109.1536.0
  p.90
 uses:
  call, operand 90
  custom-call.95.0, operand 1
 from instruction: %p109.1536.0 = bf16[1024,3072]{1,0} parameter(109), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1829 p115.1612.0 @0>
 positions:
  p115.1612.0
  p.91
 uses:
  call, operand 91
  fusion.363, operand 1
 from instruction: %p115.1612.0 = bf16[1024]{0} parameter(115), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1830 p114.1610.0 @0>
 positions:
  p114.1610.0
  p.92
 uses:
  call, operand 92
  custom-call.96.0, operand 1
 from instruction: %p114.1610.0 = bf16[6144,1024]{1,0} parameter(114), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1831 p113.1608.0 @0>
 positions:
  p113.1608.0
  p.93
 uses:
  call, operand 93
  custom-call.97.0, operand 1
 from instruction: %p113.1608.0 = bf16[1024,3072]{1,0} parameter(113), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1832 p119.1684.0 @0>
 positions:
  p119.1684.0
  p.94
 uses:
  call, operand 94
  fusion.364, operand 2
 from instruction: %p119.1684.0 = bf16[1024]{0} parameter(119), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1833 p118.1682.0 @0>
 positions:
  p118.1682.0
  p.95
 uses:
  call, operand 95
  custom-call.98.0, operand 1
 from instruction: %p118.1682.0 = bf16[6144,1024]{1,0} parameter(118), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1834 p117.1680.0 @0>
 positions:
  p117.1680.0
  p.96
 uses:
  call, operand 96
  custom-call.99.0, operand 1
 from instruction: %p117.1680.0 = bf16[1024,3072]{1,0} parameter(117), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1835 p123.1756.0 @0>
 positions:
  p123.1756.0
  p.97
 uses:
  call, operand 97
  fusion.365, operand 1
 from instruction: %p123.1756.0 = bf16[1024]{0} parameter(123), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1836 p122.1754.0 @0>
 positions:
  p122.1754.0
  p.98
 uses:
  call, operand 98
  custom-call.100.0, operand 1
 from instruction: %p122.1754.0 = bf16[6144,1024]{1,0} parameter(122), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1837 p121.1752.0 @0>
 positions:
  p121.1752.0
  p.99
 uses:
  call, operand 99
  custom-call.101.0, operand 1
 from instruction: %p121.1752.0 = bf16[1024,3072]{1,0} parameter(121), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1838 p127.1828.0 @0>
 positions:
  p127.1828.0
  p.100
 uses:
  call, operand 100
  fusion.366, operand 1
 from instruction: %p127.1828.0 = bf16[1024]{0} parameter(127), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1839 p126.1826.0 @0>
 positions:
  p126.1826.0
  p.101
 uses:
  call, operand 101
  custom-call.102.0, operand 1
 from instruction: %p126.1826.0 = bf16[6144,1024]{1,0} parameter(126), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1840 p125.1824.0 @0>
 positions:
  p125.1824.0
  p.102
 uses:
  call, operand 102
  custom-call.103.0, operand 1
 from instruction: %p125.1824.0 = bf16[1024,3072]{1,0} parameter(125), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1841 p131.1900.0 @0>
 positions:
  p131.1900.0
  p.103
 uses:
  call, operand 103
  fusion.367, operand 1
 from instruction: %p131.1900.0 = bf16[1024]{0} parameter(131), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1842 p130.1898.0 @0>
 positions:
  p130.1898.0
  p.104
 uses:
  call, operand 104
  custom-call.104.0, operand 1
 from instruction: %p130.1898.0 = bf16[6144,1024]{1,0} parameter(130), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1843 p129.1896.0 @0>
 positions:
  p129.1896.0
  p.105
 uses:
  call, operand 105
  custom-call.105.0, operand 1
 from instruction: %p129.1896.0 = bf16[1024,3072]{1,0} parameter(129), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1844 p135.1972.0 @0>
 positions:
  p135.1972.0
  p.106
 uses:
  call, operand 106
  fusion.368, operand 2
 from instruction: %p135.1972.0 = bf16[1024]{0} parameter(135), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1845 p134.1970.0 @0>
 positions:
  p134.1970.0
  p.107
 uses:
  call, operand 107
  custom-call.106.0, operand 1
 from instruction: %p134.1970.0 = bf16[6144,1024]{1,0} parameter(134), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1846 p133.1968.0 @0>
 positions:
  p133.1968.0
  p.108
 uses:
  call, operand 108
  custom-call.107.0, operand 1
 from instruction: %p133.1968.0 = bf16[1024,3072]{1,0} parameter(133), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1847 p139.2044.0 @0>
 positions:
  p139.2044.0
  p.109
 uses:
  call, operand 109
  fusion.369, operand 1
 from instruction: %p139.2044.0 = bf16[1024]{0} parameter(139), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1848 p138.2042.0 @0>
 positions:
  p138.2042.0
  p.110
 uses:
  call, operand 110
  custom-call.108.0, operand 1
 from instruction: %p138.2042.0 = bf16[6144,1024]{1,0} parameter(138), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1849 p137.2040.0 @0>
 positions:
  p137.2040.0
  p.111
 uses:
  call, operand 111
  custom-call.109.0, operand 1
 from instruction: %p137.2040.0 = bf16[1024,3072]{1,0} parameter(137), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1850 p143.2116.0 @0>
 positions:
  p143.2116.0
  p.112
 uses:
  call, operand 112
  fusion.370, operand 1
 from instruction: %p143.2116.0 = bf16[1024]{0} parameter(143), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1851 p142.2114.0 @0>
 positions:
  p142.2114.0
  p.113
 uses:
  call, operand 113
  custom-call.110.0, operand 1
 from instruction: %p142.2114.0 = bf16[6144,1024]{1,0} parameter(142), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1852 p141.2112.0 @0>
 positions:
  p141.2112.0
  p.114
 uses:
  call, operand 114
  custom-call.111.0, operand 1
 from instruction: %p141.2112.0 = bf16[1024,3072]{1,0} parameter(141), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1853 p28.85.0 @0>
 positions:
  p28.85.0
  p.115
 uses:
  call, operand 115
  fusion.371, operand 2
 from instruction: %p28.85.0 = bf16[1024]{0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1854 p0.1.0 @0>
 positions:
  p0.1.0
  p.116
  tuple {0}
  get-tuple-element.86
 uses:
  call, operand 116
  copy.28, operand 0
 from instruction: %p0.1.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1855 p1.4.0 @0>
 positions:
  p1.4.0
  p.117
  tuple {1}
  get-tuple-element.87
 uses:
  call, operand 117
  copy.29, operand 0
 from instruction: %p1.4.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1856 p2.7.0 @0>
 positions:
  p2.7.0
  p.118
  tuple {2}
  get-tuple-element.88
 uses:
  call, operand 118
  copy.30, operand 0
 from instruction: %p2.7.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1857 p3.10.0 @0>
 positions:
  p3.10.0
  p.119
  tuple {3}
  get-tuple-element.89
 uses:
  call, operand 119
  copy.31, operand 0
 from instruction: %p3.10.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1858 p4.13.0 @0>
 positions:
  p4.13.0
  p.120
  tuple {4}
  get-tuple-element.90
 uses:
  call, operand 120
  copy.32, operand 0
 from instruction: %p4.13.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1859 p5.16.0 @0>
 positions:
  p5.16.0
  p.121
  tuple {5}
  get-tuple-element.91
 uses:
  call, operand 121
  copy.33, operand 0
 from instruction: %p5.16.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1860 p6.19.0 @0>
 positions:
  p6.19.0
  p.122
  tuple {6}
  get-tuple-element.92
 uses:
  call, operand 122
  copy.34, operand 0
 from instruction: %p6.19.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1861 p7.22.0 @0>
 positions:
  p7.22.0
  p.123
  tuple {7}
  get-tuple-element.93
 uses:
  call, operand 123
  copy.35, operand 0
 from instruction: %p7.22.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1862 p8.25.0 @0>
 positions:
  p8.25.0
  p.124
  tuple {8}
  get-tuple-element.94
 uses:
  call, operand 124
  copy.36, operand 0
 from instruction: %p8.25.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1863 p9.28.0 @0>
 positions:
  p9.28.0
  p.125
  tuple {9}
  get-tuple-element.95
 uses:
  call, operand 125
  copy.37, operand 0
 from instruction: %p9.28.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1864 p10.31.0 @0>
 positions:
  p10.31.0
  p.126
  tuple {10}
  get-tuple-element.96
 uses:
  call, operand 126
  copy.38, operand 0
 from instruction: %p10.31.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1865 p11.34.0 @0>
 positions:
  p11.34.0
  p.127
  tuple {11}
  get-tuple-element.97
 uses:
  call, operand 127
  copy.39, operand 0
 from instruction: %p11.34.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1866 p12.37.0 @0>
 positions:
  p12.37.0
  p.128
  tuple {12}
  get-tuple-element.98
 uses:
  call, operand 128
  copy.40, operand 0
 from instruction: %p12.37.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1867 p13.40.0 @0>
 positions:
  p13.40.0
  p.129
  tuple {13}
  get-tuple-element.99
 uses:
  call, operand 129
  copy.41, operand 0
 from instruction: %p13.40.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1868 p14.43.0 @0>
 positions:
  p14.43.0
  p.130
  tuple {14}
  get-tuple-element.100
 uses:
  call, operand 130
  copy.42, operand 0
 from instruction: %p14.43.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1869 p15.46.0 @0>
 positions:
  p15.46.0
  p.131
  tuple {15}
  get-tuple-element.101
 uses:
  call, operand 131
  copy.43, operand 0
 from instruction: %p15.46.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1870 p16.49.0 @0>
 positions:
  p16.49.0
  p.132
  tuple {16}
  get-tuple-element.102
 uses:
  call, operand 132
  copy.44, operand 0
 from instruction: %p16.49.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1871 p17.52.0 @0>
 positions:
  p17.52.0
  p.133
  tuple {17}
  get-tuple-element.103
 uses:
  call, operand 133
  copy.45, operand 0
 from instruction: %p17.52.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1872 p18.55.0 @0>
 positions:
  p18.55.0
  p.134
  tuple {18}
  get-tuple-element.104
 uses:
  call, operand 134
  copy.46, operand 0
 from instruction: %p18.55.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1873 p19.58.0 @0>
 positions:
  p19.58.0
  p.135
  tuple {19}
  get-tuple-element.105
 uses:
  call, operand 135
  copy.47, operand 0
 from instruction: %p19.58.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1874 p20.61.0 @0>
 positions:
  p20.61.0
  p.136
  tuple {20}
  get-tuple-element.106
 uses:
  call, operand 136
  copy.48, operand 0
 from instruction: %p20.61.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1875 p21.64.0 @0>
 positions:
  p21.64.0
  p.137
  tuple {21}
  get-tuple-element.107
 uses:
  call, operand 137
  copy.49, operand 0
 from instruction: %p21.64.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1876 p22.67.0 @0>
 positions:
  p22.67.0
  p.138
  tuple {22}
  get-tuple-element.108
 uses:
  call, operand 138
  copy.50, operand 0
 from instruction: %p22.67.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1877 p23.70.0 @0>
 positions:
  p23.70.0
  p.139
  tuple {23}
  get-tuple-element.109
 uses:
  call, operand 139
  copy.51, operand 0
 from instruction: %p23.70.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1878 p24.73.0 @0>
 positions:
  p24.73.0
  p.140
  tuple {24}
  get-tuple-element.110
 uses:
  call, operand 140
  copy.52, operand 0
 from instruction: %p24.73.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1879 p25.76.0 @0>
 positions:
  p25.76.0
  p.141
  tuple {25}
  get-tuple-element.111
 uses:
  call, operand 141
  copy.53, operand 0
 from instruction: %p25.76.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1880 p26.79.0 @0>
 positions:
  p26.79.0
  p.142
  tuple {26}
  get-tuple-element.112
 uses:
  call, operand 142
  copy.54, operand 0
 from instruction: %p26.79.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1881 p27.82.0 @0>
 positions:
  p27.82.0
  p.143
  tuple {27}
  get-tuple-element.113
 uses:
  call, operand 143
  copy.55, operand 0
 from instruction: %p27.82.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1882 tuple.1{} @0>
 positions:
  tuple.1 {}
 uses:
 from instruction: %tuple.1 = (bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0}, bf16[32,1024]{1,0}) tuple(bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.146, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.147, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.148, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.149, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.150, /*index=5*/bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.151, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.152, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.153, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.154, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.155, /*index=10*/bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.156, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.157, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.158, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.159, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.160, /*index=15*/bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.161, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.162, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.163, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.164, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.165, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.166, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.167, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.168, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.169, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.170, /*index=25*/bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.171, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.172, bf16[2,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.173, bf16[32,1024]{1,0} %get-tuple-element.145)


HloLiveRange (max 556):
  InstructionSequence:
    0:p29.88.0
    1:p143.2116.0
    2:p142.2114.0
    3:p141.2112.0
    4:p140.2096.0
    5:p139.2044.0
    6:p138.2042.0
    7:p137.2040.0
    8:p136.2024.0
    9:p135.1972.0
    10:p134.1970.0
    11:p133.1968.0
    12:p132.1952.0
    13:p131.1900.0
    14:p130.1898.0
    15:p129.1896.0
    16:p128.1880.0
    17:p127.1828.0
    18:p126.1826.0
    19:p125.1824.0
    20:p124.1808.0
    21:p123.1756.0
    22:p122.1754.0
    23:p121.1752.0
    24:p120.1736.0
    25:p119.1684.0
    26:p118.1682.0
    27:p117.1680.0
    28:p116.1664.0
    29:p115.1612.0
    30:p114.1610.0
    31:p113.1608.0
    32:p112.1592.0
    33:p111.1540.0
    34:p110.1538.0
    35:p109.1536.0
    36:p108.1520.0
    37:p107.1468.0
    38:p106.1466.0
    39:p105.1464.0
    40:p104.1448.0
    41:p103.1396.0
    42:p102.1394.0
    43:p101.1392.0
    44:p100.1376.0
    45:p99.1324.0
    46:p98.1322.0
    47:p97.1320.0
    48:p96.1304.0
    49:p95.1252.0
    50:p94.1250.0
    51:p93.1248.0
    52:p92.1232.0
    53:p91.1180.0
    54:p90.1178.0
    55:p89.1176.0
    56:p88.1160.0
    57:p87.1108.0
    58:p86.1106.0
    59:p85.1104.0
    60:p84.1088.0
    61:p83.1036.0
    62:p82.1034.0
    63:p81.1032.0
    64:p80.1016.0
    65:p79.964.0
    66:p78.962.0
    67:p77.960.0
    68:p76.944.0
    69:p75.892.0
    70:p74.890.0
    71:p73.888.0
    72:p72.872.0
    73:p71.820.0
    74:p70.818.0
    75:p69.816.0
    76:p68.800.0
    77:p67.748.0
    78:p66.746.0
    79:p65.744.0
    80:p64.728.0
    81:p63.676.0
    82:p62.674.0
    83:p61.672.0
    84:p60.656.0
    85:p59.604.0
    86:p58.602.0
    87:p57.600.0
    88:p56.584.0
    89:p55.532.0
    90:p54.530.0
    91:p53.528.0
    92:p52.512.0
    93:p51.460.0
    94:p50.458.0
    95:p49.456.0
    96:p48.440.0
    97:p47.388.0
    98:p46.386.0
    99:p45.384.0
    100:p44.368.0
    101:p43.316.0
    102:p42.314.0
    103:p41.312.0
    104:p40.296.0
    105:p39.244.0
    106:p38.242.0
    107:p37.240.0
    108:p36.224.0
    109:p35.172.0
    110:p34.170.0
    111:p33.168.0
    112:p32.152.0
    113:p31.148.0
    114:p30.146.0
    115:p28.85.0
    116:p27.82.0
    117:p26.79.0
    118:p25.76.0
    119:p24.73.0
    120:p23.70.0
    121:p22.67.0
    122:p21.64.0
    123:p20.61.0
    124:p19.58.0
    125:p18.55.0
    126:p17.52.0
    127:p16.49.0
    128:p15.46.0
    129:p14.43.0
    130:p13.40.0
    131:p12.37.0
    132:p11.34.0
    133:p10.31.0
    134:p9.28.0
    135:p8.25.0
    136:p7.22.0
    137:p6.19.0
    138:p5.16.0
    139:p4.13.0
    140:p3.10.0
    141:p2.7.0
    142:p1.4.0
    143:p0.1.0
    144:p
    145:p.1
    146:p.2
    147:p.3
    148:p.4
    149:p.5
    150:p.6
    151:p.7
    152:p.8
    153:p.9
    154:p.10
    155:p.11
    156:p.12
    157:p.13
    158:p.14
    159:p.15
    160:p.16
    161:p.17
    162:p.18
    163:p.19
    164:p.20
    165:p.21
    166:p.22
    167:p.23
    168:p.24
    169:p.25
    170:p.26
    171:p.27
    172:p.28
    173:p.29
    174:p.30
    175:p.31
    176:p.32
    177:p.33
    178:p.34
    179:p.35
    180:p.36
    181:p.37
    182:p.38
    183:p.39
    184:p.40
    185:p.41
    186:p.42
    187:p.43
    188:p.44
    189:p.45
    190:p.46
    191:p.47
    192:p.48
    193:p.49
    194:p.50
    195:p.51
    196:p.52
    197:p.53
    198:p.54
    199:p.55
    200:p.56
    201:p.57
    202:p.58
    203:p.59
    204:p.60
    205:p.61
    206:p.62
    207:p.63
    208:p.64
    209:p.65
    210:p.66
    211:p.67
    212:p.68
    213:p.69
    214:p.70
    215:p.71
    216:p.72
    217:p.73
    218:p.74
    219:p.75
    220:p.76
    221:p.77
    222:p.78
    223:p.79
    224:p.80
    225:p.81
    226:p.82
    227:p.83
    228:p.84
    229:p.85
    230:p.86
    231:p.87
    232:p.88
    233:p.89
    234:p.90
    235:p.91
    236:p.92
    237:p.93
    238:p.94
    239:p.95
    240:p.96
    241:p.97
    242:p.98
    243:p.99
    244:p.100
    245:p.101
    246:p.102
    247:p.103
    248:p.104
    249:p.105
    250:p.106
    251:p.107
    252:p.108
    253:p.109
    254:p.110
    255:p.111
    256:p.112
    257:p.113
    258:p.114
    259:p.115
    260:p.116
    261:p.117
    262:p.118
    263:p.119
    264:p.120
    265:p.121
    266:p.122
    267:p.123
    268:p.124
    269:p.125
    270:p.126
    271:p.127
    272:p.128
    273:p.129
    274:p.130
    275:p.131
    276:p.132
    277:p.133
    278:p.134
    279:p.135
    280:p.136
    281:p.137
    282:p.138
    283:p.139
    284:p.140
    285:p.141
    286:p.142
    287:p.143
    288:loop_gather_fusion
    289:wrapped_concatenate
    290:gemm_fusion_dot.110.0
    291:fusion.343
    292:custom-call.56.0
    293:get-tuple-element.56
    294:loop_convert_fusion
    295:custom-call.57.0
    296:get-tuple-element.1.0
    297:fusion.344
    298:custom-call.58.0
    299:get-tuple-element.2.0
    300:loop_convert_fusion.1
    301:custom-call.59.0
    302:get-tuple-element.3.0
    303:fusion.345
    304:custom-call.60.0
    305:get-tuple-element.4.0
    306:loop_convert_fusion.2
    307:custom-call.61.0
    308:get-tuple-element.5.0
    309:fusion.346
    310:custom-call.62.0
    311:get-tuple-element.6.0
    312:loop_convert_fusion.3
    313:custom-call.63.0
    314:get-tuple-element.7.0
    315:fusion.347
    316:custom-call.64.0
    317:get-tuple-element.8.0
    318:loop_convert_fusion.4
    319:custom-call.65.0
    320:get-tuple-element.9.0
    321:loop_add_fusion
    322:fusion.348
    323:custom-call.66.0
    324:get-tuple-element.10.0
    325:loop_convert_fusion.5
    326:custom-call.67.0
    327:get-tuple-element.11.0
    328:fusion.349
    329:custom-call.68.0
    330:get-tuple-element.12.0
    331:loop_convert_fusion.6
    332:custom-call.69.0
    333:get-tuple-element.13.0
    334:fusion.350
    335:custom-call.70.0
    336:get-tuple-element.14.0
    337:loop_convert_fusion.7
    338:custom-call.71.0
    339:get-tuple-element.15.0
    340:fusion.351
    341:custom-call.72.0
    342:get-tuple-element.16.0
    343:loop_convert_fusion.8
    344:custom-call.73.0
    345:get-tuple-element.17.0
    346:loop_add_fusion.1
    347:fusion.352
    348:custom-call.74.0
    349:get-tuple-element.18.0
    350:loop_convert_fusion.9
    351:custom-call.75.0
    352:get-tuple-element.19.0
    353:fusion.353
    354:custom-call.76.0
    355:get-tuple-element.20.0
    356:loop_convert_fusion.10
    357:custom-call.77.0
    358:get-tuple-element.21.0
    359:fusion.354
    360:custom-call.78.0
    361:get-tuple-element.22.0
    362:loop_convert_fusion.11
    363:custom-call.79.0
    364:get-tuple-element.23.0
    365:fusion.355
    366:custom-call.80.0
    367:get-tuple-element.24.0
    368:loop_convert_fusion.12
    369:custom-call.81.0
    370:get-tuple-element.25.0
    371:loop_add_fusion.2
    372:fusion.356
    373:custom-call.82.0
    374:get-tuple-element.26.0
    375:loop_convert_fusion.13
    376:custom-call.83.0
    377:get-tuple-element.27.0
    378:fusion.357
    379:custom-call.84.0
    380:get-tuple-element.28.0
    381:loop_convert_fusion.14
    382:custom-call.85.0
    383:get-tuple-element.29.0
    384:fusion.358
    385:custom-call.86.0
    386:get-tuple-element.30.0
    387:loop_convert_fusion.15
    388:custom-call.87.0
    389:get-tuple-element.31.0
    390:fusion.359
    391:custom-call.88.0
    392:get-tuple-element.32.0
    393:loop_convert_fusion.16
    394:custom-call.89.0
    395:get-tuple-element.33.0
    396:loop_add_fusion.3
    397:fusion.360
    398:custom-call.90.0
    399:get-tuple-element.34.0
    400:loop_convert_fusion.17
    401:custom-call.91.0
    402:get-tuple-element.35.0
    403:fusion.361
    404:custom-call.92.0
    405:get-tuple-element.36.0
    406:loop_convert_fusion.18
    407:custom-call.93.0
    408:get-tuple-element.37.0
    409:fusion.362
    410:custom-call.94.0
    411:get-tuple-element.38.0
    412:loop_convert_fusion.19
    413:custom-call.95.0
    414:get-tuple-element.39.0
    415:fusion.363
    416:custom-call.96.0
    417:get-tuple-element.40.0
    418:loop_convert_fusion.20
    419:custom-call.97.0
    420:get-tuple-element.41.0
    421:loop_add_fusion.4
    422:fusion.364
    423:custom-call.98.0
    424:get-tuple-element.42.0
    425:loop_convert_fusion.21
    426:custom-call.99.0
    427:get-tuple-element.43.0
    428:fusion.365
    429:custom-call.100.0
    430:get-tuple-element.44.0
    431:loop_convert_fusion.22
    432:custom-call.101.0
    433:get-tuple-element.45.0
    434:fusion.366
    435:custom-call.102.0
    436:get-tuple-element.46.0
    437:loop_convert_fusion.23
    438:custom-call.103.0
    439:get-tuple-element.47.0
    440:fusion.367
    441:custom-call.104.0
    442:get-tuple-element.48.0
    443:loop_convert_fusion.24
    444:custom-call.105.0
    445:get-tuple-element.49.0
    446:loop_add_fusion.5
    447:fusion.368
    448:custom-call.106.0
    449:get-tuple-element.50.0
    450:loop_convert_fusion.25
    451:custom-call.107.0
    452:get-tuple-element.51.0
    453:fusion.369
    454:custom-call.108.0
    455:get-tuple-element.52.0
    456:loop_convert_fusion.26
    457:custom-call.109.0
    458:get-tuple-element.53.0
    459:fusion.370
    460:custom-call.110.0
    461:get-tuple-element.54.0
    462:loop_convert_fusion.27
    463:custom-call.111.0
    464:get-tuple-element.55.0
    465:fusion.371
    466:tuple
    467:get-tuple-element.86
    468:get-tuple-element.87
    469:get-tuple-element.88
    470:get-tuple-element.89
    471:get-tuple-element.90
    472:get-tuple-element.91
    473:get-tuple-element.92
    474:get-tuple-element.93
    475:get-tuple-element.94
    476:get-tuple-element.95
    477:get-tuple-element.96
    478:get-tuple-element.97
    479:get-tuple-element.98
    480:get-tuple-element.99
    481:get-tuple-element.100
    482:get-tuple-element.101
    483:get-tuple-element.102
    484:get-tuple-element.103
    485:get-tuple-element.104
    486:get-tuple-element.105
    487:get-tuple-element.106
    488:get-tuple-element.107
    489:get-tuple-element.108
    490:get-tuple-element.109
    491:get-tuple-element.110
    492:get-tuple-element.111
    493:get-tuple-element.112
    494:get-tuple-element.113
    495:get-tuple-element.114
    496:copy.28
    497:copy.29
    498:copy.30
    499:copy.31
    500:copy.32
    501:copy.33
    502:copy.34
    503:copy.35
    504:copy.36
    505:copy.37
    506:copy.38
    507:copy.39
    508:copy.40
    509:copy.41
    510:copy.42
    511:copy.43
    512:copy.44
    513:copy.45
    514:copy.46
    515:copy.47
    516:copy.48
    517:copy.49
    518:copy.50
    519:copy.51
    520:copy.52
    521:copy.53
    522:copy.54
    523:copy.55
    524:tuple.2
    525:call
    526:get-tuple-element.145
    527:get-tuple-element.146
    528:get-tuple-element.147
    529:get-tuple-element.148
    530:get-tuple-element.149
    531:get-tuple-element.150
    532:get-tuple-element.151
    533:get-tuple-element.152
    534:get-tuple-element.153
    535:get-tuple-element.154
    536:get-tuple-element.155
    537:get-tuple-element.156
    538:get-tuple-element.157
    539:get-tuple-element.158
    540:get-tuple-element.159
    541:get-tuple-element.160
    542:get-tuple-element.161
    543:get-tuple-element.162
    544:get-tuple-element.163
    545:get-tuple-element.164
    546:get-tuple-element.165
    547:get-tuple-element.166
    548:get-tuple-element.167
    549:get-tuple-element.168
    550:get-tuple-element.169
    551:get-tuple-element.170
    552:get-tuple-element.171
    553:get-tuple-element.172
    554:get-tuple-element.173
    555:tuple.1
  BufferLiveRange:
    wrapped_concatenate{}:289-290
    gemm_fusion_dot.110.0{}:290-465
    loop_gather_fusion{}:288-321
    fusion.343{}:291-292
    custom-call.56.0{}:292-293
    custom-call.56.0{0}:292-294
    custom-call.56.0{1}:292-292
    loop_convert_fusion{}:294-295
    custom-call.57.0{}:295-296
    custom-call.57.0{0}:295-321
    custom-call.57.0{1}:295-295
    fusion.344{}:297-298
    custom-call.58.0{}:298-299
    custom-call.58.0{0}:298-300
    custom-call.58.0{1}:298-298
    loop_convert_fusion.1{}:300-301
    custom-call.59.0{}:301-302
    custom-call.59.0{0}:301-321
    custom-call.59.0{1}:301-301
    fusion.345{}:303-304
    custom-call.60.0{}:304-305
    custom-call.60.0{0}:304-306
    custom-call.60.0{1}:304-304
    loop_convert_fusion.2{}:306-307
    custom-call.61.0{}:307-308
    custom-call.61.0{0}:307-321
    custom-call.61.0{1}:307-307
    fusion.346{}:309-310
    custom-call.62.0{}:310-311
    custom-call.62.0{0}:310-312
    custom-call.62.0{1}:310-310
    loop_convert_fusion.3{}:312-313
    custom-call.63.0{}:313-314
    custom-call.63.0{0}:313-321
    custom-call.63.0{1}:313-313
    fusion.347{}:315-316
    custom-call.64.0{}:316-317
    custom-call.64.0{0}:316-318
    custom-call.64.0{1}:316-316
    loop_convert_fusion.4{}:318-319
    custom-call.65.0{}:319-320
    custom-call.65.0{0}:319-321
    custom-call.65.0{1}:319-319
    loop_add_fusion{}:321-346
    fusion.348{}:322-323
    custom-call.66.0{}:323-324
    custom-call.66.0{0}:323-325
    custom-call.66.0{1}:323-323
    loop_convert_fusion.5{}:325-326
    custom-call.67.0{}:326-327
    custom-call.67.0{0}:326-346
    custom-call.67.0{1}:326-326
    fusion.349{}:328-329
    custom-call.68.0{}:329-330
    custom-call.68.0{0}:329-331
    custom-call.68.0{1}:329-329
    loop_convert_fusion.6{}:331-332
    custom-call.69.0{}:332-333
    custom-call.69.0{0}:332-346
    custom-call.69.0{1}:332-332
    fusion.350{}:334-335
    custom-call.70.0{}:335-336
    custom-call.70.0{0}:335-337
    custom-call.70.0{1}:335-335
    loop_convert_fusion.7{}:337-338
    custom-call.71.0{}:338-339
    custom-call.71.0{0}:338-346
    custom-call.71.0{1}:338-338
    fusion.351{}:340-341
    custom-call.72.0{}:341-342
    custom-call.72.0{0}:341-343
    custom-call.72.0{1}:341-341
    loop_convert_fusion.8{}:343-344
    custom-call.73.0{}:344-345
    custom-call.73.0{0}:344-346
    custom-call.73.0{1}:344-344
    loop_add_fusion.1{}:346-371
    fusion.352{}:347-348
    custom-call.74.0{}:348-349
    custom-call.74.0{0}:348-350
    custom-call.74.0{1}:348-348
    loop_convert_fusion.9{}:350-351
    custom-call.75.0{}:351-352
    custom-call.75.0{0}:351-371
    custom-call.75.0{1}:351-351
    fusion.353{}:353-354
    custom-call.76.0{}:354-355
    custom-call.76.0{0}:354-356
    custom-call.76.0{1}:354-354
    loop_convert_fusion.10{}:356-357
    custom-call.77.0{}:357-358
    custom-call.77.0{0}:357-371
    custom-call.77.0{1}:357-357
    fusion.354{}:359-360
    custom-call.78.0{}:360-361
    custom-call.78.0{0}:360-362
    custom-call.78.0{1}:360-360
    loop_convert_fusion.11{}:362-363
    custom-call.79.0{}:363-364
    custom-call.79.0{0}:363-371
    custom-call.79.0{1}:363-363
    fusion.355{}:365-366
    custom-call.80.0{}:366-367
    custom-call.80.0{0}:366-368
    custom-call.80.0{1}:366-366
    loop_convert_fusion.12{}:368-369
    custom-call.81.0{}:369-370
    custom-call.81.0{0}:369-371
    custom-call.81.0{1}:369-369
    loop_add_fusion.2{}:371-396
    fusion.356{}:372-373
    custom-call.82.0{}:373-374
    custom-call.82.0{0}:373-375
    custom-call.82.0{1}:373-373
    loop_convert_fusion.13{}:375-376
    custom-call.83.0{}:376-377
    custom-call.83.0{0}:376-396
    custom-call.83.0{1}:376-376
    fusion.357{}:378-379
    custom-call.84.0{}:379-380
    custom-call.84.0{0}:379-381
    custom-call.84.0{1}:379-379
    loop_convert_fusion.14{}:381-382
    custom-call.85.0{}:382-383
    custom-call.85.0{0}:382-396
    custom-call.85.0{1}:382-382
    fusion.358{}:384-385
    custom-call.86.0{}:385-386
    custom-call.86.0{0}:385-387
    custom-call.86.0{1}:385-385
    loop_convert_fusion.15{}:387-388
    custom-call.87.0{}:388-389
    custom-call.87.0{0}:388-396
    custom-call.87.0{1}:388-388
    fusion.359{}:390-391
    custom-call.88.0{}:391-392
    custom-call.88.0{0}:391-393
    custom-call.88.0{1}:391-391
    loop_convert_fusion.16{}:393-394
    custom-call.89.0{}:394-395
    custom-call.89.0{0}:394-396
    custom-call.89.0{1}:394-394
    loop_add_fusion.3{}:396-421
    fusion.360{}:397-398
    custom-call.90.0{}:398-399
    custom-call.90.0{0}:398-400
    custom-call.90.0{1}:398-398
    loop_convert_fusion.17{}:400-401
    custom-call.91.0{}:401-402
    custom-call.91.0{0}:401-421
    custom-call.91.0{1}:401-401
    fusion.361{}:403-404
    custom-call.92.0{}:404-405
    custom-call.92.0{0}:404-406
    custom-call.92.0{1}:404-404
    loop_convert_fusion.18{}:406-407
    custom-call.93.0{}:407-408
    custom-call.93.0{0}:407-421
    custom-call.93.0{1}:407-407
    fusion.362{}:409-410
    custom-call.94.0{}:410-411
    custom-call.94.0{0}:410-412
    custom-call.94.0{1}:410-410
    loop_convert_fusion.19{}:412-413
    custom-call.95.0{}:413-414
    custom-call.95.0{0}:413-421
    custom-call.95.0{1}:413-413
    fusion.363{}:415-416
    custom-call.96.0{}:416-417
    custom-call.96.0{0}:416-418
    custom-call.96.0{1}:416-416
    loop_convert_fusion.20{}:418-419
    custom-call.97.0{}:419-420
    custom-call.97.0{0}:419-421
    custom-call.97.0{1}:419-419
    loop_add_fusion.4{}:421-446
    fusion.364{}:422-423
    custom-call.98.0{}:423-424
    custom-call.98.0{0}:423-425
    custom-call.98.0{1}:423-423
    loop_convert_fusion.21{}:425-426
    custom-call.99.0{}:426-427
    custom-call.99.0{0}:426-446
    custom-call.99.0{1}:426-426
    fusion.365{}:428-429
    custom-call.100.0{}:429-430
    custom-call.100.0{0}:429-431
    custom-call.100.0{1}:429-429
    loop_convert_fusion.22{}:431-432
    custom-call.101.0{}:432-433
    custom-call.101.0{0}:432-446
    custom-call.101.0{1}:432-432
    fusion.366{}:434-435
    custom-call.102.0{}:435-436
    custom-call.102.0{0}:435-437
    custom-call.102.0{1}:435-435
    loop_convert_fusion.23{}:437-438
    custom-call.103.0{}:438-439
    custom-call.103.0{0}:438-446
    custom-call.103.0{1}:438-438
    fusion.367{}:440-441
    custom-call.104.0{}:441-442
    custom-call.104.0{0}:441-443
    custom-call.104.0{1}:441-441
    loop_convert_fusion.24{}:443-444
    custom-call.105.0{}:444-445
    custom-call.105.0{0}:444-446
    custom-call.105.0{1}:444-444
    loop_add_fusion.5{}:446-465
    fusion.368{}:447-448
    custom-call.106.0{}:448-449
    custom-call.106.0{0}:448-450
    custom-call.106.0{1}:448-448
    loop_convert_fusion.25{}:450-451
    custom-call.107.0{}:451-452
    custom-call.107.0{0}:451-465
    custom-call.107.0{1}:451-451
    fusion.369{}:453-454
    custom-call.108.0{}:454-455
    custom-call.108.0{0}:454-456
    custom-call.108.0{1}:454-454
    loop_convert_fusion.26{}:456-457
    custom-call.109.0{}:457-458
    custom-call.109.0{0}:457-465
    custom-call.109.0{1}:457-457
    fusion.370{}:459-460
    custom-call.110.0{}:460-461
    custom-call.110.0{0}:460-462
    custom-call.110.0{1}:460-460
    loop_convert_fusion.27{}:462-463
    custom-call.111.0{}:463-464
    custom-call.111.0{0}:463-465
    custom-call.111.0{1}:463-463
    fusion.371{}:465-556
    tuple{}:466-495
    copy.28{}:496-556
    copy.29{}:497-556
    copy.30{}:498-556
    copy.31{}:499-556
    copy.32{}:500-556
    copy.33{}:501-556
    copy.34{}:502-556
    copy.35{}:503-556
    copy.36{}:504-556
    copy.37{}:505-556
    copy.38{}:506-556
    copy.39{}:507-556
    copy.40{}:508-556
    copy.41{}:509-556
    copy.42{}:510-556
    copy.43{}:511-556
    copy.44{}:512-556
    copy.45{}:513-556
    copy.46{}:514-556
    copy.47{}:515-556
    copy.48{}:516-556
    copy.49{}:517-556
    copy.50{}:518-556
    copy.51{}:519-556
    copy.52{}:520-556
    copy.53{}:521-556
    copy.54{}:522-556
    copy.55{}:523-556
    tuple.2{}:524-554
    p31.148.0{}:0-556
    p30.146.0{}:0-556
    p140.2096.0{}:0-556
    p136.2024.0{}:0-556
    p132.1952.0{}:0-556
    p128.1880.0{}:0-556
    p124.1808.0{}:0-556
    p120.1736.0{}:0-556
    p116.1664.0{}:0-556
    p112.1592.0{}:0-556
    p108.1520.0{}:0-556
    p104.1448.0{}:0-556
    p100.1376.0{}:0-556
    p96.1304.0{}:0-556
    p92.1232.0{}:0-556
    p88.1160.0{}:0-556
    p84.1088.0{}:0-556
    p80.1016.0{}:0-556
    p76.944.0{}:0-556
    p72.872.0{}:0-556
    p68.800.0{}:0-556
    p64.728.0{}:0-556
    p60.656.0{}:0-556
    p56.584.0{}:0-556
    p52.512.0{}:0-556
    p48.440.0{}:0-556
    p44.368.0{}:0-556
    p40.296.0{}:0-556
    p36.224.0{}:0-556
    p32.152.0{}:0-556
    p29.88.0{}:0-556
    p35.172.0{}:0-556
    p34.170.0{}:0-556
    p33.168.0{}:0-556
    p39.244.0{}:0-556
    p38.242.0{}:0-556
    p37.240.0{}:0-556
    p43.316.0{}:0-556
    p42.314.0{}:0-556
    p41.312.0{}:0-556
    p47.388.0{}:0-556
    p46.386.0{}:0-556
    p45.384.0{}:0-556
    p51.460.0{}:0-556
    p50.458.0{}:0-556
    p49.456.0{}:0-556
    p55.532.0{}:0-556
    p54.530.0{}:0-556
    p53.528.0{}:0-556
    p59.604.0{}:0-556
    p58.602.0{}:0-556
    p57.600.0{}:0-556
    p63.676.0{}:0-556
    p62.674.0{}:0-556
    p61.672.0{}:0-556
    p67.748.0{}:0-556
    p66.746.0{}:0-556
    p65.744.0{}:0-556
    p71.820.0{}:0-556
    p70.818.0{}:0-556
    p69.816.0{}:0-556
    p75.892.0{}:0-556
    p74.890.0{}:0-556
    p73.888.0{}:0-556
    p79.964.0{}:0-556
    p78.962.0{}:0-556
    p77.960.0{}:0-556
    p83.1036.0{}:0-556
    p82.1034.0{}:0-556
    p81.1032.0{}:0-556
    p87.1108.0{}:0-556
    p86.1106.0{}:0-556
    p85.1104.0{}:0-556
    p91.1180.0{}:0-556
    p90.1178.0{}:0-556
    p89.1176.0{}:0-556
    p95.1252.0{}:0-556
    p94.1250.0{}:0-556
    p93.1248.0{}:0-556
    p99.1324.0{}:0-556
    p98.1322.0{}:0-556
    p97.1320.0{}:0-556
    p103.1396.0{}:0-556
    p102.1394.0{}:0-556
    p101.1392.0{}:0-556
    p107.1468.0{}:0-556
    p106.1466.0{}:0-556
    p105.1464.0{}:0-556
    p111.1540.0{}:0-556
    p110.1538.0{}:0-556
    p109.1536.0{}:0-556
    p115.1612.0{}:0-556
    p114.1610.0{}:0-556
    p113.1608.0{}:0-556
    p119.1684.0{}:0-556
    p118.1682.0{}:0-556
    p117.1680.0{}:0-556
    p123.1756.0{}:0-556
    p122.1754.0{}:0-556
    p121.1752.0{}:0-556
    p127.1828.0{}:0-556
    p126.1826.0{}:0-556
    p125.1824.0{}:0-556
    p131.1900.0{}:0-556
    p130.1898.0{}:0-556
    p129.1896.0{}:0-556
    p135.1972.0{}:0-556
    p134.1970.0{}:0-556
    p133.1968.0{}:0-556
    p139.2044.0{}:0-556
    p138.2042.0{}:0-556
    p137.2040.0{}:0-556
    p143.2116.0{}:0-556
    p142.2114.0{}:0-556
    p141.2112.0{}:0-556
    p28.85.0{}:0-556
    p0.1.0{}:0-556
    p1.4.0{}:0-556
    p2.7.0{}:0-556
    p3.10.0{}:0-556
    p4.13.0{}:0-556
    p5.16.0{}:0-556
    p6.19.0{}:0-556
    p7.22.0{}:0-556
    p8.25.0{}:0-556
    p9.28.0{}:0-556
    p10.31.0{}:0-556
    p11.34.0{}:0-556
    p12.37.0{}:0-556
    p13.40.0{}:0-556
    p14.43.0{}:0-556
    p15.46.0{}:0-556
    p16.49.0{}:0-556
    p17.52.0{}:0-556
    p18.55.0{}:0-556
    p19.58.0{}:0-556
    p20.61.0{}:0-556
    p21.64.0{}:0-556
    p22.67.0{}:0-556
    p23.70.0{}:0-556
    p24.73.0{}:0-556
    p25.76.0{}:0-556
    p26.79.0{}:0-556
    p27.82.0{}:0-556
    tuple.1{}:555-556
  Live ranges at 555 (peak):
    fusion.371: 65536 bytes
    copy.28: 277413888 bytes
    copy.29: 277413888 bytes
    copy.30: 277413888 bytes
    copy.31: 277413888 bytes
    copy.32: 277413888 bytes
    copy.33: 277413888 bytes
    copy.34: 277413888 bytes
    copy.35: 277413888 bytes
    copy.36: 277413888 bytes
    copy.37: 277413888 bytes
    copy.38: 277413888 bytes
    copy.39: 277413888 bytes
    copy.40: 277413888 bytes
    copy.41: 277413888 bytes
    copy.42: 277413888 bytes
    copy.43: 277413888 bytes
    copy.44: 277413888 bytes
    copy.45: 277413888 bytes
    copy.46: 277413888 bytes
    copy.47: 277413888 bytes
    copy.48: 277413888 bytes
    copy.49: 277413888 bytes
    copy.50: 277413888 bytes
    copy.51: 277413888 bytes
    copy.52: 277413888 bytes
    copy.53: 277413888 bytes
    copy.54: 277413888 bytes
    copy.55: 277413888 bytes
    p31.148.0: 311164928 bytes
    p30.146.0: 128 bytes
    p140.2096.0: 4194304 bytes
    p136.2024.0: 4194304 bytes
    p132.1952.0: 4194304 bytes
    p128.1880.0: 4194304 bytes
    p124.1808.0: 4194304 bytes
    p120.1736.0: 4194304 bytes
    p116.1664.0: 4194304 bytes
    p112.1592.0: 4194304 bytes
    p108.1520.0: 4194304 bytes
    p104.1448.0: 4194304 bytes
    p100.1376.0: 4194304 bytes
    p96.1304.0: 4194304 bytes
    p92.1232.0: 4194304 bytes
    p88.1160.0: 4194304 bytes
    p84.1088.0: 4194304 bytes
    p80.1016.0: 4194304 bytes
    p76.944.0: 4194304 bytes
    p72.872.0: 4194304 bytes
    p68.800.0: 4194304 bytes
    p64.728.0: 4194304 bytes
    p60.656.0: 4194304 bytes
    p56.584.0: 4194304 bytes
    p52.512.0: 4194304 bytes
    p48.440.0: 4194304 bytes
    p44.368.0: 4194304 bytes
    p40.296.0: 4194304 bytes
    p36.224.0: 4194304 bytes
    p32.152.0: 4194304 bytes
    p29.88.0: 4 bytes
    p35.172.0: 2048 bytes
    p34.170.0: 12582912 bytes
    p33.168.0: 6291456 bytes
    p39.244.0: 2048 bytes
    p38.242.0: 12582912 bytes
    p37.240.0: 6291456 bytes
    p43.316.0: 2048 bytes
    p42.314.0: 12582912 bytes
    p41.312.0: 6291456 bytes
    p47.388.0: 2048 bytes
    p46.386.0: 12582912 bytes
    p45.384.0: 6291456 bytes
    p51.460.0: 2048 bytes
    p50.458.0: 12582912 bytes
    p49.456.0: 6291456 bytes
    p55.532.0: 2048 bytes
    p54.530.0: 12582912 bytes
    p53.528.0: 6291456 bytes
    p59.604.0: 2048 bytes
    p58.602.0: 12582912 bytes
    p57.600.0: 6291456 bytes
    p63.676.0: 2048 bytes
    p62.674.0: 12582912 bytes
    p61.672.0: 6291456 bytes
    p67.748.0: 2048 bytes
    p66.746.0: 12582912 bytes
    p65.744.0: 6291456 bytes
    p71.820.0: 2048 bytes
    p70.818.0: 12582912 bytes
    p69.816.0: 6291456 bytes
    p75.892.0: 2048 bytes
    p74.890.0: 12582912 bytes
    p73.888.0: 6291456 bytes
    p79.964.0: 2048 bytes
    p78.962.0: 12582912 bytes
    p77.960.0: 6291456 bytes
    p83.1036.0: 2048 bytes
    p82.1034.0: 12582912 bytes
    p81.1032.0: 6291456 bytes
    p87.1108.0: 2048 bytes
    p86.1106.0: 12582912 bytes
    p85.1104.0: 6291456 bytes
    p91.1180.0: 2048 bytes
    p90.1178.0: 12582912 bytes
    p89.1176.0: 6291456 bytes
    p95.1252.0: 2048 bytes
    p94.1250.0: 12582912 bytes
    p93.1248.0: 6291456 bytes
    p99.1324.0: 2048 bytes
    p98.1322.0: 12582912 bytes
    p97.1320.0: 6291456 bytes
    p103.1396.0: 2048 bytes
    p102.1394.0: 12582912 bytes
    p101.1392.0: 6291456 bytes
    p107.1468.0: 2048 bytes
    p106.1466.0: 12582912 bytes
    p105.1464.0: 6291456 bytes
    p111.1540.0: 2048 bytes
    p110.1538.0: 12582912 bytes
    p109.1536.0: 6291456 bytes
    p115.1612.0: 2048 bytes
    p114.1610.0: 12582912 bytes
    p113.1608.0: 6291456 bytes
    p119.1684.0: 2048 bytes
    p118.1682.0: 12582912 bytes
    p117.1680.0: 6291456 bytes
    p123.1756.0: 2048 bytes
    p122.1754.0: 12582912 bytes
    p121.1752.0: 6291456 bytes
    p127.1828.0: 2048 bytes
    p126.1826.0: 12582912 bytes
    p125.1824.0: 6291456 bytes
    p131.1900.0: 2048 bytes
    p130.1898.0: 12582912 bytes
    p129.1896.0: 6291456 bytes
    p135.1972.0: 2048 bytes
    p134.1970.0: 12582912 bytes
    p133.1968.0: 6291456 bytes
    p139.2044.0: 2048 bytes
    p138.2042.0: 12582912 bytes
    p137.2040.0: 6291456 bytes
    p143.2116.0: 2048 bytes
    p142.2114.0: 12582912 bytes
    p141.2112.0: 6291456 bytes
    p28.85.0: 2048 bytes
    p0.1.0: 277413888 bytes
    p1.4.0: 277413888 bytes
    p2.7.0: 277413888 bytes
    p3.10.0: 277413888 bytes
    p4.13.0: 277413888 bytes
    p5.16.0: 277413888 bytes
    p6.19.0: 277413888 bytes
    p7.22.0: 277413888 bytes
    p8.25.0: 277413888 bytes
    p9.28.0: 277413888 bytes
    p10.31.0: 277413888 bytes
    p11.34.0: 277413888 bytes
    p12.37.0: 277413888 bytes
    p13.40.0: 277413888 bytes
    p14.43.0: 277413888 bytes
    p15.46.0: 277413888 bytes
    p16.49.0: 277413888 bytes
    p17.52.0: 277413888 bytes
    p18.55.0: 277413888 bytes
    p19.58.0: 277413888 bytes
    p20.61.0: 277413888 bytes
    p21.64.0: 277413888 bytes
    p22.67.0: 277413888 bytes
    p23.70.0: 277413888 bytes
    p24.73.0: 277413888 bytes
    p25.76.0: 277413888 bytes
    p26.79.0: 277413888 bytes
    p27.82.0: 277413888 bytes
    tuple.1: 232 bytes
