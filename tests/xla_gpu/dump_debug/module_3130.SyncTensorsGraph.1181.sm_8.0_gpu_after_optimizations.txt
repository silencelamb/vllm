HloModule SyncTensorsGraph.1181, is_scheduled=true, entry_computation_layout={(bf16[128]{0}, f32[], bf16[4096,1024]{1,0}, bf16[1024]{0}, s32[64]{0}, /*index=5*/bf16[151936,1024]{1,0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=10*/bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, /*index=15*/bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, /*index=20*/bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, /*index=25*/bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=30*/bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, /*index=35*/bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, /*index=40*/bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, /*index=45*/bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=50*/bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, /*index=55*/bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, /*index=60*/bf16[6144,1024]{1,0}, bf16[1024]{0}, s32[64]{0}, bf16[40960,128]{1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0})->(bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})}, frontend_attributes={fingerprint_before_lhs="a92cda631a741d009e29103ade9193e5"}

%fused_gather (param_0.9: bf16[151936,1024], param_1.485: s32[64]) -> bf16[64,1,1024] {
  %param_0.9 = bf16[151936,1024]{1,0} parameter(0)
  %param_1.485 = s32[64]{0} parameter(1)
  %convert.702.1 = s64[64]{0} convert(s32[64]{0} %param_1.485), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.703.1 = u32[64]{0} convert(s64[64]{0} %convert.702.1), metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.2295.1 = u32[64,1]{1,0} bitcast(u32[64]{0} %convert.703.1)
  ROOT %gather.3 = bf16[64,1,1024]{2,0,1} gather(bf16[151936,1024]{1,0} %param_0.9, u32[64,1]{1,0} %bitcast.2295.1), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,1024}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_concatenate_computation (param_0.635: bf16[1024,2048], param_1.506: bf16[1024,2048], param_2.332: bf16[1024,2048], param_3.314: bf16[1024,2048], param_4.180: bf16[1024,2048], param_5.66: bf16[1024,2048], param_6.56: bf16[1024,2048], param_7.28: bf16[1024,2048], param_8.6: bf16[1024,2048], param_9: bf16[1024,2048], param_10: bf16[1024,2048], param_11: bf16[1024,2048], param_12: bf16[1024,2048], param_13: bf16[1024,2048]) -> bf16[14336,2048] {
  %param_0.635 = bf16[1024,2048]{1,0} parameter(0)
  %param_1.506 = bf16[1024,2048]{1,0} parameter(1)
  %param_2.332 = bf16[1024,2048]{1,0} parameter(2)
  %param_3.314 = bf16[1024,2048]{1,0} parameter(3)
  %param_4.180 = bf16[1024,2048]{1,0} parameter(4)
  %param_5.66 = bf16[1024,2048]{1,0} parameter(5)
  %param_6.56 = bf16[1024,2048]{1,0} parameter(6)
  %param_7.28 = bf16[1024,2048]{1,0} parameter(7)
  %param_8.6 = bf16[1024,2048]{1,0} parameter(8)
  %param_9 = bf16[1024,2048]{1,0} parameter(9)
  %param_10 = bf16[1024,2048]{1,0} parameter(10)
  %param_11 = bf16[1024,2048]{1,0} parameter(11)
  %param_12 = bf16[1024,2048]{1,0} parameter(12)
  %param_13 = bf16[1024,2048]{1,0} parameter(13)
  ROOT %concatenate.31.1 = bf16[14336,2048]{1,0} concatenate(bf16[1024,2048]{1,0} %param_0.635, bf16[1024,2048]{1,0} %param_1.506, bf16[1024,2048]{1,0} %param_2.332, bf16[1024,2048]{1,0} %param_3.314, bf16[1024,2048]{1,0} %param_4.180, /*index=5*/bf16[1024,2048]{1,0} %param_5.66, bf16[1024,2048]{1,0} %param_6.56, bf16[1024,2048]{1,0} %param_7.28, bf16[1024,2048]{1,0} %param_8.6, bf16[1024,2048]{1,0} %param_9, /*index=10*/bf16[1024,2048]{1,0} %param_10, bf16[1024,2048]{1,0} %param_11, bf16[1024,2048]{1,0} %param_12, bf16[1024,2048]{1,0} %param_13), dimensions={0}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
}

%gemm_fusion_dot.56_computation (parameter_0: bf16[14336,2048]) -> bf16[64,14336] {
  %constant_155 = bf16[] constant(0), metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.128 = bf16[64,2048]{1,0} broadcast(bf16[] %constant_155), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %parameter_0 = bf16[14336,2048]{1,0} parameter(0)
  ROOT %dot.57 = bf16[64,14336]{1,0} dot(bf16[64,2048]{1,0} %broadcast.128, bf16[14336,2048]{1,0} %parameter_0), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%AddComputation.74 (x.75: f32[], y.76: f32[]) -> f32[] {
  %y.76 = f32[] parameter(1)
  %x.75 = f32[] parameter(0)
  ROOT %add.190 = f32[] add(f32[] %x.75, f32[] %y.76)
}

%fused_computation.150 (param_0.607: f32[], param_1.482: bf16[64,1,1024], param_2.301: bf16[64,14336], param_3.270: bf16[1024]) -> bf16[64,1024] {
  %param_2.301 = bf16[64,14336]{1,0} parameter(2)
  %convert.701.48 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_2.301), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.200.9 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.48), slice={[0:64], [13312:14336]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.482 = bf16[64,1,1024]{2,0,1} parameter(1)
  %bitcast.2298.9 = bf16[64,1024]{1,0} bitcast(bf16[64,1,1024]{2,0,1} %param_1.482)
  %convert.705.9 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %bitcast.2298.9), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.191.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.200.9, f32[64,1024]{1,0} %convert.705.9), metadata={op_type="aten__add" op_name="aten__add.1027/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.394 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.191.7, f32[64,1024]{1,0} %add.191.7), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_344 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.33 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.394, f32[] %constant_344), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_343 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.452 = f32[64]{0} broadcast(f32[] %constant_343), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.393 = f32[64]{0} multiply(f32[64]{0} %reduce.33, f32[64]{0} %broadcast.452), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.607 = f32[] parameter(0)
  %broadcast.450 = f32[64]{0} broadcast(f32[] %param_0.607), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.238 = f32[64]{0} add(f32[64]{0} %multiply.393, f32[64]{0} %broadcast.450), metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.81 = f32[64]{0} rsqrt(f32[64]{0} %add.238), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.449 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.81), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1029/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.392 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.191.7, f32[64,1024]{1,0} %broadcast.449), metadata={op_type="aten__mul" op_name="aten__mul.1029/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.270 = bf16[1024]{0} parameter(3)
  %convert.706.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.270), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.330.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.706.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1030/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.336.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.392, f32[64,1024]{1,0} %broadcast.330.1), metadata={op_type="aten__mul" op_name="aten__mul.1030/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.707.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.336.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.13 (param_0.519: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.519 = bf16[64,6144]{1,0} parameter(0)
  %slice.201.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.519), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.710.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.201.1)
  %constant_1_4 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.2 = f32[] convert(bf16[] %constant_1_4)
  %broadcast.384.112 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.2), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.42.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.710.6)
  %convert.714.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.42.7)
  %exponential.42.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.714.5)
  %convert.716.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.42.3)
  %add.192.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.112, f32[64,3072]{1,0} %convert.716.1)
  %divide.42.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.112, f32[64,3072]{1,0} %add.192.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.337.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.710.6, f32[64,3072]{1,0} %divide.42.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.202.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.519), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.717.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.202.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.338.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.337.3, f32[64,3072]{1,0} %convert.717.1), metadata={op_type="aten__mul" op_name="aten__mul.1031/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.718.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.338.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.149 (param_0.623: f32[], param_1.491: bf16[1024], param_2.312: bf16[64,1,1024], param_3.288: bf16[64,14336], param_4.151: bf16[64,1024]) -> bf16[64,1024] {
  %param_3.288 = bf16[64,14336]{1,0} parameter(3)
  %convert.701.84 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_3.288), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.199.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.84), slice={[0:64], [12288:13312]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.151 = bf16[64,1024]{1,0} parameter(4)
  %convert.719.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_4.151), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.200.11 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.84), slice={[0:64], [13312:14336]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.312 = bf16[64,1,1024]{2,0,1} parameter(2)
  %bitcast.2298.11 = bf16[64,1024]{1,0} bitcast(bf16[64,1,1024]{2,0,1} %param_2.312)
  %convert.705.11 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %bitcast.2298.11), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.191.9 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.200.11, f32[64,1024]{1,0} %convert.705.11), metadata={op_type="aten__add" op_name="aten__add.1027/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.193.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.719.3, f32[64,1024]{1,0} %add.191.9), metadata={op_type="aten__add" op_name="aten__add.1032/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.194.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.199.5, f32[64,1024]{1,0} %add.193.3), metadata={op_type="aten__add" op_name="aten__add.1045/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.403 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.194.3, f32[64,1024]{1,0} %add.194.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_349 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.35 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.403, f32[] %constant_349), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_348 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.460 = f32[64]{0} broadcast(f32[] %constant_348), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.401 = f32[64]{0} multiply(f32[64]{0} %reduce.35, f32[64]{0} %broadcast.460), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.623 = f32[] parameter(0)
  %broadcast.458 = f32[64]{0} broadcast(f32[] %param_0.623), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.240 = f32[64]{0} add(f32[64]{0} %multiply.401, f32[64]{0} %broadcast.458), metadata={op_type="aten__add" op_name="aten__add.1046/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.83 = f32[64]{0} rsqrt(f32[64]{0} %add.240), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.457 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.83), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1047/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.400 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.194.3, f32[64,1024]{1,0} %broadcast.457), metadata={op_type="aten__mul" op_name="aten__mul.1047/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.491 = bf16[1024]{0} parameter(1)
  %convert.720.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.491), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.333.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.720.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1048/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.339.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.400, f32[64,1024]{1,0} %broadcast.333.1), metadata={op_type="aten__mul" op_name="aten__mul.1048/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.721.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.339.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.12 (param_0.521: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.521 = bf16[64,6144]{1,0} parameter(0)
  %slice.203.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.521), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.722.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.203.1)
  %constant_1_6 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.4 = f32[] convert(bf16[] %constant_1_6)
  %broadcast.384.108 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.4), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.43.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.722.6)
  %convert.726.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.43.7)
  %exponential.43.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.726.5)
  %convert.727.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.43.3)
  %add.195.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.108, f32[64,3072]{1,0} %convert.727.1)
  %divide.43.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.108, f32[64,3072]{1,0} %add.195.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.340.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.722.6, f32[64,3072]{1,0} %divide.43.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.204.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.521), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.728.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.204.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.341.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.340.3, f32[64,3072]{1,0} %convert.728.1), metadata={op_type="aten__mul" op_name="aten__mul.1049/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.729.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.341.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.148 (param_0.605: f32[], param_1.494: bf16[64,1024], param_2.316: bf16[64,14336], param_3.291: bf16[1024], param_4.157: bf16[64,1,1024], param_5.35: bf16[64,1024]) -> bf16[64,1024] {
  %param_2.316 = bf16[64,14336]{1,0} parameter(2)
  %convert.701.70 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_2.316), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.198.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.70), slice={[0:64], [11264:12288]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.494 = bf16[64,1024]{1,0} parameter(1)
  %convert.731.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_1.494), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.199.9 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.70), slice={[0:64], [12288:13312]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.35 = bf16[64,1024]{1,0} parameter(5)
  %convert.719.7 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_5.35), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.200.15 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.70), slice={[0:64], [13312:14336]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.157 = bf16[64,1,1024]{2,0,1} parameter(4)
  %bitcast.2298.15 = bf16[64,1024]{1,0} bitcast(bf16[64,1,1024]{2,0,1} %param_4.157)
  %convert.705.15 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %bitcast.2298.15), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.191.13 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.200.15, f32[64,1024]{1,0} %convert.705.15), metadata={op_type="aten__add" op_name="aten__add.1027/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.193.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.719.7, f32[64,1024]{1,0} %add.191.13), metadata={op_type="aten__add" op_name="aten__add.1032/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.194.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.199.9, f32[64,1024]{1,0} %add.193.7), metadata={op_type="aten__add" op_name="aten__add.1045/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.196.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.731.3, f32[64,1024]{1,0} %add.194.7), metadata={op_type="aten__add" op_name="aten__add.1050/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.197.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.198.5, f32[64,1024]{1,0} %add.196.3), metadata={op_type="aten__add" op_name="aten__add.1063/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.409 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.197.3, f32[64,1024]{1,0} %add.197.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_354 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.37 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.409, f32[] %constant_354), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_353 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.466 = f32[64]{0} broadcast(f32[] %constant_353), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.408 = f32[64]{0} multiply(f32[64]{0} %reduce.37, f32[64]{0} %broadcast.466), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.605 = f32[] parameter(0)
  %broadcast.465 = f32[64]{0} broadcast(f32[] %param_0.605), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.242 = f32[64]{0} add(f32[64]{0} %multiply.408, f32[64]{0} %broadcast.465), metadata={op_type="aten__add" op_name="aten__add.1064/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.85 = f32[64]{0} rsqrt(f32[64]{0} %add.242), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.464 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.85), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1065/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.407 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.197.3, f32[64,1024]{1,0} %broadcast.464), metadata={op_type="aten__mul" op_name="aten__mul.1065/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.291 = bf16[1024]{0} parameter(3)
  %convert.732.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.291), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.336.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.732.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1066/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.342.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.407, f32[64,1024]{1,0} %broadcast.336.1), metadata={op_type="aten__mul" op_name="aten__mul.1066/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.733.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.342.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.11 (param_0.523: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.523 = bf16[64,6144]{1,0} parameter(0)
  %slice.205.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.523), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.734.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.205.1)
  %constant_1_8 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.6 = f32[] convert(bf16[] %constant_1_8)
  %broadcast.384.104 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.6), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.44.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.734.6)
  %convert.739.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.44.7)
  %exponential.44.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.739.5)
  %convert.740.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.44.3)
  %add.198.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.104, f32[64,3072]{1,0} %convert.740.1)
  %divide.44.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.104, f32[64,3072]{1,0} %add.198.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.343.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.734.6, f32[64,3072]{1,0} %divide.44.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.206.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.523), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.741.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.206.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.344.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.343.3, f32[64,3072]{1,0} %convert.741.1), metadata={op_type="aten__mul" op_name="aten__mul.1067/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.742.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.344.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_add (param_0.504: bf16[64,14336], param_1.453: bf16[64,1024], param_2.314: bf16[64,1024], param_3.290: bf16[64,1,1024], param_4.155: bf16[64,1024]) -> f32[64,1024] {
  %param_0.504 = bf16[64,14336]{1,0} parameter(0)
  %convert.701.22 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_0.504), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.197.3 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.22), slice={[0:64], [10240:11264]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.453 = bf16[64,1024]{1,0} parameter(1)
  %convert.743.1 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_1.453), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.198.7 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.22), slice={[0:64], [11264:12288]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.314 = bf16[64,1024]{1,0} parameter(2)
  %convert.731.5 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_2.314), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.199.7 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.22), slice={[0:64], [12288:13312]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.155 = bf16[64,1024]{1,0} parameter(4)
  %convert.719.5 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_4.155), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.200.13 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.22), slice={[0:64], [13312:14336]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.290 = bf16[64,1,1024]{2,0,1} parameter(3)
  %bitcast.2298.13 = bf16[64,1024]{1,0} bitcast(bf16[64,1,1024]{2,0,1} %param_3.290)
  %convert.705.13 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %bitcast.2298.13), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.191.11 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.200.13, f32[64,1024]{1,0} %convert.705.13), metadata={op_type="aten__add" op_name="aten__add.1027/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.193.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.719.5, f32[64,1024]{1,0} %add.191.11), metadata={op_type="aten__add" op_name="aten__add.1032/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.194.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.199.7, f32[64,1024]{1,0} %add.193.5), metadata={op_type="aten__add" op_name="aten__add.1045/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.196.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.731.5, f32[64,1024]{1,0} %add.194.5), metadata={op_type="aten__add" op_name="aten__add.1050/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.197.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.198.7, f32[64,1024]{1,0} %add.196.5), metadata={op_type="aten__add" op_name="aten__add.1063/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.199.1 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.743.1, f32[64,1024]{1,0} %add.197.5), metadata={op_type="aten__add" op_name="aten__add.1068/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %add.200.1 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.197.3, f32[64,1024]{1,0} %add.199.1), metadata={op_type="aten__add" op_name="aten__add.1081/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.147 (param_0.604: f32[64,1024], param_1.479: f32[], param_2.298: bf16[1024]) -> bf16[64,1024] {
  %param_0.604 = f32[64,1024]{1,0} parameter(0)
  %multiply.415 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %param_0.604, f32[64,1024]{1,0} %param_0.604), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_363 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.39 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.415, f32[] %constant_363), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_362 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.472 = f32[64]{0} broadcast(f32[] %constant_362), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.414 = f32[64]{0} multiply(f32[64]{0} %reduce.39, f32[64]{0} %broadcast.472), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.479 = f32[] parameter(1)
  %broadcast.471 = f32[64]{0} broadcast(f32[] %param_1.479), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.244 = f32[64]{0} add(f32[64]{0} %multiply.414, f32[64]{0} %broadcast.471), metadata={op_type="aten__add" op_name="aten__add.1082/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.87 = f32[64]{0} rsqrt(f32[64]{0} %add.244), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.470 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.87), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1083/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.413 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %param_0.604, f32[64,1024]{1,0} %broadcast.470), metadata={op_type="aten__mul" op_name="aten__mul.1083/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.298 = bf16[1024]{0} parameter(2)
  %convert.744.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.298), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.341.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.744.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1084/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.345.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.413, f32[64,1024]{1,0} %broadcast.341.1), metadata={op_type="aten__mul" op_name="aten__mul.1084/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.747.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.345.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.10 (param_0.525: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.525 = bf16[64,6144]{1,0} parameter(0)
  %slice.207.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.525), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.748.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.207.1)
  %constant_1_10 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.8 = f32[] convert(bf16[] %constant_1_10)
  %broadcast.384.100 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.8), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.45.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.748.6)
  %convert.754.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.45.7)
  %exponential.45.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.754.5)
  %convert.755.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.45.3)
  %add.201.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.100, f32[64,3072]{1,0} %convert.755.1)
  %divide.45.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.100, f32[64,3072]{1,0} %add.201.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.347.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.748.6, f32[64,3072]{1,0} %divide.45.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.208.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.525), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.757.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.208.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.348.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.347.3, f32[64,3072]{1,0} %convert.757.1), metadata={op_type="aten__mul" op_name="aten__mul.1085/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.759.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.348.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.146 (param_0.603: f32[], param_1.478: f32[64,1024], param_2.297: bf16[64,1024], param_3.268: bf16[64,14336], param_4.136: bf16[1024]) -> bf16[64,1024] {
  %param_3.268 = bf16[64,14336]{1,0} parameter(3)
  %convert.701.66 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_3.268), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.196.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.66), slice={[0:64], [9216:10240]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.297 = bf16[64,1024]{1,0} parameter(2)
  %convert.761.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_2.297), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.478 = f32[64,1024]{1,0} parameter(1)
  %add.202.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.761.3, f32[64,1024]{1,0} %param_1.478), metadata={op_type="aten__add" op_name="aten__add.1086/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.204.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.196.5, f32[64,1024]{1,0} %add.202.3), metadata={op_type="aten__add" op_name="aten__add.1099/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.422 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.204.3, f32[64,1024]{1,0} %add.204.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_371 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.41 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.422, f32[] %constant_371), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_370 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.479 = f32[64]{0} broadcast(f32[] %constant_370), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.421 = f32[64]{0} multiply(f32[64]{0} %reduce.41, f32[64]{0} %broadcast.479), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.603 = f32[] parameter(0)
  %broadcast.478 = f32[64]{0} broadcast(f32[] %param_0.603), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.246 = f32[64]{0} add(f32[64]{0} %multiply.421, f32[64]{0} %broadcast.478), metadata={op_type="aten__add" op_name="aten__add.1100/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.89 = f32[64]{0} rsqrt(f32[64]{0} %add.246), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.477 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.89), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1101/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.420 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.204.3, f32[64,1024]{1,0} %broadcast.477), metadata={op_type="aten__mul" op_name="aten__mul.1101/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.136 = bf16[1024]{0} parameter(4)
  %convert.762.1 = f32[1024]{0} convert(bf16[1024]{0} %param_4.136), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.344.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.762.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1102/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.349.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.420, f32[64,1024]{1,0} %broadcast.344.1), metadata={op_type="aten__mul" op_name="aten__mul.1102/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.763.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.349.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.9 (param_0.527: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.527 = bf16[64,6144]{1,0} parameter(0)
  %slice.209.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.527), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.766.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.209.1)
  %constant_1_12 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.10 = f32[] convert(bf16[] %constant_1_12)
  %broadcast.384.96 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.10), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.46.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.766.6)
  %convert.770.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.46.7)
  %exponential.46.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.770.5)
  %convert.771.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.46.3)
  %add.205.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.96, f32[64,3072]{1,0} %convert.771.1)
  %divide.46.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.96, f32[64,3072]{1,0} %add.205.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.350.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.766.6, f32[64,3072]{1,0} %divide.46.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.210.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.527), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.772.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.210.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.351.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.350.3, f32[64,3072]{1,0} %convert.772.1), metadata={op_type="aten__mul" op_name="aten__mul.1103/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.773.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.351.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.145 (param_0.632: f32[], param_1.502: bf16[1024], param_2.327: f32[64,1024], param_3.308: bf16[64,1024], param_4.173: bf16[64,14336], param_5.56: bf16[64,1024]) -> bf16[64,1024] {
  %param_4.173 = bf16[64,14336]{1,0} parameter(4)
  %convert.701.108 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_4.173), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.195.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.108), slice={[0:64], [8192:9216]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.56 = bf16[64,1024]{1,0} parameter(5)
  %convert.774.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_5.56), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.196.9 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.108), slice={[0:64], [9216:10240]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.308 = bf16[64,1024]{1,0} parameter(3)
  %convert.761.7 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_3.308), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.327 = f32[64,1024]{1,0} parameter(2)
  %add.202.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.761.7, f32[64,1024]{1,0} %param_2.327), metadata={op_type="aten__add" op_name="aten__add.1086/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.204.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.196.9, f32[64,1024]{1,0} %add.202.7), metadata={op_type="aten__add" op_name="aten__add.1099/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.206.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.774.3, f32[64,1024]{1,0} %add.204.7), metadata={op_type="aten__add" op_name="aten__add.1104/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.207.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.195.5, f32[64,1024]{1,0} %add.206.3), metadata={op_type="aten__add" op_name="aten__add.1117/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.428 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.207.3, f32[64,1024]{1,0} %add.207.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_376 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.43 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.428, f32[] %constant_376), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_375 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.487 = f32[64]{0} broadcast(f32[] %constant_375), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.427 = f32[64]{0} multiply(f32[64]{0} %reduce.43, f32[64]{0} %broadcast.487), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.632 = f32[] parameter(0)
  %broadcast.486 = f32[64]{0} broadcast(f32[] %param_0.632), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.248 = f32[64]{0} add(f32[64]{0} %multiply.427, f32[64]{0} %broadcast.486), metadata={op_type="aten__add" op_name="aten__add.1118/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.91 = f32[64]{0} rsqrt(f32[64]{0} %add.248), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.485 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.91), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1119/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.426 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.207.3, f32[64,1024]{1,0} %broadcast.485), metadata={op_type="aten__mul" op_name="aten__mul.1119/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.502 = bf16[1024]{0} parameter(1)
  %convert.775.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.502), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.348.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.775.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1120/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.352.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.426, f32[64,1024]{1,0} %broadcast.348.1), metadata={op_type="aten__mul" op_name="aten__mul.1120/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.777.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.352.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.8 (param_0.529: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.529 = bf16[64,6144]{1,0} parameter(0)
  %slice.211.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.529), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.778.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.211.1)
  %constant_1_14 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.12 = f32[] convert(bf16[] %constant_1_14)
  %broadcast.384.92 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.12), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.47.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.778.6)
  %convert.784.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.47.7)
  %exponential.47.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.784.5)
  %convert.785.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.47.3)
  %add.208.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.92, f32[64,3072]{1,0} %convert.785.1)
  %divide.47.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.92, f32[64,3072]{1,0} %add.208.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.353.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.778.6, f32[64,3072]{1,0} %divide.47.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.212.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.529), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.786.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.212.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.354.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.353.3, f32[64,3072]{1,0} %convert.786.1), metadata={op_type="aten__mul" op_name="aten__mul.1121/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.788.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.354.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.144 (param_0.601: f32[], param_1.505: bf16[64,1024], param_2.331: bf16[64,14336], param_3.313: bf16[1024], param_4.179: f32[64,1024], param_5.65: bf16[64,1024], param_6.55: bf16[64,1024]) -> bf16[64,1024] {
  %param_2.331 = bf16[64,14336]{1,0} parameter(2)
  %convert.701.62 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_2.331), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.194.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.62), slice={[0:64], [7168:8192]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.505 = bf16[64,1024]{1,0} parameter(1)
  %convert.789.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_1.505), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.195.9 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.62), slice={[0:64], [8192:9216]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_6.55 = bf16[64,1024]{1,0} parameter(6)
  %convert.774.7 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_6.55), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.196.13 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.62), slice={[0:64], [9216:10240]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.65 = bf16[64,1024]{1,0} parameter(5)
  %convert.761.11 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_5.65), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.179 = f32[64,1024]{1,0} parameter(4)
  %add.202.11 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.761.11, f32[64,1024]{1,0} %param_4.179), metadata={op_type="aten__add" op_name="aten__add.1086/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.204.11 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.196.13, f32[64,1024]{1,0} %add.202.11), metadata={op_type="aten__add" op_name="aten__add.1099/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.206.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.774.7, f32[64,1024]{1,0} %add.204.11), metadata={op_type="aten__add" op_name="aten__add.1104/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.207.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.195.9, f32[64,1024]{1,0} %add.206.7), metadata={op_type="aten__add" op_name="aten__add.1117/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.209.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.789.3, f32[64,1024]{1,0} %add.207.7), metadata={op_type="aten__add" op_name="aten__add.1122/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.210.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.194.5, f32[64,1024]{1,0} %add.209.3), metadata={op_type="aten__add" op_name="aten__add.1135/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.434 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.210.3, f32[64,1024]{1,0} %add.210.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_381 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.45 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.434, f32[] %constant_381), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_380 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.494 = f32[64]{0} broadcast(f32[] %constant_380), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.433 = f32[64]{0} multiply(f32[64]{0} %reduce.45, f32[64]{0} %broadcast.494), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.601 = f32[] parameter(0)
  %broadcast.493 = f32[64]{0} broadcast(f32[] %param_0.601), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.250 = f32[64]{0} add(f32[64]{0} %multiply.433, f32[64]{0} %broadcast.493), metadata={op_type="aten__add" op_name="aten__add.1136/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.94 = f32[64]{0} rsqrt(f32[64]{0} %add.250), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.492 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.94), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1137/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.432 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.210.3, f32[64,1024]{1,0} %broadcast.492), metadata={op_type="aten__mul" op_name="aten__mul.1137/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.313 = bf16[1024]{0} parameter(3)
  %convert.790.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.313), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.351.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.790.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1138/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.355.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.432, f32[64,1024]{1,0} %broadcast.351.1), metadata={op_type="aten__mul" op_name="aten__mul.1138/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.791.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.355.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.7 (param_0.531: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.531 = bf16[64,6144]{1,0} parameter(0)
  %slice.213.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.531), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.792.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.213.1)
  %constant_1_1 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.14 = f32[] convert(bf16[] %constant_1_1)
  %broadcast.384.88 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.14), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.48.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.792.6)
  %convert.796.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.48.7)
  %exponential.48.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.796.5)
  %convert.797.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.48.3)
  %add.211.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.88, f32[64,3072]{1,0} %convert.797.1)
  %divide.48.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.88, f32[64,3072]{1,0} %add.211.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.356.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.792.6, f32[64,3072]{1,0} %divide.48.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.214.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.531), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.798.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.214.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.357.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.356.3, f32[64,3072]{1,0} %convert.798.1), metadata={op_type="aten__mul" op_name="aten__mul.1139/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.799.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.357.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_add.1 (param_0.492: bf16[64,14336], param_1.445: bf16[64,1024], param_2.329: bf16[64,1024], param_3.311: f32[64,1024], param_4.177: bf16[64,1024], param_5.61: bf16[64,1024]) -> f32[64,1024] {
  %param_0.492 = bf16[64,14336]{1,0} parameter(0)
  %convert.701.30 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_0.492), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.193.3 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.30), slice={[0:64], [6144:7168]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.445 = bf16[64,1024]{1,0} parameter(1)
  %convert.800.1 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_1.445), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.194.7 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.30), slice={[0:64], [7168:8192]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.329 = bf16[64,1024]{1,0} parameter(2)
  %convert.789.5 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_2.329), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.195.7 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.30), slice={[0:64], [8192:9216]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.61 = bf16[64,1024]{1,0} parameter(5)
  %convert.774.5 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_5.61), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.196.11 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.30), slice={[0:64], [9216:10240]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.177 = bf16[64,1024]{1,0} parameter(4)
  %convert.761.9 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_4.177), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.311 = f32[64,1024]{1,0} parameter(3)
  %add.202.9 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.761.9, f32[64,1024]{1,0} %param_3.311), metadata={op_type="aten__add" op_name="aten__add.1086/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.204.9 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.196.11, f32[64,1024]{1,0} %add.202.9), metadata={op_type="aten__add" op_name="aten__add.1099/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.206.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.774.5, f32[64,1024]{1,0} %add.204.9), metadata={op_type="aten__add" op_name="aten__add.1104/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.207.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.195.7, f32[64,1024]{1,0} %add.206.5), metadata={op_type="aten__add" op_name="aten__add.1117/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.209.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.789.5, f32[64,1024]{1,0} %add.207.5), metadata={op_type="aten__add" op_name="aten__add.1122/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.210.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.194.7, f32[64,1024]{1,0} %add.209.5), metadata={op_type="aten__add" op_name="aten__add.1135/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.212.1 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.800.1, f32[64,1024]{1,0} %add.210.5), metadata={op_type="aten__add" op_name="aten__add.1140/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %add.213.1 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.193.3, f32[64,1024]{1,0} %add.212.1), metadata={op_type="aten__add" op_name="aten__add.1153/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.143 (param_0.600: f32[64,1024], param_1.475: f32[], param_2.294: bf16[1024]) -> bf16[64,1024] {
  %param_0.600 = f32[64,1024]{1,0} parameter(0)
  %multiply.440 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %param_0.600, f32[64,1024]{1,0} %param_0.600), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_386 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.47 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.440, f32[] %constant_386), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_385 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.500 = f32[64]{0} broadcast(f32[] %constant_385), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.439 = f32[64]{0} multiply(f32[64]{0} %reduce.47, f32[64]{0} %broadcast.500), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.475 = f32[] parameter(1)
  %broadcast.499 = f32[64]{0} broadcast(f32[] %param_1.475), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.252 = f32[64]{0} add(f32[64]{0} %multiply.439, f32[64]{0} %broadcast.499), metadata={op_type="aten__add" op_name="aten__add.1154/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.96 = f32[64]{0} rsqrt(f32[64]{0} %add.252), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.498 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.96), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1155/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.438 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %param_0.600, f32[64,1024]{1,0} %broadcast.498), metadata={op_type="aten__mul" op_name="aten__mul.1155/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.294 = bf16[1024]{0} parameter(2)
  %convert.801.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.294), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.354.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.801.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1156/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.358.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.438, f32[64,1024]{1,0} %broadcast.354.1), metadata={op_type="aten__mul" op_name="aten__mul.1156/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.803.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.358.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.6 (param_0.522: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.522 = bf16[64,6144]{1,0} parameter(0)
  %slice.215.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.522), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.804.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.215.1)
  %constant_1_7 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.5 = f32[] convert(bf16[] %constant_1_7)
  %broadcast.384.84 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.5), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.49.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.804.6)
  %convert.808.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.49.7)
  %exponential.49.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.808.5)
  %convert.810.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.49.3)
  %add.214.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.84, f32[64,3072]{1,0} %convert.810.1)
  %divide.49.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.84, f32[64,3072]{1,0} %add.214.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.359.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.804.6, f32[64,3072]{1,0} %divide.49.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.216.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.522), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.811.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.216.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.360.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.359.3, f32[64,3072]{1,0} %convert.811.1), metadata={op_type="aten__mul" op_name="aten__mul.1157/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.812.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.360.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.142 (param_0.599: f32[], param_1.474: f32[64,1024], param_2.293: bf16[64,1024], param_3.266: bf16[64,14336], param_4.134: bf16[1024]) -> bf16[64,1024] {
  %param_3.266 = bf16[64,14336]{1,0} parameter(3)
  %convert.701.58 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_3.266), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.192.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.58), slice={[0:64], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.293 = bf16[64,1024]{1,0} parameter(2)
  %convert.813.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_2.293), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.474 = f32[64,1024]{1,0} parameter(1)
  %add.215.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.813.3, f32[64,1024]{1,0} %param_1.474), metadata={op_type="aten__add" op_name="aten__add.1158/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.216.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.192.5, f32[64,1024]{1,0} %add.215.3), metadata={op_type="aten__add" op_name="aten__add.1171/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.446 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.216.3, f32[64,1024]{1,0} %add.216.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_391 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.49 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.446, f32[] %constant_391), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_390 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.507 = f32[64]{0} broadcast(f32[] %constant_390), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.445 = f32[64]{0} multiply(f32[64]{0} %reduce.49, f32[64]{0} %broadcast.507), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.599 = f32[] parameter(0)
  %broadcast.506 = f32[64]{0} broadcast(f32[] %param_0.599), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.254 = f32[64]{0} add(f32[64]{0} %multiply.445, f32[64]{0} %broadcast.506), metadata={op_type="aten__add" op_name="aten__add.1172/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.98 = f32[64]{0} rsqrt(f32[64]{0} %add.254), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.505 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.98), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1173/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.444 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.216.3, f32[64,1024]{1,0} %broadcast.505), metadata={op_type="aten__mul" op_name="aten__mul.1173/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.134 = bf16[1024]{0} parameter(4)
  %convert.814.1 = f32[1024]{0} convert(bf16[1024]{0} %param_4.134), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.357.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.814.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1174/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.361.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.444, f32[64,1024]{1,0} %broadcast.357.1), metadata={op_type="aten__mul" op_name="aten__mul.1174/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.815.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.361.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.5 (param_0.526: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.526 = bf16[64,6144]{1,0} parameter(0)
  %slice.217.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.526), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.816.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.217.1)
  %constant_1_11 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.9 = f32[] convert(bf16[] %constant_1_11)
  %broadcast.384.80 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.9), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.50.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.816.6)
  %convert.823.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.50.7)
  %exponential.50.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.823.5)
  %convert.825.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.50.3)
  %add.217.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.80, f32[64,3072]{1,0} %convert.825.1)
  %divide.50.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.80, f32[64,3072]{1,0} %add.217.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.362.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.816.6, f32[64,3072]{1,0} %divide.50.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.218.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.526), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.826.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.218.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.363.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.362.3, f32[64,3072]{1,0} %convert.826.1), metadata={op_type="aten__mul" op_name="aten__mul.1175/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.827.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.363.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.141 (param_0.628: f32[], param_1.497: bf16[1024], param_2.320: f32[64,1024], param_3.298: bf16[64,1024], param_4.162: bf16[64,14336], param_5.41: bf16[64,1024]) -> bf16[64,1024] {
  %param_4.162 = bf16[64,14336]{1,0} parameter(4)
  %convert.701.96 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_4.162), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.191.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.96), slice={[0:64], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.41 = bf16[64,1024]{1,0} parameter(5)
  %convert.829.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_5.41), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.192.9 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.96), slice={[0:64], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.298 = bf16[64,1024]{1,0} parameter(3)
  %convert.813.7 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_3.298), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.320 = f32[64,1024]{1,0} parameter(2)
  %add.215.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.813.7, f32[64,1024]{1,0} %param_2.320), metadata={op_type="aten__add" op_name="aten__add.1158/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.216.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.192.9, f32[64,1024]{1,0} %add.215.7), metadata={op_type="aten__add" op_name="aten__add.1171/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.218.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.829.3, f32[64,1024]{1,0} %add.216.7), metadata={op_type="aten__add" op_name="aten__add.1176/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.219.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.191.5, f32[64,1024]{1,0} %add.218.3), metadata={op_type="aten__add" op_name="aten__add.1189/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.454 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.219.3, f32[64,1024]{1,0} %add.219.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_396 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.51 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.454, f32[] %constant_396), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_395 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.513 = f32[64]{0} broadcast(f32[] %constant_395), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.453 = f32[64]{0} multiply(f32[64]{0} %reduce.51, f32[64]{0} %broadcast.513), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.628 = f32[] parameter(0)
  %broadcast.512 = f32[64]{0} broadcast(f32[] %param_0.628), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.256 = f32[64]{0} add(f32[64]{0} %multiply.453, f32[64]{0} %broadcast.512), metadata={op_type="aten__add" op_name="aten__add.1190/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.100 = f32[64]{0} rsqrt(f32[64]{0} %add.256), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.511 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.100), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1191/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.452 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.219.3, f32[64,1024]{1,0} %broadcast.511), metadata={op_type="aten__mul" op_name="aten__mul.1191/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.497 = bf16[1024]{0} parameter(1)
  %convert.831.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.497), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.361.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.831.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1192/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.364.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.452, f32[64,1024]{1,0} %broadcast.361.1), metadata={op_type="aten__mul" op_name="aten__mul.1192/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.833.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.364.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.4 (param_0.530: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.530 = bf16[64,6144]{1,0} parameter(0)
  %slice.219.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.530), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.834.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.219.1)
  %constant_1_15 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.13 = f32[] convert(bf16[] %constant_1_15)
  %broadcast.384.76 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.13), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.51.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.834.6)
  %convert.840.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.51.7)
  %exponential.51.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.840.5)
  %convert.841.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.51.3)
  %add.220.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.76, f32[64,3072]{1,0} %convert.841.1)
  %divide.51.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.76, f32[64,3072]{1,0} %add.220.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.365.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.834.6, f32[64,3072]{1,0} %divide.51.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.220.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.530), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.842.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.220.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.366.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.365.3, f32[64,3072]{1,0} %convert.842.1), metadata={op_type="aten__mul" op_name="aten__mul.1193/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.843.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.366.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.140 (param_0.597: f32[], param_1.500: bf16[64,1024], param_2.324: bf16[64,14336], param_3.303: bf16[1024], param_4.168: f32[64,1024], param_5.50: bf16[64,1024], param_6.38: bf16[64,1024]) -> bf16[64,1024] {
  %param_2.324 = bf16[64,14336]{1,0} parameter(2)
  %convert.701.54 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_2.324), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.190.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.54), slice={[0:64], [3072:4096]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.500 = bf16[64,1024]{1,0} parameter(1)
  %convert.844.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_1.500), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.191.9 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.54), slice={[0:64], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_6.38 = bf16[64,1024]{1,0} parameter(6)
  %convert.829.7 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_6.38), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.192.13 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.54), slice={[0:64], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.50 = bf16[64,1024]{1,0} parameter(5)
  %convert.813.11 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_5.50), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.168 = f32[64,1024]{1,0} parameter(4)
  %add.215.11 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.813.11, f32[64,1024]{1,0} %param_4.168), metadata={op_type="aten__add" op_name="aten__add.1158/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.216.11 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.192.13, f32[64,1024]{1,0} %add.215.11), metadata={op_type="aten__add" op_name="aten__add.1171/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.218.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.829.7, f32[64,1024]{1,0} %add.216.11), metadata={op_type="aten__add" op_name="aten__add.1176/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.219.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.191.9, f32[64,1024]{1,0} %add.218.7), metadata={op_type="aten__add" op_name="aten__add.1189/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.222.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.844.3, f32[64,1024]{1,0} %add.219.7), metadata={op_type="aten__add" op_name="aten__add.1194/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.223.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.190.5, f32[64,1024]{1,0} %add.222.3), metadata={op_type="aten__add" op_name="aten__add.1207/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.462 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.223.3, f32[64,1024]{1,0} %add.223.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_401 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.53 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.462, f32[] %constant_401), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_400 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.520 = f32[64]{0} broadcast(f32[] %constant_400), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.461 = f32[64]{0} multiply(f32[64]{0} %reduce.53, f32[64]{0} %broadcast.520), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.597 = f32[] parameter(0)
  %broadcast.519 = f32[64]{0} broadcast(f32[] %param_0.597), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.258 = f32[64]{0} add(f32[64]{0} %multiply.461, f32[64]{0} %broadcast.519), metadata={op_type="aten__add" op_name="aten__add.1208/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.102 = f32[64]{0} rsqrt(f32[64]{0} %add.258), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.517 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.102), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1209/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.459 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.223.3, f32[64,1024]{1,0} %broadcast.517), metadata={op_type="aten__mul" op_name="aten__mul.1209/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.303 = bf16[1024]{0} parameter(3)
  %convert.845.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.303), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.364.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.845.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1210/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.367.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.459, f32[64,1024]{1,0} %broadcast.364.1), metadata={op_type="aten__mul" op_name="aten__mul.1210/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.846.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.367.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.3 (param_0.524: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.524 = bf16[64,6144]{1,0} parameter(0)
  %slice.221.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.524), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.847.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.221.1)
  %constant_1_9 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.7 = f32[] convert(bf16[] %constant_1_9)
  %broadcast.384.72 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.7), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.52.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.847.6)
  %convert.854.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.52.7)
  %exponential.52.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.854.5)
  %convert.855.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.52.3)
  %add.224.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.72, f32[64,3072]{1,0} %convert.855.1)
  %divide.52.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.72, f32[64,3072]{1,0} %add.224.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.368.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.847.6, f32[64,3072]{1,0} %divide.52.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.222.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.524), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.856.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.222.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.369.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.368.3, f32[64,3072]{1,0} %convert.856.1), metadata={op_type="aten__mul" op_name="aten__mul.1211/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.857.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.369.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_add.2 (param_0.480: bf16[64,14336], param_1.437: bf16[64,1024], param_2.322: bf16[64,1024], param_3.301: f32[64,1024], param_4.166: bf16[64,1024], param_5.46: bf16[64,1024]) -> f32[64,1024] {
  %param_0.480 = bf16[64,14336]{1,0} parameter(0)
  %convert.701.38 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_0.480), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.189.3 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.38), slice={[0:64], [2048:3072]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.437 = bf16[64,1024]{1,0} parameter(1)
  %convert.858.1 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_1.437), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.190.7 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.38), slice={[0:64], [3072:4096]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.322 = bf16[64,1024]{1,0} parameter(2)
  %convert.844.5 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_2.322), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.191.7 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.38), slice={[0:64], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.46 = bf16[64,1024]{1,0} parameter(5)
  %convert.829.5 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_5.46), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.192.11 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.38), slice={[0:64], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.166 = bf16[64,1024]{1,0} parameter(4)
  %convert.813.9 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_4.166), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.301 = f32[64,1024]{1,0} parameter(3)
  %add.215.9 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.813.9, f32[64,1024]{1,0} %param_3.301), metadata={op_type="aten__add" op_name="aten__add.1158/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.216.9 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.192.11, f32[64,1024]{1,0} %add.215.9), metadata={op_type="aten__add" op_name="aten__add.1171/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.218.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.829.5, f32[64,1024]{1,0} %add.216.9), metadata={op_type="aten__add" op_name="aten__add.1176/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.219.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.191.7, f32[64,1024]{1,0} %add.218.5), metadata={op_type="aten__add" op_name="aten__add.1189/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.222.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.844.5, f32[64,1024]{1,0} %add.219.5), metadata={op_type="aten__add" op_name="aten__add.1194/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.223.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.190.7, f32[64,1024]{1,0} %add.222.5), metadata={op_type="aten__add" op_name="aten__add.1207/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.225.1 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.858.1, f32[64,1024]{1,0} %add.223.5), metadata={op_type="aten__add" op_name="aten__add.1212/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %add.226.1 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.189.3, f32[64,1024]{1,0} %add.225.1), metadata={op_type="aten__add" op_name="aten__add.1225/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.139 (param_0.596: f32[64,1024], param_1.471: f32[], param_2.290: bf16[1024]) -> bf16[64,1024] {
  %param_0.596 = f32[64,1024]{1,0} parameter(0)
  %multiply.470 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %param_0.596, f32[64,1024]{1,0} %param_0.596), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_406 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.55 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.470, f32[] %constant_406), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_405 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.528 = f32[64]{0} broadcast(f32[] %constant_405), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.468 = f32[64]{0} multiply(f32[64]{0} %reduce.55, f32[64]{0} %broadcast.528), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.471 = f32[] parameter(1)
  %broadcast.526 = f32[64]{0} broadcast(f32[] %param_1.471), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.261 = f32[64]{0} add(f32[64]{0} %multiply.468, f32[64]{0} %broadcast.526), metadata={op_type="aten__add" op_name="aten__add.1226/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.104 = f32[64]{0} rsqrt(f32[64]{0} %add.261), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.525 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.104), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1227/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.466 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %param_0.596, f32[64,1024]{1,0} %broadcast.525), metadata={op_type="aten__mul" op_name="aten__mul.1227/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.290 = bf16[1024]{0} parameter(2)
  %convert.860.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.290), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.367.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.860.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1228/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.370.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.466, f32[64,1024]{1,0} %broadcast.367.1), metadata={op_type="aten__mul" op_name="aten__mul.1228/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.861.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.370.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.2 (param_0.520: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.520 = bf16[64,6144]{1,0} parameter(0)
  %slice.223.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.520), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.862.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.223.1)
  %constant_1_5 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.3 = f32[] convert(bf16[] %constant_1_5)
  %broadcast.384.68 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.3), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.53.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.862.6)
  %convert.866.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.53.7)
  %exponential.53.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.866.5)
  %convert.867.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.53.3)
  %add.227.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.68, f32[64,3072]{1,0} %convert.867.1)
  %divide.53.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.68, f32[64,3072]{1,0} %add.227.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.371.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.862.6, f32[64,3072]{1,0} %divide.53.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.224.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.520), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.868.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.224.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.372.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.371.3, f32[64,3072]{1,0} %convert.868.1), metadata={op_type="aten__mul" op_name="aten__mul.1229/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.869.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.372.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.138 (param_0.595: f32[], param_1.470: f32[64,1024], param_2.289: bf16[64,1024], param_3.264: bf16[64,14336], param_4.132: bf16[1024]) -> bf16[64,1024] {
  %param_3.264 = bf16[64,14336]{1,0} parameter(3)
  %convert.701.50 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_3.264), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.188.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.50), slice={[0:64], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.289 = bf16[64,1024]{1,0} parameter(2)
  %convert.870.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_2.289), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.470 = f32[64,1024]{1,0} parameter(1)
  %add.228.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.870.3, f32[64,1024]{1,0} %param_1.470), metadata={op_type="aten__add" op_name="aten__add.1230/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.229.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.188.5, f32[64,1024]{1,0} %add.228.3), metadata={op_type="aten__add" op_name="aten__add.1243/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.477 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.229.3, f32[64,1024]{1,0} %add.229.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_412 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.57 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.477, f32[] %constant_412), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_411 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.535 = f32[64]{0} broadcast(f32[] %constant_411), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.476 = f32[64]{0} multiply(f32[64]{0} %reduce.57, f32[64]{0} %broadcast.535), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.595 = f32[] parameter(0)
  %broadcast.534 = f32[64]{0} broadcast(f32[] %param_0.595), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.263 = f32[64]{0} add(f32[64]{0} %multiply.476, f32[64]{0} %broadcast.534), metadata={op_type="aten__add" op_name="aten__add.1244/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.106 = f32[64]{0} rsqrt(f32[64]{0} %add.263), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.533 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.106), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1245/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.475 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.229.3, f32[64,1024]{1,0} %broadcast.533), metadata={op_type="aten__mul" op_name="aten__mul.1245/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.132 = bf16[1024]{0} parameter(4)
  %convert.871.1 = f32[1024]{0} convert(bf16[1024]{0} %param_4.132), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.370.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.871.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1246/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.373.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.475, f32[64,1024]{1,0} %broadcast.370.1), metadata={op_type="aten__mul" op_name="aten__mul.1246/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.872.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.373.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.1 (param_0.528: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.528 = bf16[64,6144]{1,0} parameter(0)
  %slice.225.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.528), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.873.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.225.1)
  %constant_1_13 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.11 = f32[] convert(bf16[] %constant_1_13)
  %broadcast.384.64 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.11), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.54.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.873.6)
  %convert.878.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.54.7)
  %exponential.54.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.878.5)
  %convert.879.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.54.3)
  %add.230.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.64, f32[64,3072]{1,0} %convert.879.1)
  %divide.54.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.64, f32[64,3072]{1,0} %add.230.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.374.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.873.6, f32[64,3072]{1,0} %divide.54.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.226.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.528), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.880.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.226.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.376.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.374.3, f32[64,3072]{1,0} %convert.880.1), metadata={op_type="aten__mul" op_name="aten__mul.1247/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.882.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.376.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.137 (param_0.620: f32[], param_1.487: bf16[1024], param_2.307: f32[64,1024], param_3.281: bf16[64,1024], param_4.142: bf16[64,14336], param_5.14: bf16[64,1024]) -> bf16[64,1024] {
  %param_4.142 = bf16[64,14336]{1,0} parameter(4)
  %convert.701.76 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_4.142), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.187.5 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.76), slice={[0:64], [0:1024]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.14 = bf16[64,1024]{1,0} parameter(5)
  %convert.883.3 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_5.14), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.188.9 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.76), slice={[0:64], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.281 = bf16[64,1024]{1,0} parameter(3)
  %convert.870.7 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_3.281), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.307 = f32[64,1024]{1,0} parameter(2)
  %add.228.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.870.7, f32[64,1024]{1,0} %param_2.307), metadata={op_type="aten__add" op_name="aten__add.1230/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.229.7 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.188.9, f32[64,1024]{1,0} %add.228.7), metadata={op_type="aten__add" op_name="aten__add.1243/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.231.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.883.3, f32[64,1024]{1,0} %add.229.7), metadata={op_type="aten__add" op_name="aten__add.1248/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.232.3 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.187.5, f32[64,1024]{1,0} %add.231.3), metadata={op_type="aten__add" op_name="aten__add.1261/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.483 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.232.3, f32[64,1024]{1,0} %add.232.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_417 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.59 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.483, f32[] %constant_417), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_416 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.541 = f32[64]{0} broadcast(f32[] %constant_416), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.482 = f32[64]{0} multiply(f32[64]{0} %reduce.59, f32[64]{0} %broadcast.541), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.620 = f32[] parameter(0)
  %broadcast.540 = f32[64]{0} broadcast(f32[] %param_0.620), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.265 = f32[64]{0} add(f32[64]{0} %multiply.482, f32[64]{0} %broadcast.540), metadata={op_type="aten__add" op_name="aten__add.1262/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.108 = f32[64]{0} rsqrt(f32[64]{0} %add.265), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.539 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.108), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1263/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.481 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.232.3, f32[64,1024]{1,0} %broadcast.539), metadata={op_type="aten__mul" op_name="aten__mul.1263/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.487 = bf16[1024]{0} parameter(1)
  %convert.884.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.487), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.373.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.884.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1264/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.377.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.481, f32[64,1024]{1,0} %broadcast.373.1), metadata={op_type="aten__mul" op_name="aten__mul.1264/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.885.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.377.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert (param_0.518: bf16[64,6144]) -> bf16[64,3072] {
  %param_0.518 = bf16[64,6144]{1,0} parameter(0)
  %slice.227.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.518), slice={[0:64], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.886.6 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.227.1)
  %constant_1_3 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.911.1 = f32[] convert(bf16[] %constant_1_3)
  %broadcast.384.60 = f32[64,3072]{1,0} broadcast(f32[] %convert.911.1), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.55.7 = f32[64,3072]{1,0} negate(f32[64,3072]{1,0} %convert.886.6)
  %convert.892.5 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %negate.55.7)
  %exponential.55.3 = bf16[64,3072]{1,0} exponential(bf16[64,3072]{1,0} %convert.892.5)
  %convert.894.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %exponential.55.3)
  %add.233.5 = f32[64,3072]{1,0} add(f32[64,3072]{1,0} %broadcast.384.60, f32[64,3072]{1,0} %convert.894.1)
  %divide.55.3 = f32[64,3072]{1,0} divide(f32[64,3072]{1,0} %broadcast.384.60, f32[64,3072]{1,0} %add.233.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.379.3 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %convert.886.6, f32[64,3072]{1,0} %divide.55.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.228.1 = bf16[64,3072]{1,0} slice(bf16[64,6144]{1,0} %param_0.518), slice={[0:64], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.895.1 = f32[64,3072]{1,0} convert(bf16[64,3072]{1,0} %slice.228.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.380.1 = f32[64,3072]{1,0} multiply(f32[64,3072]{1,0} %multiply.379.3, f32[64,3072]{1,0} %convert.895.1), metadata={op_type="aten__mul" op_name="aten__mul.1265/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.897.1 = bf16[64,3072]{1,0} convert(f32[64,3072]{1,0} %multiply.380.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.135 (param_0.593: f32[], param_1.489: bf16[64,1024], param_2.309: bf16[1024], param_3.284: f32[64,1024], param_4.146: bf16[64,1024], param_5.19: bf16[64,14336], param_6.12: bf16[64,1024]) -> bf16[64,1024] {
  %param_1.489 = bf16[64,1024]{1,0} parameter(1)
  %convert.898.5 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_1.489), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.19 = bf16[64,14336]{1,0} parameter(5)
  %convert.701.80 = f32[64,14336]{1,0} convert(bf16[64,14336]{1,0} %param_5.19), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.187.7 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.80), slice={[0:64], [0:1024]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_6.12 = bf16[64,1024]{1,0} parameter(6)
  %convert.883.5 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_6.12), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.188.11 = f32[64,1024]{1,0} slice(f32[64,14336]{1,0} %convert.701.80), slice={[0:64], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.146 = bf16[64,1024]{1,0} parameter(4)
  %convert.870.9 = f32[64,1024]{1,0} convert(bf16[64,1024]{1,0} %param_4.146), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.284 = f32[64,1024]{1,0} parameter(3)
  %add.228.9 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.870.9, f32[64,1024]{1,0} %param_3.284), metadata={op_type="aten__add" op_name="aten__add.1230/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.229.9 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.188.11, f32[64,1024]{1,0} %add.228.9), metadata={op_type="aten__add" op_name="aten__add.1243/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.231.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.883.5, f32[64,1024]{1,0} %add.229.9), metadata={op_type="aten__add" op_name="aten__add.1248/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.232.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %slice.187.7, f32[64,1024]{1,0} %add.231.5), metadata={op_type="aten__add" op_name="aten__add.1261/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.234.5 = f32[64,1024]{1,0} add(f32[64,1024]{1,0} %convert.898.5, f32[64,1024]{1,0} %add.232.5), metadata={op_type="aten__add" op_name="aten__add.1266/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.489 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.234.5, f32[64,1024]{1,0} %add.234.5), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_422 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.61 = f32[64]{0} reduce(f32[64,1024]{1,0} %multiply.489, f32[] %constant_422), dimensions={1}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_421 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.548 = f32[64]{0} broadcast(f32[] %constant_421), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.488 = f32[64]{0} multiply(f32[64]{0} %reduce.61, f32[64]{0} %broadcast.548), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.593 = f32[] parameter(0)
  %broadcast.547 = f32[64]{0} broadcast(f32[] %param_0.593), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1028/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.267 = f32[64]{0} add(f32[64]{0} %multiply.488, f32[64]{0} %broadcast.547), metadata={op_type="aten__add" op_name="aten__add.1267/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.110 = f32[64]{0} rsqrt(f32[64]{0} %add.267), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.546 = f32[64,1024]{1,0} broadcast(f32[64]{0} %rsqrt.110), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1268/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.487 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %add.234.5, f32[64,1024]{1,0} %broadcast.546), metadata={op_type="aten__mul" op_name="aten__mul.1268/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.309 = bf16[1024]{0} parameter(2)
  %convert.899.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.309), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.377.1 = f32[64,1024]{1,0} broadcast(f32[1024]{0} %convert.899.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1269/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.381.1 = f32[64,1024]{1,0} multiply(f32[64,1024]{1,0} %multiply.487, f32[64,1024]{1,0} %broadcast.377.1), metadata={op_type="aten__mul" op_name="aten__mul.1269/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.901.1 = bf16[64,1024]{1,0} convert(f32[64,1024]{1,0} %multiply.381.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_slice_computation (param_0.636: bf16[64,4096]) -> bf16[64,1024] {
  %param_0.636 = bf16[64,4096]{1,0} parameter(0)
  ROOT %slice.234.1 = bf16[64,1024]{1,0} slice(bf16[64,4096]{1,0} %param_0.636), slice={[0:64], [3072:4096]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%triton_softmax_computation.16 (param_0.419: f32[], param_1.413: bf16[64,4096]) -> f32[64,8,128] {
  %param_1.413 = bf16[64,4096]{1,0} parameter(1)
  %slice.229.1 = bf16[64,1024]{1,0} slice(bf16[64,4096]{1,0} %param_1.413), slice={[0:64], [2048:3072]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.2945.3 = bf16[64,8,128]{2,1,0} bitcast(bf16[64,1024]{1,0} %slice.229.1)
  %convert.903.3 = f32[64,8,128]{2,1,0} convert(bf16[64,8,128]{2,1,0} %bitcast.2945.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.281 = f32[64,8,128]{2,1,0} multiply(f32[64,8,128]{2,1,0} %convert.903.3, f32[64,8,128]{2,1,0} %convert.903.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_188 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.16 = f32[64,8]{1,0} reduce(f32[64,8,128]{2,1,0} %multiply.281, f32[] %constant_188), dimensions={2}, to_apply=%AddComputation.74, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_189 = f32[] constant(0.0078125), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.276 = f32[64,8]{1,0} broadcast(f32[] %constant_189), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.282 = f32[64,8]{1,0} multiply(f32[64,8]{1,0} %reduce.16, f32[64,8]{1,0} %broadcast.276), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.419 = f32[] parameter(0)
  %broadcast.277 = f32[64,8]{1,0} broadcast(f32[] %param_0.419), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1270/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.173 = f32[64,8]{1,0} add(f32[64,8]{1,0} %multiply.282, f32[64,8]{1,0} %broadcast.277), metadata={op_type="aten__add" op_name="aten__add.1270/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.64 = f32[64,8]{1,0} rsqrt(f32[64,8]{1,0} %add.173), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.278 = f32[64,8,128]{2,1,0} broadcast(f32[64,8]{1,0} %rsqrt.64), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul.1271/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %multiply.283 = f32[64,8,128]{2,1,0} multiply(f32[64,8,128]{2,1,0} %convert.903.3, f32[64,8,128]{2,1,0} %broadcast.278), metadata={op_type="aten__mul" op_name="aten__mul.1271/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_concatenate (param_0.618: f32[64,8,128], param_1.483: bf16[128], param_2.317: bf16[40960,128], param_3.293: s32[64]) -> bf16[64,8,128] {
  %param_0.618 = f32[64,8,128]{2,1,0} parameter(0)
  %param_1.483 = bf16[128]{0} parameter(1)
  %convert.905.1 = f32[128]{0} convert(bf16[128]{0} %param_1.483), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.378.18 = f32[64,8,128]{2,1,0} broadcast(f32[128]{0} %convert.905.1), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul.1272/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.382.18 = f32[64,8,128]{2,1,0} multiply(f32[64,8,128]{2,1,0} %param_0.618, f32[64,8,128]{2,1,0} %broadcast.378.18), metadata={op_type="aten__mul" op_name="aten__mul.1272/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.230.9 = f32[64,8,64]{2,1,0} slice(f32[64,8,128]{2,1,0} %multiply.382.18), slice={[0:64], [0:8], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.317 = bf16[40960,128]{1,0} parameter(2)
  %param_3.293 = s32[64]{0} parameter(3)
  %bitcast.2961.3 = s32[64,1]{1,0} bitcast(s32[64]{0} %param_3.293)
  %gather.1.3 = bf16[64,1,128]{2,0,1} gather(bf16[40960,128]{1,0} %param_2.317, s32[64,1]{1,0} %bitcast.2961.3), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,128}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.2964.7 = bf16[64,128]{1,0} bitcast(bf16[64,1,128]{2,0,1} %gather.1.3)
  %convert.906.7 = f32[64,128]{1,0} convert(bf16[64,128]{1,0} %bitcast.2964.7), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.231.3 = f32[64,64]{1,0} slice(f32[64,128]{1,0} %convert.906.7), slice={[0:64], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.380.14 = f32[64,8,64]{2,1,0} broadcast(f32[64,64]{1,0} %slice.231.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.1273/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.383.7 = f32[64,8,64]{2,1,0} multiply(f32[64,8,64]{2,1,0} %slice.230.9, f32[64,8,64]{2,1,0} %broadcast.380.14), metadata={op_type="aten__mul" op_name="aten__mul.1273/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.232.9 = f32[64,8,64]{2,1,0} slice(f32[64,8,128]{2,1,0} %multiply.382.18), slice={[0:64], [0:8], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.233.3 = f32[64,64]{1,0} slice(f32[64,128]{1,0} %convert.906.7), slice={[0:64], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.381.10 = f32[64,8,64]{2,1,0} broadcast(f32[64,64]{1,0} %slice.233.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.1274/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.385.5 = f32[64,8,64]{2,1,0} multiply(f32[64,8,64]{2,1,0} %slice.232.9, f32[64,8,64]{2,1,0} %broadcast.381.10), metadata={op_type="aten__mul" op_name="aten__mul.1274/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %subtract.2.5 = f32[64,8,64]{2,1,0} subtract(f32[64,8,64]{2,1,0} %multiply.383.7, f32[64,8,64]{2,1,0} %multiply.385.5), metadata={op_type="aten__sub" op_name="aten__sub.1275/aten__sub" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.907.3 = bf16[64,8,64]{2,1,0} convert(f32[64,8,64]{2,1,0} %subtract.2.5)
  %multiply.386.7 = f32[64,8,64]{2,1,0} multiply(f32[64,8,64]{2,1,0} %slice.232.9, f32[64,8,64]{2,1,0} %broadcast.380.14), metadata={op_type="aten__mul" op_name="aten__mul.1276/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.387.5 = f32[64,8,64]{2,1,0} multiply(f32[64,8,64]{2,1,0} %slice.230.9, f32[64,8,64]{2,1,0} %broadcast.381.10), metadata={op_type="aten__mul" op_name="aten__mul.1277/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.235.5 = f32[64,8,64]{2,1,0} add(f32[64,8,64]{2,1,0} %multiply.386.7, f32[64,8,64]{2,1,0} %multiply.387.5), metadata={op_type="aten__add" op_name="aten__add.1278/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.910.3 = bf16[64,8,64]{2,1,0} convert(f32[64,8,64]{2,1,0} %add.235.5)
  ROOT %concatenate.32.1 = bf16[64,8,128]{2,1,0} concatenate(bf16[64,8,64]{2,1,0} %convert.907.3, bf16[64,8,64]{2,1,0} %convert.910.3), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_slice (param_0.1: bf16[2,4233,16,8,128]) -> bf16[69353472] {
  %param_0.1 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  %bitcast.3005.1 = bf16[138706944]{0} bitcast(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.1)
  ROOT %slice.236.1 = bf16[69353472]{0} slice(bf16[138706944]{0} %bitcast.3005.1), slice={[69353472:138706944]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%wrapped_slice_computation.1 (param_0.637: bf16[2,4233,16,8,128]) -> bf16[1,4233,16,8,128] {
  %param_0.637 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  ROOT %slice.235.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} slice(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.637), slice={[0:1], [0:4233], [0:16], [0:8], [0:128]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%command_buffer (p: bf16[151936,1024], p.1: s32[64], p.2: bf16[1024,2048], p.3: bf16[1024,2048], p.4: bf16[1024,2048], p.5: bf16[1024,2048], p.6: bf16[1024,2048], p.7: bf16[1024,2048], p.8: bf16[1024,2048], p.9: bf16[1024,2048], p.10: bf16[1024,2048], p.11: bf16[1024,2048], p.12: bf16[1024,2048], p.13: bf16[1024,2048], p.14: bf16[1024,2048], p.15: bf16[1024,2048], p.16: f32[], p.17: bf16[1024], p.18: bf16[6144,1024], p.19: bf16[1024,3072], p.20: bf16[1024], p.21: bf16[6144,1024], p.22: bf16[1024,3072], p.23: bf16[1024], p.24: bf16[6144,1024], p.25: bf16[1024,3072], p.26: bf16[1024], p.27: bf16[6144,1024], p.28: bf16[1024,3072], p.29: bf16[1024], p.30: bf16[6144,1024], p.31: bf16[1024,3072], p.32: bf16[1024], p.33: bf16[6144,1024], p.34: bf16[1024,3072], p.35: bf16[1024], p.36: bf16[6144,1024], p.37: bf16[1024,3072], p.38: bf16[1024], p.39: bf16[6144,1024], p.40: bf16[1024,3072], p.41: bf16[1024], p.42: bf16[6144,1024], p.43: bf16[1024,3072], p.44: bf16[1024], p.45: bf16[6144,1024], p.46: bf16[1024,3072], p.47: bf16[1024], p.48: bf16[6144,1024], p.49: bf16[1024,3072], p.50: bf16[1024], p.51: bf16[6144,1024], p.52: bf16[1024,3072], p.53: bf16[1024], p.54: bf16[6144,1024], p.55: bf16[1024,3072], p.56: bf16[1024], p.57: bf16[6144,1024], p.58: bf16[1024,3072], p.59: bf16[1024], p.60: bf16[4096,1024], p.61: bf16[128], p.62: bf16[40960,128], p.63: s32[64], p.64: bf16[2,4233,16,8,128]) -> (bf16[64,8,128], bf16[64,8,128], bf16[4233,16,8,128], bf16[1,4233,16,8,128]) {
  %p = bf16[151936,1024]{1,0} parameter(0)
  %p.1 = s32[64]{0} parameter(1)
  %p.2 = bf16[1024,2048]{1,0} parameter(2)
  %p.3 = bf16[1024,2048]{1,0} parameter(3)
  %p.4 = bf16[1024,2048]{1,0} parameter(4)
  %p.5 = bf16[1024,2048]{1,0} parameter(5)
  %p.6 = bf16[1024,2048]{1,0} parameter(6)
  %p.7 = bf16[1024,2048]{1,0} parameter(7)
  %p.8 = bf16[1024,2048]{1,0} parameter(8)
  %p.9 = bf16[1024,2048]{1,0} parameter(9)
  %p.10 = bf16[1024,2048]{1,0} parameter(10)
  %p.11 = bf16[1024,2048]{1,0} parameter(11)
  %p.12 = bf16[1024,2048]{1,0} parameter(12)
  %p.13 = bf16[1024,2048]{1,0} parameter(13)
  %p.14 = bf16[1024,2048]{1,0} parameter(14)
  %p.15 = bf16[1024,2048]{1,0} parameter(15)
  %p.16 = f32[] parameter(16)
  %p.17 = bf16[1024]{0} parameter(17)
  %p.18 = bf16[6144,1024]{1,0} parameter(18)
  %p.19 = bf16[1024,3072]{1,0} parameter(19)
  %p.20 = bf16[1024]{0} parameter(20)
  %p.21 = bf16[6144,1024]{1,0} parameter(21)
  %p.22 = bf16[1024,3072]{1,0} parameter(22)
  %p.23 = bf16[1024]{0} parameter(23)
  %p.24 = bf16[6144,1024]{1,0} parameter(24)
  %p.25 = bf16[1024,3072]{1,0} parameter(25)
  %p.26 = bf16[1024]{0} parameter(26)
  %p.27 = bf16[6144,1024]{1,0} parameter(27)
  %p.28 = bf16[1024,3072]{1,0} parameter(28)
  %p.29 = bf16[1024]{0} parameter(29)
  %p.30 = bf16[6144,1024]{1,0} parameter(30)
  %p.31 = bf16[1024,3072]{1,0} parameter(31)
  %p.32 = bf16[1024]{0} parameter(32)
  %p.33 = bf16[6144,1024]{1,0} parameter(33)
  %p.34 = bf16[1024,3072]{1,0} parameter(34)
  %p.35 = bf16[1024]{0} parameter(35)
  %p.36 = bf16[6144,1024]{1,0} parameter(36)
  %p.37 = bf16[1024,3072]{1,0} parameter(37)
  %p.38 = bf16[1024]{0} parameter(38)
  %p.39 = bf16[6144,1024]{1,0} parameter(39)
  %p.40 = bf16[1024,3072]{1,0} parameter(40)
  %p.41 = bf16[1024]{0} parameter(41)
  %p.42 = bf16[6144,1024]{1,0} parameter(42)
  %p.43 = bf16[1024,3072]{1,0} parameter(43)
  %p.44 = bf16[1024]{0} parameter(44)
  %p.45 = bf16[6144,1024]{1,0} parameter(45)
  %p.46 = bf16[1024,3072]{1,0} parameter(46)
  %p.47 = bf16[1024]{0} parameter(47)
  %p.48 = bf16[6144,1024]{1,0} parameter(48)
  %p.49 = bf16[1024,3072]{1,0} parameter(49)
  %p.50 = bf16[1024]{0} parameter(50)
  %p.51 = bf16[6144,1024]{1,0} parameter(51)
  %p.52 = bf16[1024,3072]{1,0} parameter(52)
  %p.53 = bf16[1024]{0} parameter(53)
  %p.54 = bf16[6144,1024]{1,0} parameter(54)
  %p.55 = bf16[1024,3072]{1,0} parameter(55)
  %p.56 = bf16[1024]{0} parameter(56)
  %p.57 = bf16[6144,1024]{1,0} parameter(57)
  %p.58 = bf16[1024,3072]{1,0} parameter(58)
  %p.59 = bf16[1024]{0} parameter(59)
  %p.60 = bf16[4096,1024]{1,0} parameter(60)
  %p.61 = bf16[128]{0} parameter(61)
  %p.62 = bf16[40960,128]{1,0} parameter(62)
  %p.63 = s32[64]{0} parameter(63)
  %p.64 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(64)
  %loop_gather_fusion = bf16[64,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[64]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_concatenate = bf16[14336,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9, bf16[1024,2048]{1,0} %p.10, bf16[1024,2048]{1,0} %p.11, /*index=10*/bf16[1024,2048]{1,0} %p.12, bf16[1024,2048]{1,0} %p.13, bf16[1024,2048]{1,0} %p.14, bf16[1024,2048]{1,0} %p.15), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %gemm_fusion_dot.56.0 = bf16[64,14336]{1,0} fusion(bf16[14336,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.56_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"64","block_n":"256","block_k":"128","split_k":"1","num_stages":"3","num_warps":"8","num_ctas":"1"}},"force_earliest_schedule":false}
  %fusion.183 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[1024]{0} %p.17), kind=kCustom, calls=%fused_computation.150, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.29.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.183, bf16[6144,1024]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.29 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.29.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.13 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.29), kind=kLoop, calls=%fused_convert.13, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.30.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.1.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.30.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.182 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[1024]{0} %p.20, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[64,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.149, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.31.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.182, bf16[6144,1024]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.2.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.31.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.12 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.12, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.32.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.3.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.32.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.181 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[64,1024]{1,0} %get-tuple-element.3.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[1024]{0} %p.23, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.148, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.33.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.181, bf16[6144,1024]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.4.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.33.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.11 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.11, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.34.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.5.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.34.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_add_fusion = f32[64,1024]{1,0} fusion(bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[64,1024]{1,0} %get-tuple-element.5.0, bf16[64,1024]{1,0} %get-tuple-element.3.0, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,1024]{1,0} %get-tuple-element.1.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.1081/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.180 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion, f32[] %p.16, bf16[1024]{0} %p.26), kind=kCustom, calls=%fused_computation.147, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.172"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.35.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.180, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.6.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.35.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.10 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.10, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.36.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.7.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.36.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.179 = bf16[64,1024]{1,0} fusion(f32[] %p.16, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.7.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[1024]{0} %p.29), kind=kCustom, calls=%fused_computation.146, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.37.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.179, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.8.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.37.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.9 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.9, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.38.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.9.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.38.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.178 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[1024]{0} %p.32, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.7.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.9.0), kind=kCustom, calls=%fused_computation.145, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.39.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.178, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.10.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.39.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.8 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.8, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.40.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.11.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.40.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.177 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[64,1024]{1,0} %get-tuple-element.11.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[1024]{0} %p.35, f32[64,1024]{1,0} %loop_add_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.7.0, bf16[64,1024]{1,0} %get-tuple-element.9.0), kind=kCustom, calls=%fused_computation.144, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.41.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.177, bf16[6144,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.12.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.41.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.7 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.42.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.13.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.42.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_add_fusion.1 = f32[64,1024]{1,0} fusion(bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[64,1024]{1,0} %get-tuple-element.13.0, bf16[64,1024]{1,0} %get-tuple-element.11.0, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.7.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.9.0), kind=kLoop, calls=%fused_add.1, metadata={op_type="aten__add" op_name="aten__add.1153/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.176 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.1, f32[] %p.16, bf16[1024]{0} %p.38), kind=kCustom, calls=%fused_computation.143, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.172"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.43.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.176, bf16[6144,1024]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.14.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.43.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.6 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.44.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.15.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.44.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.175 = bf16[64,1024]{1,0} fusion(f32[] %p.16, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.15.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[1024]{0} %p.41), kind=kCustom, calls=%fused_computation.142, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.45.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.175, bf16[6144,1024]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.16.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.45.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.5 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.46.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.17.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.46.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.174 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[1024]{0} %p.44, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.15.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.17.0), kind=kCustom, calls=%fused_computation.141, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.47.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.174, bf16[6144,1024]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.18.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.47.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.4 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.18.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.48.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.19.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.48.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.173 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[64,1024]{1,0} %get-tuple-element.19.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[1024]{0} %p.47, f32[64,1024]{1,0} %loop_add_fusion.1, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.15.0, bf16[64,1024]{1,0} %get-tuple-element.17.0), kind=kCustom, calls=%fused_computation.140, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.49.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.173, bf16[6144,1024]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.20.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.49.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.3 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.20.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.50.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.21.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.50.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_add_fusion.2 = f32[64,1024]{1,0} fusion(bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[64,1024]{1,0} %get-tuple-element.21.0, bf16[64,1024]{1,0} %get-tuple-element.19.0, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.15.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.17.0), kind=kLoop, calls=%fused_add.2, metadata={op_type="aten__add" op_name="aten__add.1225/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.172 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.2, f32[] %p.16, bf16[1024]{0} %p.50), kind=kCustom, calls=%fused_computation.139, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.172"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.51.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.172, bf16[6144,1024]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.22.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.51.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.2 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.22.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.52.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.23.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.52.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.171 = bf16[64,1024]{1,0} fusion(f32[] %p.16, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.23.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[1024]{0} %p.53), kind=kCustom, calls=%fused_computation.138, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.53.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.171, bf16[6144,1024]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.24.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.53.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.1 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.24.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.54.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.25.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.54.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.170 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[1024]{0} %p.56, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.23.0, bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.25.0), kind=kCustom, calls=%fused_computation.137, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.170, bf16[6144,1024]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.26.0 = bf16[64,6144]{1,0} get-tuple-element((bf16[64,6144]{1,0}, s8[4194304]{0}) %custom-call.55.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.26.0), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.27.0 = bf16[64,1024]{1,0} get-tuple-element((bf16[64,1024]{1,0}, s8[4194304]{0}) %custom-call.56.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.168 = bf16[64,1024]{1,0} fusion(f32[] %p.16, bf16[64,1024]{1,0} %get-tuple-element.27.0, bf16[1024]{0} %p.59, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.23.0, /*index=5*/bf16[64,14336]{1,0} %gemm_fusion_dot.56.0, bf16[64,1024]{1,0} %get-tuple-element.25.0), kind=kCustom, calls=%fused_computation.135, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.57.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.168, bf16[4096,1024]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.28.0 = bf16[64,4096]{1,0} get-tuple-element((bf16[64,4096]{1,0}, s8[4194304]{0}) %custom-call.57.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_slice = bf16[64,1024]{1,0} fusion(bf16[64,4096]{1,0} %get-tuple-element.28.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %triton_softmax.16.0 = f32[64,8,128]{2,1,0} fusion(f32[] %p.16, bf16[64,4096]{1,0} %get-tuple-element.28.0), kind=kCustom, calls=%triton_softmax_computation.16, metadata={op_type="aten__mul" op_name="aten__mul.1271/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %input_concatenate_fusion = bf16[64,8,128]{2,1,0} fusion(f32[64,8,128]{2,1,0} %triton_softmax.16.0, bf16[128]{0} %p.61, bf16[40960,128]{1,0} %p.62, s32[64]{0} %p.63), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.2997.0 = bf16[64,8,128]{2,1,0} bitcast(bf16[64,1024]{1,0} %wrapped_slice)
  %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.64), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  %bitcast.3009.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[69353472]{0} %loop_slice_fusion)
  %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.64), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  ROOT %tuple = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %input_concatenate_fusion, bf16[64,8,128]{2,1,0} %bitcast.2997.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.3009.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
}

ENTRY %SyncTensorsGraph.1181 (p0.1.0: bf16[128], p1.4.0: f32[], p2.6.0: bf16[4096,1024], p3.8.0: bf16[1024], p4.40.0: s32[64], p5.42.0: bf16[151936,1024], p6.46.0: bf16[1024,2048], p7.62.0: bf16[1024,3072], p8.64.0: bf16[6144,1024], p9.66.0: bf16[1024], p10.118.0: bf16[1024,2048], p11.134.0: bf16[1024,3072], p12.136.0: bf16[6144,1024], p13.138.0: bf16[1024], p14.190.0: bf16[1024,2048], p15.206.0: bf16[1024,3072], p16.208.0: bf16[6144,1024], p17.210.0: bf16[1024], p18.262.0: bf16[1024,2048], p19.278.0: bf16[1024,3072], p20.280.0: bf16[6144,1024], p21.282.0: bf16[1024], p22.334.0: bf16[1024,2048], p23.350.0: bf16[1024,3072], p24.352.0: bf16[6144,1024], p25.354.0: bf16[1024], p26.406.0: bf16[1024,2048], p27.422.0: bf16[1024,3072], p28.424.0: bf16[6144,1024], p29.426.0: bf16[1024], p30.478.0: bf16[1024,2048], p31.494.0: bf16[1024,3072], p32.496.0: bf16[6144,1024], p33.498.0: bf16[1024], p34.550.0: bf16[1024,2048], p35.566.0: bf16[1024,3072], p36.568.0: bf16[6144,1024], p37.570.0: bf16[1024], p38.622.0: bf16[1024,2048], p39.638.0: bf16[1024,3072], p40.640.0: bf16[6144,1024], p41.642.0: bf16[1024], p42.694.0: bf16[1024,2048], p43.710.0: bf16[1024,3072], p44.712.0: bf16[6144,1024], p45.714.0: bf16[1024], p46.766.0: bf16[1024,2048], p47.782.0: bf16[1024,3072], p48.784.0: bf16[6144,1024], p49.786.0: bf16[1024], p50.838.0: bf16[1024,2048], p51.854.0: bf16[1024,3072], p52.856.0: bf16[6144,1024], p53.858.0: bf16[1024], p54.910.0: bf16[1024,2048], p55.926.0: bf16[1024,3072], p56.928.0: bf16[6144,1024], p57.930.0: bf16[1024], p58.982.0: bf16[1024,2048], p59.998.0: bf16[1024,3072], p60.1000.0: bf16[6144,1024], p61.1002.0: bf16[1024], p62.1126.0: s32[64], p63.1127.0: bf16[40960,128], p64.1171.0: bf16[2,4233,16,8,128]) -> (bf16[64,8,128], bf16[64,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128]) {
  %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %p64.1171.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(64), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p63.1127.0 = bf16[40960,128]{1,0} parameter(63), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p62.1126.0 = s32[64]{0} parameter(62), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p61.1002.0 = bf16[1024]{0} parameter(61), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p60.1000.0 = bf16[6144,1024]{1,0} parameter(60), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p59.998.0 = bf16[1024,3072]{1,0} parameter(59), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p58.982.0 = bf16[1024,2048]{1,0} parameter(58), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p57.930.0 = bf16[1024]{0} parameter(57), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p56.928.0 = bf16[6144,1024]{1,0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p55.926.0 = bf16[1024,3072]{1,0} parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p54.910.0 = bf16[1024,2048]{1,0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p53.858.0 = bf16[1024]{0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p52.856.0 = bf16[6144,1024]{1,0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p51.854.0 = bf16[1024,3072]{1,0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p50.838.0 = bf16[1024,2048]{1,0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p49.786.0 = bf16[1024]{0} parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p48.784.0 = bf16[6144,1024]{1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p47.782.0 = bf16[1024,3072]{1,0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p46.766.0 = bf16[1024,2048]{1,0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p45.714.0 = bf16[1024]{0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p44.712.0 = bf16[6144,1024]{1,0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p43.710.0 = bf16[1024,3072]{1,0} parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p42.694.0 = bf16[1024,2048]{1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p41.642.0 = bf16[1024]{0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p40.640.0 = bf16[6144,1024]{1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p39.638.0 = bf16[1024,3072]{1,0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p38.622.0 = bf16[1024,2048]{1,0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p37.570.0 = bf16[1024]{0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p36.568.0 = bf16[6144,1024]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p35.566.0 = bf16[1024,3072]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p34.550.0 = bf16[1024,2048]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p33.498.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p32.496.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p31.494.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p30.478.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p29.426.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p28.424.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p27.422.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p26.406.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p25.354.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p24.352.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p23.350.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p22.334.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p21.282.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p20.280.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p19.278.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p18.262.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p17.210.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p16.208.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p15.206.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p14.190.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p13.138.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p12.136.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p11.134.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p10.118.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p9.66.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p8.64.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p7.62.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p6.46.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p5.42.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p4.40.0 = s32[64]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %call = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) call(bf16[151936,1024]{1,0} %p5.42.0, s32[64]{0} %p4.40.0, bf16[1024,2048]{1,0} %p58.982.0, bf16[1024,2048]{1,0} %p54.910.0, bf16[1024,2048]{1,0} %p50.838.0, /*index=5*/bf16[1024,2048]{1,0} %p46.766.0, bf16[1024,2048]{1,0} %p42.694.0, bf16[1024,2048]{1,0} %p38.622.0, bf16[1024,2048]{1,0} %p34.550.0, bf16[1024,2048]{1,0} %p30.478.0, /*index=10*/bf16[1024,2048]{1,0} %p26.406.0, bf16[1024,2048]{1,0} %p22.334.0, bf16[1024,2048]{1,0} %p18.262.0, bf16[1024,2048]{1,0} %p14.190.0, bf16[1024,2048]{1,0} %p10.118.0, /*index=15*/bf16[1024,2048]{1,0} %p6.46.0, f32[] %p1.4.0, bf16[1024]{0} %p9.66.0, bf16[6144,1024]{1,0} %p8.64.0, bf16[1024,3072]{1,0} %p7.62.0, /*index=20*/bf16[1024]{0} %p13.138.0, bf16[6144,1024]{1,0} %p12.136.0, bf16[1024,3072]{1,0} %p11.134.0, bf16[1024]{0} %p17.210.0, bf16[6144,1024]{1,0} %p16.208.0, /*index=25*/bf16[1024,3072]{1,0} %p15.206.0, bf16[1024]{0} %p21.282.0, bf16[6144,1024]{1,0} %p20.280.0, bf16[1024,3072]{1,0} %p19.278.0, bf16[1024]{0} %p25.354.0, /*index=30*/bf16[6144,1024]{1,0} %p24.352.0, bf16[1024,3072]{1,0} %p23.350.0, bf16[1024]{0} %p29.426.0, bf16[6144,1024]{1,0} %p28.424.0, bf16[1024,3072]{1,0} %p27.422.0, /*index=35*/bf16[1024]{0} %p33.498.0, bf16[6144,1024]{1,0} %p32.496.0, bf16[1024,3072]{1,0} %p31.494.0, bf16[1024]{0} %p37.570.0, bf16[6144,1024]{1,0} %p36.568.0, /*index=40*/bf16[1024,3072]{1,0} %p35.566.0, bf16[1024]{0} %p41.642.0, bf16[6144,1024]{1,0} %p40.640.0, bf16[1024,3072]{1,0} %p39.638.0, bf16[1024]{0} %p45.714.0, /*index=45*/bf16[6144,1024]{1,0} %p44.712.0, bf16[1024,3072]{1,0} %p43.710.0, bf16[1024]{0} %p49.786.0, bf16[6144,1024]{1,0} %p48.784.0, bf16[1024,3072]{1,0} %p47.782.0, /*index=50*/bf16[1024]{0} %p53.858.0, bf16[6144,1024]{1,0} %p52.856.0, bf16[1024,3072]{1,0} %p51.854.0, bf16[1024]{0} %p57.930.0, bf16[6144,1024]{1,0} %p56.928.0, /*index=55*/bf16[1024,3072]{1,0} %p55.926.0, bf16[1024]{0} %p61.1002.0, bf16[6144,1024]{1,0} %p60.1000.0, bf16[1024,3072]{1,0} %p59.998.0, bf16[1024]{0} %p3.8.0, /*index=60*/bf16[4096,1024]{1,0} %p2.6.0, bf16[128]{0} %p0.1.0, bf16[40960,128]{1,0} %p63.1127.0, s32[64]{0} %p62.1126.0, bf16[2,4233,16,8,128]{4,3,2,1,0} %p64.1171.0), to_apply=%command_buffer
  %get-tuple-element.31 = bf16[64,8,128]{2,1,0} get-tuple-element((bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=0
  %get-tuple-element.32 = bf16[64,8,128]{2,1,0} get-tuple-element((bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=1
  %get-tuple-element.33 = bf16[4233,16,8,128]{3,2,1,0} get-tuple-element((bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=2
  %get-tuple-element.34 = bf16[1,4233,16,8,128]{4,3,2,1,0} get-tuple-element((bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=3
  %bitcast.3002.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[1,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.34)
  ROOT %tuple.1180.0 = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %get-tuple-element.31, bf16[64,8,128]{2,1,0} %get-tuple-element.32, bf16[4233,16,8,128]{3,2,1,0} %bitcast.3002.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.33)
}

