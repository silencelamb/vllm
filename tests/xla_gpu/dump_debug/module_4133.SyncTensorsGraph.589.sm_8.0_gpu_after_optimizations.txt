HloModule SyncTensorsGraph.589, is_scheduled=true, entry_computation_layout={(bf16[128]{0}, f32[], bf16[4096,1024]{1,0}, bf16[1024]{0}, s32[128]{0}, /*index=5*/bf16[151936,1024]{1,0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=10*/bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, /*index=15*/bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, /*index=20*/bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, /*index=25*/bf16[1024]{0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=30*/s32[128]{0}, bf16[40960,128]{1,0}, bf16[2,4233,16,8,128]{4,3,2,1,0})->(bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})}, frontend_attributes={fingerprint_before_lhs="6ac502ea1b01cfa00fd58d4efa460ea0"}

%fused_gather (param_0.9: bf16[151936,1024], param_1.285: s32[128]) -> bf16[128,1,1024] {
  %param_0.9 = bf16[151936,1024]{1,0} parameter(0)
  %param_1.285 = s32[128]{0} parameter(1)
  %convert.343.3 = s64[128]{0} convert(s32[128]{0} %param_1.285), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.344.3 = u32[128]{0} convert(s64[128]{0} %convert.343.3), metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.1167.1 = u32[128,1]{1,0} bitcast(u32[128]{0} %convert.344.3)
  ROOT %gather.3 = bf16[128,1,1024]{2,0,1} gather(bf16[151936,1024]{1,0} %param_0.9, u32[128,1]{1,0} %bitcast.1167.1), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,1024}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_concatenate_computation (param_0.351: bf16[1024,2048], param_1.297: bf16[1024,2048], param_2.183: bf16[1024,2048], param_3.153: bf16[1024,2048], param_4.86: bf16[1024,2048], param_5.36: bf16[1024,2048]) -> bf16[6144,2048] {
  %param_0.351 = bf16[1024,2048]{1,0} parameter(0)
  %param_1.297 = bf16[1024,2048]{1,0} parameter(1)
  %param_2.183 = bf16[1024,2048]{1,0} parameter(2)
  %param_3.153 = bf16[1024,2048]{1,0} parameter(3)
  %param_4.86 = bf16[1024,2048]{1,0} parameter(4)
  %param_5.36 = bf16[1024,2048]{1,0} parameter(5)
  ROOT %concatenate.15.1 = bf16[6144,2048]{1,0} concatenate(bf16[1024,2048]{1,0} %param_0.351, bf16[1024,2048]{1,0} %param_1.297, bf16[1024,2048]{1,0} %param_2.183, bf16[1024,2048]{1,0} %param_3.153, bf16[1024,2048]{1,0} %param_4.86, /*index=5*/bf16[1024,2048]{1,0} %param_5.36), dimensions={0}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
}

%gemm_fusion_dot.23_computation (parameter_0: bf16[6144,2048]) -> bf16[128,6144] {
  %constant_75 = bf16[] constant(0), metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.65 = bf16[128,2048]{1,0} broadcast(bf16[] %constant_75), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %parameter_0 = bf16[6144,2048]{1,0} parameter(0)
  ROOT %dot.24 = bf16[128,6144]{1,0} dot(bf16[128,2048]{1,0} %broadcast.65, bf16[6144,2048]{1,0} %parameter_0), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%AddComputation.58 (x.59: f32[], y.60: f32[]) -> f32[] {
  %y.60 = f32[] parameter(1)
  %x.59 = f32[] parameter(0)
  ROOT %add.89 = f32[] add(f32[] %x.59, f32[] %y.60)
}

%fused_computation.79 (param_0.336: f32[], param_1.281: bf16[128,1,1024], param_2.167: bf16[128,6144], param_3.130: bf16[1024]) -> bf16[128,1024] {
  %param_2.167 = bf16[128,6144]{1,0} parameter(2)
  %convert.342.24 = f32[128,6144]{1,0} convert(bf16[128,6144]{1,0} %param_2.167), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.94.9 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.24), slice={[0:128], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.281 = bf16[128,1,1024]{2,0,1} parameter(1)
  %bitcast.1170.9 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_1.281)
  %convert.345.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1170.9), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.90.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.94.9, f32[128,1024]{1,0} %convert.345.9), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.198 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.90.7, f32[128,1024]{1,0} %add.90.7), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_179 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.17 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.198, f32[] %constant_179), dimensions={1}, to_apply=%AddComputation.58, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_178 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.226 = f32[128]{0} broadcast(f32[] %constant_178), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.197 = f32[128]{0} multiply(f32[128]{0} %reduce.17, f32[128]{0} %broadcast.226), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.336 = f32[] parameter(0)
  %broadcast.225 = f32[128]{0} broadcast(f32[] %param_0.336), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.111 = f32[128]{0} add(f32[128]{0} %multiply.197, f32[128]{0} %broadcast.225), metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.41 = f32[128]{0} rsqrt(f32[128]{0} %add.111), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.224 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.41), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1536/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.196 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.90.7, f32[128,1024]{1,0} %broadcast.224), metadata={op_type="aten__mul" op_name="aten__mul.1536/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.130 = bf16[1024]{0} parameter(3)
  %convert.346.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.130), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.167.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.346.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1537/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.167.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.196, f32[128,1024]{1,0} %broadcast.167.1), metadata={op_type="aten__mul" op_name="aten__mul.1537/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.347.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.167.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert (param_0.313: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.313 = bf16[128,6144]{1,0} parameter(0)
  %slice.95.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.313), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.348.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.95.1)
  %constant_1_4 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.436.2 = f32[] convert(bf16[] %constant_1_4)
  %broadcast.194.48 = f32[128,3072]{1,0} broadcast(f32[] %convert.436.2), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.18.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.348.8)
  %convert.352.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.18.7)
  %exponential.18.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.352.5)
  %convert.353.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.18.3)
  %add.91.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.194.48, f32[128,3072]{1,0} %convert.353.1)
  %divide.18.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.194.48, f32[128,3072]{1,0} %add.91.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.168.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.348.8, f32[128,3072]{1,0} %divide.18.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.96.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.313), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.355.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.96.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.169.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.168.5, f32[128,3072]{1,0} %convert.355.1), metadata={op_type="aten__mul" op_name="aten__mul.1538/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.356.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.169.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.77 (param_0.347: f32[], param_1.291: bf16[1024], param_2.177: bf16[128,1,1024], param_3.147: bf16[128,6144], param_4.79: bf16[128,1024]) -> bf16[128,1024] {
  %param_3.147 = bf16[128,6144]{1,0} parameter(3)
  %convert.342.44 = f32[128,6144]{1,0} convert(bf16[128,6144]{1,0} %param_3.147), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.93.5 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.44), slice={[0:128], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.79 = bf16[128,1024]{1,0} parameter(4)
  %convert.357.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_4.79), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.94.11 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.44), slice={[0:128], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.177 = bf16[128,1,1024]{2,0,1} parameter(2)
  %bitcast.1170.11 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_2.177)
  %convert.345.11 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1170.11), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.90.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.94.11, f32[128,1024]{1,0} %convert.345.11), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.92.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.357.5, f32[128,1024]{1,0} %add.90.9), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.93.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.93.5, f32[128,1024]{1,0} %add.92.5), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.204 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.93.3, f32[128,1024]{1,0} %add.93.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_184 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.19 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.204, f32[] %constant_184), dimensions={1}, to_apply=%AddComputation.58, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_183 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.233 = f32[128]{0} broadcast(f32[] %constant_183), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.203 = f32[128]{0} multiply(f32[128]{0} %reduce.19, f32[128]{0} %broadcast.233), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.347 = f32[] parameter(0)
  %broadcast.232 = f32[128]{0} broadcast(f32[] %param_0.347), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.113 = f32[128]{0} add(f32[128]{0} %multiply.203, f32[128]{0} %broadcast.232), metadata={op_type="aten__add" op_name="aten__add.1553/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.43 = f32[128]{0} rsqrt(f32[128]{0} %add.113), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.231 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.43), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1554/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.202 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.93.3, f32[128,1024]{1,0} %broadcast.231), metadata={op_type="aten__mul" op_name="aten__mul.1554/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.291 = bf16[1024]{0} parameter(1)
  %convert.358.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.291), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.171.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.358.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1555/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.171.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.202, f32[128,1024]{1,0} %broadcast.171.1), metadata={op_type="aten__mul" op_name="aten__mul.1555/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.359.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.171.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.1 (param_0.315: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.315 = bf16[128,6144]{1,0} parameter(0)
  %slice.97.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.315), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.360.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.97.1)
  %constant_1_6 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.436.4 = f32[] convert(bf16[] %constant_1_6)
  %broadcast.194.44 = f32[128,3072]{1,0} broadcast(f32[] %convert.436.4), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.19.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.360.8)
  %convert.365.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.19.7)
  %exponential.19.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.365.5)
  %convert.366.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.19.3)
  %add.94.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.194.44, f32[128,3072]{1,0} %convert.366.1)
  %divide.19.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.194.44, f32[128,3072]{1,0} %add.94.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.172.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.360.8, f32[128,3072]{1,0} %divide.19.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.98.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.315), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.367.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.98.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.173.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.172.5, f32[128,3072]{1,0} %convert.367.1), metadata={op_type="aten__mul" op_name="aten__mul.1556/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.368.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.173.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.75 (param_0.334: f32[], param_1.294: bf16[128,1024], param_2.181: bf16[128,6144], param_3.150: bf16[1024], param_4.85: bf16[128,1,1024], param_5.35: bf16[128,1024]) -> bf16[128,1024] {
  %param_2.181 = bf16[128,6144]{1,0} parameter(2)
  %convert.342.30 = f32[128,6144]{1,0} convert(bf16[128,6144]{1,0} %param_2.181), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.92.5 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.30), slice={[0:128], [3072:4096]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.294 = bf16[128,1024]{1,0} parameter(1)
  %convert.371.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_1.294), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.93.9 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.30), slice={[0:128], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.35 = bf16[128,1024]{1,0} parameter(5)
  %convert.357.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_5.35), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.94.15 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.30), slice={[0:128], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.85 = bf16[128,1,1024]{2,0,1} parameter(4)
  %bitcast.1170.15 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_4.85)
  %convert.345.15 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1170.15), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.90.13 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.94.15, f32[128,1024]{1,0} %convert.345.15), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.92.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.357.9, f32[128,1024]{1,0} %add.90.13), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.93.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.93.9, f32[128,1024]{1,0} %add.92.9), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.95.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.371.5, f32[128,1024]{1,0} %add.93.7), metadata={op_type="aten__add" op_name="aten__add.1557/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.96.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.92.5, f32[128,1024]{1,0} %add.95.5), metadata={op_type="aten__add" op_name="aten__add.1570/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.210 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.96.3, f32[128,1024]{1,0} %add.96.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_189 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.21 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.210, f32[] %constant_189), dimensions={1}, to_apply=%AddComputation.58, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_188 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.239 = f32[128]{0} broadcast(f32[] %constant_188), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.209 = f32[128]{0} multiply(f32[128]{0} %reduce.21, f32[128]{0} %broadcast.239), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.334 = f32[] parameter(0)
  %broadcast.238 = f32[128]{0} broadcast(f32[] %param_0.334), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.116 = f32[128]{0} add(f32[128]{0} %multiply.209, f32[128]{0} %broadcast.238), metadata={op_type="aten__add" op_name="aten__add.1571/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.45 = f32[128]{0} rsqrt(f32[128]{0} %add.116), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.237 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.45), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1572/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.208 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.96.3, f32[128,1024]{1,0} %broadcast.237), metadata={op_type="aten__mul" op_name="aten__mul.1572/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.150 = bf16[1024]{0} parameter(3)
  %convert.372.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.150), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.174.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.372.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1573/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.174.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.208, f32[128,1024]{1,0} %broadcast.174.1), metadata={op_type="aten__mul" op_name="aten__mul.1573/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.374.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.174.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.2 (param_0.317: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.317 = bf16[128,6144]{1,0} parameter(0)
  %slice.99.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.317), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.375.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.99.1)
  %constant_1_1 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.436.6 = f32[] convert(bf16[] %constant_1_1)
  %broadcast.194.40 = f32[128,3072]{1,0} broadcast(f32[] %convert.436.6), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.20.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.375.8)
  %convert.381.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.20.7)
  %exponential.20.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.381.5)
  %convert.383.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.20.3)
  %add.97.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.194.40, f32[128,3072]{1,0} %convert.383.1)
  %divide.20.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.194.40, f32[128,3072]{1,0} %add.97.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.175.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.375.8, f32[128,3072]{1,0} %divide.20.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.100.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.317), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.385.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.100.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.176.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.175.5, f32[128,3072]{1,0} %convert.385.1), metadata={op_type="aten__mul" op_name="aten__mul.1574/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.386.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.176.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_add (param_0.263: bf16[128,6144], param_1.254: bf16[128,1024], param_2.179: bf16[128,1024], param_3.149: bf16[128,1,1024], param_4.83: bf16[128,1024]) -> f32[128,1024] {
  %param_0.263 = bf16[128,6144]{1,0} parameter(0)
  %convert.342.14 = f32[128,6144]{1,0} convert(bf16[128,6144]{1,0} %param_0.263), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.91.3 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.14), slice={[0:128], [2048:3072]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.254 = bf16[128,1024]{1,0} parameter(1)
  %convert.387.3 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_1.254), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.92.7 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.14), slice={[0:128], [3072:4096]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.179 = bf16[128,1024]{1,0} parameter(2)
  %convert.371.7 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_2.179), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.93.7 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.14), slice={[0:128], [4096:5120]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.83 = bf16[128,1024]{1,0} parameter(4)
  %convert.357.7 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_4.83), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.94.13 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.14), slice={[0:128], [5120:6144]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.149 = bf16[128,1,1024]{2,0,1} parameter(3)
  %bitcast.1170.13 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_3.149)
  %convert.345.13 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.1170.13), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.90.11 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.94.13, f32[128,1024]{1,0} %convert.345.13), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.92.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.357.7, f32[128,1024]{1,0} %add.90.11), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.93.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.93.7, f32[128,1024]{1,0} %add.92.7), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.95.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.371.7, f32[128,1024]{1,0} %add.93.5), metadata={op_type="aten__add" op_name="aten__add.1557/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.96.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.92.7, f32[128,1024]{1,0} %add.95.7), metadata={op_type="aten__add" op_name="aten__add.1570/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.98.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.387.3, f32[128,1024]{1,0} %add.96.5), metadata={op_type="aten__add" op_name="aten__add.1575/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %add.100.1 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.91.3, f32[128,1024]{1,0} %add.98.3), metadata={op_type="aten__add" op_name="aten__add.1588/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.73 (param_0.333: f32[128,1024], param_1.278: f32[], param_2.164: bf16[1024]) -> bf16[128,1024] {
  %param_0.333 = f32[128,1024]{1,0} parameter(0)
  %multiply.217 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_0.333, f32[128,1024]{1,0} %param_0.333), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_194 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.23 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.217, f32[] %constant_194), dimensions={1}, to_apply=%AddComputation.58, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_193 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.246 = f32[128]{0} broadcast(f32[] %constant_193), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.216 = f32[128]{0} multiply(f32[128]{0} %reduce.23, f32[128]{0} %broadcast.246), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.278 = f32[] parameter(1)
  %broadcast.245 = f32[128]{0} broadcast(f32[] %param_1.278), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.118 = f32[128]{0} add(f32[128]{0} %multiply.216, f32[128]{0} %broadcast.245), metadata={op_type="aten__add" op_name="aten__add.1589/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.47 = f32[128]{0} rsqrt(f32[128]{0} %add.118), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.244 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.47), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1590/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.214 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %param_0.333, f32[128,1024]{1,0} %broadcast.244), metadata={op_type="aten__mul" op_name="aten__mul.1590/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.164 = bf16[1024]{0} parameter(2)
  %convert.390.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.164), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.177.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.390.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1591/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.177.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.214, f32[128,1024]{1,0} %broadcast.177.1), metadata={op_type="aten__mul" op_name="aten__mul.1591/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.391.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.177.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.3 (param_0.316: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.316 = bf16[128,6144]{1,0} parameter(0)
  %slice.101.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.316), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.392.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.101.1)
  %constant_1_7 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.436.5 = f32[] convert(bf16[] %constant_1_7)
  %broadcast.194.36 = f32[128,3072]{1,0} broadcast(f32[] %convert.436.5), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.21.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.392.8)
  %convert.396.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.21.7)
  %exponential.21.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.396.5)
  %convert.397.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.21.3)
  %add.101.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.194.36, f32[128,3072]{1,0} %convert.397.1)
  %divide.21.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.194.36, f32[128,3072]{1,0} %add.101.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.178.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.392.8, f32[128,3072]{1,0} %divide.21.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.102.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.316), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.398.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.102.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.179.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.178.5, f32[128,3072]{1,0} %convert.398.1), metadata={op_type="aten__mul" op_name="aten__mul.1592/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.399.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.179.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.71 (param_0.332: f32[], param_1.277: f32[128,1024], param_2.163: bf16[128,1024], param_3.128: bf16[128,6144], param_4.64: bf16[1024]) -> bf16[128,1024] {
  %param_3.128 = bf16[128,6144]{1,0} parameter(3)
  %convert.342.26 = f32[128,6144]{1,0} convert(bf16[128,6144]{1,0} %param_3.128), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.90.5 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.26), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.163 = bf16[128,1024]{1,0} parameter(2)
  %convert.401.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_2.163), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.277 = f32[128,1024]{1,0} parameter(1)
  %add.102.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.401.5, f32[128,1024]{1,0} %param_1.277), metadata={op_type="aten__add" op_name="aten__add.1593/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.103.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.90.5, f32[128,1024]{1,0} %add.102.5), metadata={op_type="aten__add" op_name="aten__add.1606/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.225 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.103.3, f32[128,1024]{1,0} %add.103.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_203 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.25 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.225, f32[] %constant_203), dimensions={1}, to_apply=%AddComputation.58, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_202 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.254 = f32[128]{0} broadcast(f32[] %constant_202), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.223 = f32[128]{0} multiply(f32[128]{0} %reduce.25, f32[128]{0} %broadcast.254), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.332 = f32[] parameter(0)
  %broadcast.253 = f32[128]{0} broadcast(f32[] %param_0.332), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.120 = f32[128]{0} add(f32[128]{0} %multiply.223, f32[128]{0} %broadcast.253), metadata={op_type="aten__add" op_name="aten__add.1607/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.49 = f32[128]{0} rsqrt(f32[128]{0} %add.120), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.251 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.49), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1608/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.222 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.103.3, f32[128,1024]{1,0} %broadcast.251), metadata={op_type="aten__mul" op_name="aten__mul.1608/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.64 = bf16[1024]{0} parameter(4)
  %convert.402.1 = f32[1024]{0} convert(bf16[1024]{0} %param_4.64), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.182.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.402.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1609/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.180.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.222, f32[128,1024]{1,0} %broadcast.182.1), metadata={op_type="aten__mul" op_name="aten__mul.1609/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.403.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.180.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.4 (param_0.314: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.314 = bf16[128,6144]{1,0} parameter(0)
  %slice.103.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.314), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.406.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.103.1)
  %constant_1_5 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.436.3 = f32[] convert(bf16[] %constant_1_5)
  %broadcast.194.32 = f32[128,3072]{1,0} broadcast(f32[] %convert.436.3), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.22.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.406.8)
  %convert.410.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.22.7)
  %exponential.22.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.410.5)
  %convert.412.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.22.3)
  %add.104.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.194.32, f32[128,3072]{1,0} %convert.412.1)
  %divide.22.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.194.32, f32[128,3072]{1,0} %add.104.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.181.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.406.8, f32[128,3072]{1,0} %divide.22.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.104.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.314), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.413.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.104.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.182.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.181.5, f32[128,3072]{1,0} %convert.413.1), metadata={op_type="aten__mul" op_name="aten__mul.1610/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.414.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.182.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.69 (param_0.344: f32[], param_1.287: bf16[1024], param_2.172: f32[128,1024], param_3.140: bf16[128,1024], param_4.70: bf16[128,6144], param_5.14: bf16[128,1024]) -> bf16[128,1024] {
  %param_4.70 = bf16[128,6144]{1,0} parameter(4)
  %convert.342.36 = f32[128,6144]{1,0} convert(bf16[128,6144]{1,0} %param_4.70), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.88.5 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.36), slice={[0:128], [0:1024]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.14 = bf16[128,1024]{1,0} parameter(5)
  %convert.415.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_5.14), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.90.9 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.36), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.140 = bf16[128,1024]{1,0} parameter(3)
  %convert.401.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_3.140), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.172 = f32[128,1024]{1,0} parameter(2)
  %add.102.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.401.9, f32[128,1024]{1,0} %param_2.172), metadata={op_type="aten__add" op_name="aten__add.1593/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.103.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.90.9, f32[128,1024]{1,0} %add.102.9), metadata={op_type="aten__add" op_name="aten__add.1606/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.105.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.415.5, f32[128,1024]{1,0} %add.103.7), metadata={op_type="aten__add" op_name="aten__add.1611/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.106.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.88.5, f32[128,1024]{1,0} %add.105.5), metadata={op_type="aten__add" op_name="aten__add.1624/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.232 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.106.3, f32[128,1024]{1,0} %add.106.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_211 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.27 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.232, f32[] %constant_211), dimensions={1}, to_apply=%AddComputation.58, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_210 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.261 = f32[128]{0} broadcast(f32[] %constant_210), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.231 = f32[128]{0} multiply(f32[128]{0} %reduce.27, f32[128]{0} %broadcast.261), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.344 = f32[] parameter(0)
  %broadcast.260 = f32[128]{0} broadcast(f32[] %param_0.344), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.122 = f32[128]{0} add(f32[128]{0} %multiply.231, f32[128]{0} %broadcast.260), metadata={op_type="aten__add" op_name="aten__add.1625/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.51 = f32[128]{0} rsqrt(f32[128]{0} %add.122), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.259 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.51), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1626/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.230 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.106.3, f32[128,1024]{1,0} %broadcast.259), metadata={op_type="aten__mul" op_name="aten__mul.1626/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.287 = bf16[1024]{0} parameter(1)
  %convert.416.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.287), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.186.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.416.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1627/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.183.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.230, f32[128,1024]{1,0} %broadcast.186.1), metadata={op_type="aten__mul" op_name="aten__mul.1627/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.417.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.183.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.5 (param_0.312: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.312 = bf16[128,6144]{1,0} parameter(0)
  %slice.105.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.312), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.418.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.105.1)
  %constant_1_3 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.436.1 = f32[] convert(bf16[] %constant_1_3)
  %broadcast.194.28 = f32[128,3072]{1,0} broadcast(f32[] %convert.436.1), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.23.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.418.8)
  %convert.422.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.23.7)
  %exponential.23.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.422.5)
  %convert.423.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.23.3)
  %add.107.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.194.28, f32[128,3072]{1,0} %convert.423.1)
  %divide.23.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.194.28, f32[128,3072]{1,0} %add.107.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.184.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.418.8, f32[128,3072]{1,0} %divide.23.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.106.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.312), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.424.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.106.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.185.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.184.5, f32[128,3072]{1,0} %convert.424.1), metadata={op_type="aten__mul" op_name="aten__mul.1628/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.425.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.185.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.67 (param_0.330: f32[], param_1.289: bf16[128,1024], param_2.174: bf16[1024], param_3.143: f32[128,1024], param_4.74: bf16[128,1024], param_5.19: bf16[128,6144], param_6.12: bf16[128,1024]) -> bf16[128,1024] {
  %param_1.289 = bf16[128,1024]{1,0} parameter(1)
  %convert.427.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_1.289), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.19 = bf16[128,6144]{1,0} parameter(5)
  %convert.342.40 = f32[128,6144]{1,0} convert(bf16[128,6144]{1,0} %param_5.19), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.88.7 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.40), slice={[0:128], [0:1024]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_6.12 = bf16[128,1024]{1,0} parameter(6)
  %convert.415.7 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_6.12), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.90.11 = f32[128,1024]{1,0} slice(f32[128,6144]{1,0} %convert.342.40), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.74 = bf16[128,1024]{1,0} parameter(4)
  %convert.401.11 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_4.74), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.143 = f32[128,1024]{1,0} parameter(3)
  %add.102.11 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.401.11, f32[128,1024]{1,0} %param_3.143), metadata={op_type="aten__add" op_name="aten__add.1593/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.103.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.90.11, f32[128,1024]{1,0} %add.102.11), metadata={op_type="aten__add" op_name="aten__add.1606/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.105.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.415.7, f32[128,1024]{1,0} %add.103.9), metadata={op_type="aten__add" op_name="aten__add.1611/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.106.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.88.7, f32[128,1024]{1,0} %add.105.7), metadata={op_type="aten__add" op_name="aten__add.1624/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.108.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.427.5, f32[128,1024]{1,0} %add.106.5), metadata={op_type="aten__add" op_name="aten__add.1629/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.240 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.108.5, f32[128,1024]{1,0} %add.108.5), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_216 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.29 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.240, f32[] %constant_216), dimensions={1}, to_apply=%AddComputation.58, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_215 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.267 = f32[128]{0} broadcast(f32[] %constant_215), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.239 = f32[128]{0} multiply(f32[128]{0} %reduce.29, f32[128]{0} %broadcast.267), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.330 = f32[] parameter(0)
  %broadcast.266 = f32[128]{0} broadcast(f32[] %param_0.330), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.124 = f32[128]{0} add(f32[128]{0} %multiply.239, f32[128]{0} %broadcast.266), metadata={op_type="aten__add" op_name="aten__add.1630/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.53 = f32[128]{0} rsqrt(f32[128]{0} %add.124), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.265 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.53), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1631/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.238 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.108.5, f32[128,1024]{1,0} %broadcast.265), metadata={op_type="aten__mul" op_name="aten__mul.1631/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.174 = bf16[1024]{0} parameter(2)
  %convert.428.1 = f32[1024]{0} convert(bf16[1024]{0} %param_2.174), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.189.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.428.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1632/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.187.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.238, f32[128,1024]{1,0} %broadcast.189.1), metadata={op_type="aten__mul" op_name="aten__mul.1632/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.429.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.187.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_slice_computation (param_0.352: bf16[128,4096]) -> bf16[128,1024] {
  %param_0.352 = bf16[128,4096]{1,0} parameter(0)
  ROOT %slice.112.1 = bf16[128,1024]{1,0} slice(bf16[128,4096]{1,0} %param_0.352), slice={[0:128], [3072:4096]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%triton_softmax_computation.8 (param_0.253: f32[], param_1.246: bf16[128,4096]) -> f32[128,8,128] {
  %param_1.246 = bf16[128,4096]{1,0} parameter(1)
  %slice.107.1 = bf16[128,1024]{1,0} slice(bf16[128,4096]{1,0} %param_1.246), slice={[0:128], [2048:3072]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.1457.3 = bf16[128,8,128]{2,1,0} bitcast(bf16[128,1024]{1,0} %slice.107.1)
  %convert.430.3 = f32[128,8,128]{2,1,0} convert(bf16[128,8,128]{2,1,0} %bitcast.1457.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.137 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %convert.430.3, f32[128,8,128]{2,1,0} %convert.430.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_92 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.8 = f32[128,8]{1,0} reduce(f32[128,8,128]{2,1,0} %multiply.137, f32[] %constant_92), dimensions={2}, to_apply=%AddComputation.58, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_93 = f32[] constant(0.0078125), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.139 = f32[128,8]{1,0} broadcast(f32[] %constant_93), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.138 = f32[128,8]{1,0} multiply(f32[128,8]{1,0} %reduce.8, f32[128,8]{1,0} %broadcast.139), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.253 = f32[] parameter(0)
  %broadcast.140 = f32[128,8]{1,0} broadcast(f32[] %param_0.253), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1633/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.81 = f32[128,8]{1,0} add(f32[128,8]{1,0} %multiply.138, f32[128,8]{1,0} %broadcast.140), metadata={op_type="aten__add" op_name="aten__add.1633/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.32 = f32[128,8]{1,0} rsqrt(f32[128,8]{1,0} %add.81), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.141 = f32[128,8,128]{2,1,0} broadcast(f32[128,8]{1,0} %rsqrt.32), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul.1634/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %multiply.139 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %convert.430.3, f32[128,8,128]{2,1,0} %broadcast.141), metadata={op_type="aten__mul" op_name="aten__mul.1634/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_concatenate (param_0.339: f32[128,8,128], param_1.296: bf16[128], param_2.182: bf16[40960,128], param_3.152: s32[128]) -> bf16[128,8,128] {
  %param_0.339 = f32[128,8,128]{2,1,0} parameter(0)
  %param_1.296 = bf16[128]{0} parameter(1)
  %convert.431.1 = f32[128]{0} convert(bf16[128]{0} %param_1.296), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.190.18 = f32[128,8,128]{2,1,0} broadcast(f32[128]{0} %convert.431.1), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul.1635/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.188.18 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %param_0.339, f32[128,8,128]{2,1,0} %broadcast.190.18), metadata={op_type="aten__mul" op_name="aten__mul.1635/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.108.9 = f32[128,8,64]{2,1,0} slice(f32[128,8,128]{2,1,0} %multiply.188.18), slice={[0:128], [0:8], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.182 = bf16[40960,128]{1,0} parameter(2)
  %param_3.152 = s32[128]{0} parameter(3)
  %bitcast.1473.3 = s32[128,1]{1,0} bitcast(s32[128]{0} %param_3.152)
  %gather.1.3 = bf16[128,1,128]{2,0,1} gather(bf16[40960,128]{1,0} %param_2.182, s32[128,1]{1,0} %bitcast.1473.3), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,128}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.1476.7 = bf16[128,128]{1,0} bitcast(bf16[128,1,128]{2,0,1} %gather.1.3)
  %convert.432.7 = f32[128,128]{1,0} convert(bf16[128,128]{1,0} %bitcast.1476.7), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.109.3 = f32[128,64]{1,0} slice(f32[128,128]{1,0} %convert.432.7), slice={[0:128], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.191.14 = f32[128,8,64]{2,1,0} broadcast(f32[128,64]{1,0} %slice.109.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.1636/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.189.7 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.108.9, f32[128,8,64]{2,1,0} %broadcast.191.14), metadata={op_type="aten__mul" op_name="aten__mul.1636/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.110.9 = f32[128,8,64]{2,1,0} slice(f32[128,8,128]{2,1,0} %multiply.188.18), slice={[0:128], [0:8], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.111.3 = f32[128,64]{1,0} slice(f32[128,128]{1,0} %convert.432.7), slice={[0:128], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.192.10 = f32[128,8,64]{2,1,0} broadcast(f32[128,64]{1,0} %slice.111.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.1637/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.190.5 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.110.9, f32[128,8,64]{2,1,0} %broadcast.192.10), metadata={op_type="aten__mul" op_name="aten__mul.1637/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %subtract.2.5 = f32[128,8,64]{2,1,0} subtract(f32[128,8,64]{2,1,0} %multiply.189.7, f32[128,8,64]{2,1,0} %multiply.190.5), metadata={op_type="aten__sub" op_name="aten__sub.1638/aten__sub" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.434.3 = bf16[128,8,64]{2,1,0} convert(f32[128,8,64]{2,1,0} %subtract.2.5)
  %multiply.191.7 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.110.9, f32[128,8,64]{2,1,0} %broadcast.191.14), metadata={op_type="aten__mul" op_name="aten__mul.1639/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.192.5 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.108.9, f32[128,8,64]{2,1,0} %broadcast.192.10), metadata={op_type="aten__mul" op_name="aten__mul.1640/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.109.5 = f32[128,8,64]{2,1,0} add(f32[128,8,64]{2,1,0} %multiply.191.7, f32[128,8,64]{2,1,0} %multiply.192.5), metadata={op_type="aten__add" op_name="aten__add.1641/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.435.3 = bf16[128,8,64]{2,1,0} convert(f32[128,8,64]{2,1,0} %add.109.5)
  ROOT %concatenate.16.1 = bf16[128,8,128]{2,1,0} concatenate(bf16[128,8,64]{2,1,0} %convert.434.3, bf16[128,8,64]{2,1,0} %convert.435.3), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_slice (param_0.1: bf16[2,4233,16,8,128]) -> bf16[69353472] {
  %param_0.1 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  %bitcast.1517.1 = bf16[138706944]{0} bitcast(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.1)
  ROOT %slice.114.1 = bf16[69353472]{0} slice(bf16[138706944]{0} %bitcast.1517.1), slice={[69353472:138706944]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%wrapped_slice_computation.1 (param_0.353: bf16[2,4233,16,8,128]) -> bf16[1,4233,16,8,128] {
  %param_0.353 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  ROOT %slice.113.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} slice(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.353), slice={[0:1], [0:4233], [0:16], [0:8], [0:128]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%command_buffer (p: bf16[151936,1024], p.1: s32[128], p.2: bf16[1024,2048], p.3: bf16[1024,2048], p.4: bf16[1024,2048], p.5: bf16[1024,2048], p.6: bf16[1024,2048], p.7: bf16[1024,2048], p.8: f32[], p.9: bf16[1024], p.10: bf16[6144,1024], p.11: bf16[1024,3072], p.12: bf16[1024], p.13: bf16[6144,1024], p.14: bf16[1024,3072], p.15: bf16[1024], p.16: bf16[6144,1024], p.17: bf16[1024,3072], p.18: bf16[1024], p.19: bf16[6144,1024], p.20: bf16[1024,3072], p.21: bf16[1024], p.22: bf16[6144,1024], p.23: bf16[1024,3072], p.24: bf16[1024], p.25: bf16[6144,1024], p.26: bf16[1024,3072], p.27: bf16[1024], p.28: bf16[4096,1024], p.29: bf16[128], p.30: bf16[40960,128], p.31: s32[128], p.32: bf16[2,4233,16,8,128]) -> (bf16[128,8,128], bf16[128,8,128], bf16[4233,16,8,128], bf16[1,4233,16,8,128]) {
  %p = bf16[151936,1024]{1,0} parameter(0)
  %p.1 = s32[128]{0} parameter(1)
  %p.2 = bf16[1024,2048]{1,0} parameter(2)
  %p.3 = bf16[1024,2048]{1,0} parameter(3)
  %p.4 = bf16[1024,2048]{1,0} parameter(4)
  %p.5 = bf16[1024,2048]{1,0} parameter(5)
  %p.6 = bf16[1024,2048]{1,0} parameter(6)
  %p.7 = bf16[1024,2048]{1,0} parameter(7)
  %p.8 = f32[] parameter(8)
  %p.9 = bf16[1024]{0} parameter(9)
  %p.10 = bf16[6144,1024]{1,0} parameter(10)
  %p.11 = bf16[1024,3072]{1,0} parameter(11)
  %p.12 = bf16[1024]{0} parameter(12)
  %p.13 = bf16[6144,1024]{1,0} parameter(13)
  %p.14 = bf16[1024,3072]{1,0} parameter(14)
  %p.15 = bf16[1024]{0} parameter(15)
  %p.16 = bf16[6144,1024]{1,0} parameter(16)
  %p.17 = bf16[1024,3072]{1,0} parameter(17)
  %p.18 = bf16[1024]{0} parameter(18)
  %p.19 = bf16[6144,1024]{1,0} parameter(19)
  %p.20 = bf16[1024,3072]{1,0} parameter(20)
  %p.21 = bf16[1024]{0} parameter(21)
  %p.22 = bf16[6144,1024]{1,0} parameter(22)
  %p.23 = bf16[1024,3072]{1,0} parameter(23)
  %p.24 = bf16[1024]{0} parameter(24)
  %p.25 = bf16[6144,1024]{1,0} parameter(25)
  %p.26 = bf16[1024,3072]{1,0} parameter(26)
  %p.27 = bf16[1024]{0} parameter(27)
  %p.28 = bf16[4096,1024]{1,0} parameter(28)
  %p.29 = bf16[128]{0} parameter(29)
  %p.30 = bf16[40960,128]{1,0} parameter(30)
  %p.31 = s32[128]{0} parameter(31)
  %p.32 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(32)
  %loop_gather_fusion = bf16[128,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[128]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_concatenate = bf16[6144,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %gemm_fusion_dot.23.0 = bf16[128,6144]{1,0} fusion(bf16[6144,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.23_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"128","block_n":"32","block_k":"64","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
  %fusion.96 = bf16[128,1024]{1,0} fusion(f32[] %p.8, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,6144]{1,0} %gemm_fusion_dot.23.0, bf16[1024]{0} %p.9), kind=kCustom, calls=%fused_computation.79, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.13.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.96, bf16[6144,1024]{1,0} %p.10), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.13 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.13.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.13), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.14.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.1.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.14.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.94 = bf16[128,1024]{1,0} fusion(f32[] %p.8, bf16[1024]{0} %p.12, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,6144]{1,0} %gemm_fusion_dot.23.0, bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.77, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.15.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.94, bf16[6144,1024]{1,0} %p.13), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.2.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.15.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.1 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.16.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.3.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.16.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.92 = bf16[128,1024]{1,0} fusion(f32[] %p.8, bf16[128,1024]{1,0} %get-tuple-element.3.0, bf16[128,6144]{1,0} %gemm_fusion_dot.23.0, bf16[1024]{0} %p.15, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.75, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.17.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.92, bf16[6144,1024]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.4.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.17.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.2 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.18.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.5.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.18.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_add_fusion = f32[128,1024]{1,0} fusion(bf16[128,6144]{1,0} %gemm_fusion_dot.23.0, bf16[128,1024]{1,0} %get-tuple-element.5.0, bf16[128,1024]{1,0} %get-tuple-element.3.0, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.1588/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.90 = bf16[128,1024]{1,0} fusion(f32[128,1024]{1,0} %loop_add_fusion, f32[] %p.8, bf16[1024]{0} %p.18), kind=kCustom, calls=%fused_computation.73, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.19.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.90, bf16[6144,1024]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.6.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.19.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.3 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.20.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.7.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.20.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.88 = bf16[128,1024]{1,0} fusion(f32[] %p.8, f32[128,1024]{1,0} %loop_add_fusion, bf16[128,1024]{1,0} %get-tuple-element.7.0, bf16[128,6144]{1,0} %gemm_fusion_dot.23.0, bf16[1024]{0} %p.21), kind=kCustom, calls=%fused_computation.71, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.21.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.88, bf16[6144,1024]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.8.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.21.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.4 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.22.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.9.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.22.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.86 = bf16[128,1024]{1,0} fusion(f32[] %p.8, bf16[1024]{0} %p.24, f32[128,1024]{1,0} %loop_add_fusion, bf16[128,1024]{1,0} %get-tuple-element.7.0, bf16[128,6144]{1,0} %gemm_fusion_dot.23.0, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.9.0), kind=kCustom, calls=%fused_computation.69, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.23.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.86, bf16[6144,1024]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.10.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.23.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.5 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.24.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.11.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.24.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.84 = bf16[128,1024]{1,0} fusion(f32[] %p.8, bf16[128,1024]{1,0} %get-tuple-element.11.0, bf16[1024]{0} %p.27, f32[128,1024]{1,0} %loop_add_fusion, bf16[128,1024]{1,0} %get-tuple-element.7.0, /*index=5*/bf16[128,6144]{1,0} %gemm_fusion_dot.23.0, bf16[128,1024]{1,0} %get-tuple-element.9.0), kind=kCustom, calls=%fused_computation.67, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.25.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.84, bf16[4096,1024]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.12.0 = bf16[128,4096]{1,0} get-tuple-element((bf16[128,4096]{1,0}, s8[4194304]{0}) %custom-call.25.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_slice = bf16[128,1024]{1,0} fusion(bf16[128,4096]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %triton_softmax.8.0 = f32[128,8,128]{2,1,0} fusion(f32[] %p.8, bf16[128,4096]{1,0} %get-tuple-element.12.0), kind=kCustom, calls=%triton_softmax_computation.8, metadata={op_type="aten__mul" op_name="aten__mul.1634/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %input_concatenate_fusion = bf16[128,8,128]{2,1,0} fusion(f32[128,8,128]{2,1,0} %triton_softmax.8.0, bf16[128]{0} %p.29, bf16[40960,128]{1,0} %p.30, s32[128]{0} %p.31), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.1509.0 = bf16[128,8,128]{2,1,0} bitcast(bf16[128,1024]{1,0} %wrapped_slice)
  %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.32), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  %bitcast.1521.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[69353472]{0} %loop_slice_fusion)
  %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.32), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  ROOT %tuple = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %input_concatenate_fusion, bf16[128,8,128]{2,1,0} %bitcast.1509.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.1521.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
}

ENTRY %SyncTensorsGraph.589 (p0.1.0: bf16[128], p1.4.0: f32[], p2.6.0: bf16[4096,1024], p3.8.0: bf16[1024], p4.24.0: s32[128], p5.26.0: bf16[151936,1024], p6.30.0: bf16[1024,2048], p7.46.0: bf16[1024,3072], p8.48.0: bf16[6144,1024], p9.50.0: bf16[1024], p10.102.0: bf16[1024,2048], p11.118.0: bf16[1024,3072], p12.120.0: bf16[6144,1024], p13.122.0: bf16[1024], p14.174.0: bf16[1024,2048], p15.190.0: bf16[1024,3072], p16.192.0: bf16[6144,1024], p17.194.0: bf16[1024], p18.246.0: bf16[1024,2048], p19.262.0: bf16[1024,3072], p20.264.0: bf16[6144,1024], p21.266.0: bf16[1024], p22.318.0: bf16[1024,2048], p23.334.0: bf16[1024,3072], p24.336.0: bf16[6144,1024], p25.338.0: bf16[1024], p26.390.0: bf16[1024,2048], p27.406.0: bf16[1024,3072], p28.408.0: bf16[6144,1024], p29.410.0: bf16[1024], p30.534.0: s32[128], p31.535.0: bf16[40960,128], p32.579.0: bf16[2,4233,16,8,128]) -> (bf16[128,8,128], bf16[128,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128]) {
  %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %p32.579.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p31.535.0 = bf16[40960,128]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p30.534.0 = s32[128]{0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p29.410.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p28.408.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p27.406.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p26.390.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p25.338.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p24.336.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p23.334.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p22.318.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p21.266.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p20.264.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p19.262.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p18.246.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p17.194.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p16.192.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p15.190.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p14.174.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p13.122.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p12.120.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p11.118.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p10.102.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p9.50.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p8.48.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p7.46.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p6.30.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p5.26.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p4.24.0 = s32[128]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %call = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) call(bf16[151936,1024]{1,0} %p5.26.0, s32[128]{0} %p4.24.0, bf16[1024,2048]{1,0} %p26.390.0, bf16[1024,2048]{1,0} %p22.318.0, bf16[1024,2048]{1,0} %p18.246.0, /*index=5*/bf16[1024,2048]{1,0} %p14.174.0, bf16[1024,2048]{1,0} %p10.102.0, bf16[1024,2048]{1,0} %p6.30.0, f32[] %p1.4.0, bf16[1024]{0} %p9.50.0, /*index=10*/bf16[6144,1024]{1,0} %p8.48.0, bf16[1024,3072]{1,0} %p7.46.0, bf16[1024]{0} %p13.122.0, bf16[6144,1024]{1,0} %p12.120.0, bf16[1024,3072]{1,0} %p11.118.0, /*index=15*/bf16[1024]{0} %p17.194.0, bf16[6144,1024]{1,0} %p16.192.0, bf16[1024,3072]{1,0} %p15.190.0, bf16[1024]{0} %p21.266.0, bf16[6144,1024]{1,0} %p20.264.0, /*index=20*/bf16[1024,3072]{1,0} %p19.262.0, bf16[1024]{0} %p25.338.0, bf16[6144,1024]{1,0} %p24.336.0, bf16[1024,3072]{1,0} %p23.334.0, bf16[1024]{0} %p29.410.0, /*index=25*/bf16[6144,1024]{1,0} %p28.408.0, bf16[1024,3072]{1,0} %p27.406.0, bf16[1024]{0} %p3.8.0, bf16[4096,1024]{1,0} %p2.6.0, bf16[128]{0} %p0.1.0, /*index=30*/bf16[40960,128]{1,0} %p31.535.0, s32[128]{0} %p30.534.0, bf16[2,4233,16,8,128]{4,3,2,1,0} %p32.579.0), to_apply=%command_buffer
  %get-tuple-element.15 = bf16[128,8,128]{2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=0
  %get-tuple-element.16 = bf16[128,8,128]{2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=1
  %get-tuple-element.17 = bf16[4233,16,8,128]{3,2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=2
  %get-tuple-element.18 = bf16[1,4233,16,8,128]{4,3,2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=3
  %bitcast.1514.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[1,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.18)
  ROOT %tuple.588.0 = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %get-tuple-element.15, bf16[128,8,128]{2,1,0} %get-tuple-element.16, bf16[4233,16,8,128]{3,2,1,0} %bitcast.1514.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.17)
}

