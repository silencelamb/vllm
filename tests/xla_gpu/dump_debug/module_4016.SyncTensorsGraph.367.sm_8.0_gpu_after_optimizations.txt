HloModule SyncTensorsGraph.367, is_scheduled=true, entry_computation_layout={(bf16[128]{0}, f32[], bf16[4096,1024]{1,0}, bf16[1024]{0}, s32[128]{0}, /*index=5*/bf16[151936,1024]{1,0}, bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, /*index=10*/bf16[1024,2048]{1,0}, bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, bf16[1024,2048]{1,0}, /*index=15*/bf16[1024,3072]{1,0}, bf16[6144,1024]{1,0}, bf16[1024]{0}, s32[128]{0}, bf16[40960,128]{1,0}, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0})->(bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})}, frontend_attributes={fingerprint_before_lhs="5e8530c378dcaf8c8d3fea55f1212f98"}

%fused_gather (param_0.9: bf16[151936,1024], param_1.175: s32[128]) -> bf16[128,1,1024] {
  %param_0.9 = bf16[151936,1024]{1,0} parameter(0)
  %param_1.175 = s32[128]{0} parameter(1)
  %convert.206.3 = s64[128]{0} convert(s32[128]{0} %param_1.175), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.207.3 = u32[128]{0} convert(s64[128]{0} %convert.206.3), metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.743.1 = u32[128,1]{1,0} bitcast(u32[128]{0} %convert.207.3)
  ROOT %gather.3 = bf16[128,1,1024]{2,0,1} gather(bf16[151936,1024]{1,0} %param_0.9, u32[128,1]{1,0} %bitcast.743.1), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,1024}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%gemm_fusion_dot.11_computation (parameter_0: bf16[1024,2048], parameter_1: bf16[1024,2048], parameter_2: bf16[1024,2048]) -> bf16[128,3072] {
  %parameter_0 = bf16[1024,2048]{1,0} parameter(0)
  %parameter_1 = bf16[1024,2048]{1,0} parameter(1)
  %parameter_2 = bf16[1024,2048]{1,0} parameter(2)
  %concatenate.7 = bf16[3072,2048]{1,0} concatenate(bf16[1024,2048]{1,0} %parameter_0, bf16[1024,2048]{1,0} %parameter_1, bf16[1024,2048]{1,0} %parameter_2), dimensions={0}, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %constant_41 = bf16[] constant(0), metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.43 = bf16[128,2048]{1,0} broadcast(bf16[] %constant_41), dimensions={}, metadata={op_type="aten__expand" op_name="aten__expand" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %dot.13 = bf16[3072,128]{0,1} dot(bf16[3072,2048]{1,0} %concatenate.7, bf16[128,2048]{1,0} %broadcast.43), lhs_contracting_dims={1}, rhs_contracting_dims={1}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %bitcast.363 = bf16[128,3072]{1,0} bitcast(bf16[3072,128]{0,1} %dot.13), metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%AddComputation.52 (x.53: f32[], y.54: f32[]) -> f32[] {
  %y.54 = f32[] parameter(1)
  %x.53 = f32[] parameter(0)
  ROOT %add.51 = f32[] add(f32[] %x.53, f32[] %y.54)
}

%fused_computation.43 (param_0.202: f32[], param_1.171: bf16[128,1,1024], param_2.103: bf16[128,3072], param_3.80: bf16[1024]) -> bf16[128,1024] {
  %param_2.103 = bf16[128,3072]{1,0} parameter(2)
  %convert.205.15 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %param_2.103), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.53.9 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.15), slice={[0:128], [2048:3072]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.171 = bf16[128,1,1024]{2,0,1} parameter(1)
  %bitcast.746.9 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_1.171)
  %convert.208.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.746.9), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.52.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.53.9, f32[128,1024]{1,0} %convert.208.9), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.124 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.52.7, f32[128,1024]{1,0} %add.52.7), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_105 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.11 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.124, f32[] %constant_105), dimensions={1}, to_apply=%AddComputation.52, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_104 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.140 = f32[128]{0} broadcast(f32[] %constant_104), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.123 = f32[128]{0} multiply(f32[128]{0} %reduce.11, f32[128]{0} %broadcast.140), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.202 = f32[] parameter(0)
  %broadcast.139 = f32[128]{0} broadcast(f32[] %param_0.202), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.64 = f32[128]{0} add(f32[128]{0} %multiply.123, f32[128]{0} %broadcast.139), metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.26 = f32[128]{0} rsqrt(f32[128]{0} %add.64), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.138 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.26), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1536/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.122 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.52.7, f32[128,1024]{1,0} %broadcast.138), metadata={op_type="aten__mul" op_name="aten__mul.1536/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.80 = bf16[1024]{0} parameter(3)
  %convert.209.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.80), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.108.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.209.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1537/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.103.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.122, f32[128,1024]{1,0} %broadcast.108.1), metadata={op_type="aten__mul" op_name="aten__mul.1537/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.210.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.103.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert (param_0.189: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.189 = bf16[128,6144]{1,0} parameter(0)
  %slice.54.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.189), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.212.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.54.1)
  %constant_1_4 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.259.2 = f32[] convert(bf16[] %constant_1_4)
  %broadcast.123.24 = f32[128,3072]{1,0} broadcast(f32[] %convert.259.2), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.9.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.212.8)
  %convert.216.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.9.7)
  %exponential.9.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.216.5)
  %convert.217.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.9.3)
  %add.53.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.123.24, f32[128,3072]{1,0} %convert.217.1)
  %divide.9.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.123.24, f32[128,3072]{1,0} %add.53.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.104.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.212.8, f32[128,3072]{1,0} %divide.9.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.55.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.189), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.218.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.55.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.105.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.104.5, f32[128,3072]{1,0} %convert.218.1), metadata={op_type="aten__mul" op_name="aten__mul.1538/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.221.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.105.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.41 (param_0.210: f32[], param_1.177: bf16[1024], param_2.108: bf16[128,1,1024], param_3.89: bf16[128,3072], param_4.49: bf16[128,1024]) -> bf16[128,1024] {
  %param_3.89 = bf16[128,3072]{1,0} parameter(3)
  %convert.205.23 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %param_3.89), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.52.5 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.23), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.49 = bf16[128,1024]{1,0} parameter(4)
  %convert.222.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_4.49), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.53.11 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.23), slice={[0:128], [2048:3072]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.108 = bf16[128,1,1024]{2,0,1} parameter(2)
  %bitcast.746.11 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_2.108)
  %convert.208.11 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.746.11), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.52.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.53.11, f32[128,1024]{1,0} %convert.208.11), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.54.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.222.5, f32[128,1024]{1,0} %add.52.9), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.56.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.52.5, f32[128,1024]{1,0} %add.54.5), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.130 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.56.3, f32[128,1024]{1,0} %add.56.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_110 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.13 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.130, f32[] %constant_110), dimensions={1}, to_apply=%AddComputation.52, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_109 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.148 = f32[128]{0} broadcast(f32[] %constant_109), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.129 = f32[128]{0} multiply(f32[128]{0} %reduce.13, f32[128]{0} %broadcast.148), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.210 = f32[] parameter(0)
  %broadcast.147 = f32[128]{0} broadcast(f32[] %param_0.210), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.66 = f32[128]{0} add(f32[128]{0} %multiply.129, f32[128]{0} %broadcast.147), metadata={op_type="aten__add" op_name="aten__add.1553/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.28 = f32[128]{0} rsqrt(f32[128]{0} %add.66), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.146 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.28), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1554/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.128 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.56.3, f32[128,1024]{1,0} %broadcast.146), metadata={op_type="aten__mul" op_name="aten__mul.1554/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.177 = bf16[1024]{0} parameter(1)
  %convert.224.1 = f32[1024]{0} convert(bf16[1024]{0} %param_1.177), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.111.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.224.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1555/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.106.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.128, f32[128,1024]{1,0} %broadcast.111.1), metadata={op_type="aten__mul" op_name="aten__mul.1555/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.225.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.106.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.1 (param_0.190: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.190 = bf16[128,6144]{1,0} parameter(0)
  %slice.56.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.190), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.227.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.56.1)
  %constant_1_1 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.259.3 = f32[] convert(bf16[] %constant_1_1)
  %broadcast.123.20 = f32[128,3072]{1,0} broadcast(f32[] %convert.259.3), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.10.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.227.8)
  %convert.233.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.10.7)
  %exponential.10.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.233.5)
  %convert.235.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.10.3)
  %add.57.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.123.20, f32[128,3072]{1,0} %convert.235.1)
  %divide.10.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.123.20, f32[128,3072]{1,0} %add.57.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.107.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.227.8, f32[128,3072]{1,0} %divide.10.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.57.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.190), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.236.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.57.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.109.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.107.5, f32[128,3072]{1,0} %convert.236.1), metadata={op_type="aten__mul" op_name="aten__mul.1556/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.237.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.109.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.39 (param_0.200: f32[], param_1.180: bf16[128,1024], param_2.112: bf16[128,3072], param_3.91: bf16[1024], param_4.52: bf16[128,1,1024], param_5.26: bf16[128,1024]) -> bf16[128,1024] {
  %param_2.112 = bf16[128,3072]{1,0} parameter(2)
  %convert.205.17 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %param_2.112), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.51.5 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.17), slice={[0:128], [0:1024]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_1.180 = bf16[128,1024]{1,0} parameter(1)
  %convert.238.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_1.180), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.52.9 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.17), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.26 = bf16[128,1024]{1,0} parameter(5)
  %convert.222.9 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_5.26), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.53.15 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.17), slice={[0:128], [2048:3072]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.52 = bf16[128,1,1024]{2,0,1} parameter(4)
  %bitcast.746.15 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_4.52)
  %convert.208.15 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.746.15), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.52.13 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.53.15, f32[128,1024]{1,0} %convert.208.15), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.54.9 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.222.9, f32[128,1024]{1,0} %add.52.13), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.56.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.52.9, f32[128,1024]{1,0} %add.54.9), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.58.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.238.5, f32[128,1024]{1,0} %add.56.7), metadata={op_type="aten__add" op_name="aten__add.1557/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.59.3 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.51.5, f32[128,1024]{1,0} %add.58.5), metadata={op_type="aten__add" op_name="aten__add.1570/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.136 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.59.3, f32[128,1024]{1,0} %add.59.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_115 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.15 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.136, f32[] %constant_115), dimensions={1}, to_apply=%AddComputation.52, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_114 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.155 = f32[128]{0} broadcast(f32[] %constant_114), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.135 = f32[128]{0} multiply(f32[128]{0} %reduce.15, f32[128]{0} %broadcast.155), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.200 = f32[] parameter(0)
  %broadcast.154 = f32[128]{0} broadcast(f32[] %param_0.200), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.68 = f32[128]{0} add(f32[128]{0} %multiply.135, f32[128]{0} %broadcast.154), metadata={op_type="aten__add" op_name="aten__add.1571/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.30 = f32[128]{0} rsqrt(f32[128]{0} %add.68), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.153 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.30), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1572/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.134 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.59.3, f32[128,1024]{1,0} %broadcast.153), metadata={op_type="aten__mul" op_name="aten__mul.1572/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.91 = bf16[1024]{0} parameter(3)
  %convert.239.1 = f32[1024]{0} convert(bf16[1024]{0} %param_3.91), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.114.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.239.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1573/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.110.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.134, f32[128,1024]{1,0} %broadcast.114.1), metadata={op_type="aten__mul" op_name="aten__mul.1573/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.240.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.110.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_convert.2 (param_0.188: bf16[128,6144]) -> bf16[128,3072] {
  %param_0.188 = bf16[128,6144]{1,0} parameter(0)
  %slice.58.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.188), slice={[0:128], [0:3072]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.241.8 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.58.1)
  %constant_1_3 = bf16[] constant(1), metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.259.1 = f32[] convert(bf16[] %constant_1_3)
  %broadcast.123.16 = f32[128,3072]{1,0} broadcast(f32[] %convert.259.1), dimensions={}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %negate.11.7 = f32[128,3072]{1,0} negate(f32[128,3072]{1,0} %convert.241.8)
  %convert.245.5 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %negate.11.7)
  %exponential.11.3 = bf16[128,3072]{1,0} exponential(bf16[128,3072]{1,0} %convert.245.5)
  %convert.246.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %exponential.11.3)
  %add.60.5 = f32[128,3072]{1,0} add(f32[128,3072]{1,0} %broadcast.123.16, f32[128,3072]{1,0} %convert.246.1)
  %divide.11.3 = f32[128,3072]{1,0} divide(f32[128,3072]{1,0} %broadcast.123.16, f32[128,3072]{1,0} %add.60.5), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.111.5 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %convert.241.8, f32[128,3072]{1,0} %divide.11.3), metadata={op_type="aten__silu" op_name="aten__silu" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.59.1 = bf16[128,3072]{1,0} slice(bf16[128,6144]{1,0} %param_0.188), slice={[0:128], [3072:6144]}, metadata={op_type="xla__select" op_name="xla__select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.247.1 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %slice.59.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.112.3 = f32[128,3072]{1,0} multiply(f32[128,3072]{1,0} %multiply.111.5, f32[128,3072]{1,0} %convert.247.1), metadata={op_type="aten__mul" op_name="aten__mul.1574/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.248.1 = bf16[128,3072]{1,0} convert(f32[128,3072]{1,0} %multiply.112.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_computation.37 (param_0.199: f32[], param_1.168: bf16[128,1024], param_2.110: bf16[128,1024], param_3.90: bf16[128,3072], param_4.50: bf16[1024], param_5.22: bf16[128,1,1024], param_6.4: bf16[128,1024]) -> bf16[128,1024] {
  %param_1.168 = bf16[128,1024]{1,0} parameter(1)
  %convert.249.5 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_1.168), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_3.90 = bf16[128,3072]{1,0} parameter(3)
  %convert.205.19 = f32[128,3072]{1,0} convert(bf16[128,3072]{1,0} %param_3.90), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.51.7 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.19), slice={[0:128], [0:1024]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.110 = bf16[128,1024]{1,0} parameter(2)
  %convert.238.7 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_2.110), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.52.7 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.19), slice={[0:128], [1024:2048]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_6.4 = bf16[128,1024]{1,0} parameter(6)
  %convert.222.7 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %param_6.4), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.53.13 = f32[128,1024]{1,0} slice(f32[128,3072]{1,0} %convert.205.19), slice={[0:128], [2048:3072]}, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_5.22 = bf16[128,1,1024]{2,0,1} parameter(5)
  %bitcast.746.13 = bf16[128,1024]{1,0} bitcast(bf16[128,1,1024]{2,0,1} %param_5.22)
  %convert.208.13 = f32[128,1024]{1,0} convert(bf16[128,1024]{1,0} %bitcast.746.13), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.52.11 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.53.13, f32[128,1024]{1,0} %convert.208.13), metadata={op_type="aten__add" op_name="aten__add.1534/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.54.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.222.7, f32[128,1024]{1,0} %add.52.11), metadata={op_type="aten__add" op_name="aten__add.1539/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.56.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.52.7, f32[128,1024]{1,0} %add.54.7), metadata={op_type="aten__add" op_name="aten__add.1552/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.58.7 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.238.7, f32[128,1024]{1,0} %add.56.5), metadata={op_type="aten__add" op_name="aten__add.1557/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.59.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %slice.51.7, f32[128,1024]{1,0} %add.58.7), metadata={op_type="aten__add" op_name="aten__add.1570/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.61.5 = f32[128,1024]{1,0} add(f32[128,1024]{1,0} %convert.249.5, f32[128,1024]{1,0} %add.59.5), metadata={op_type="aten__add" op_name="aten__add.1575/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.144 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.61.5, f32[128,1024]{1,0} %add.61.5), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_124 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.17 = f32[128]{0} reduce(f32[128,1024]{1,0} %multiply.144, f32[] %constant_124), dimensions={1}, to_apply=%AddComputation.52, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_121 = f32[] constant(0.0009765625), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.161 = f32[128]{0} broadcast(f32[] %constant_121), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.143 = f32[128]{0} multiply(f32[128]{0} %reduce.17, f32[128]{0} %broadcast.161), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.199 = f32[] parameter(0)
  %broadcast.160 = f32[128]{0} broadcast(f32[] %param_0.199), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1535/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.71 = f32[128]{0} add(f32[128]{0} %multiply.143, f32[128]{0} %broadcast.160), metadata={op_type="aten__add" op_name="aten__add.1576/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.32 = f32[128]{0} rsqrt(f32[128]{0} %add.71), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.159 = f32[128,1024]{1,0} broadcast(f32[128]{0} %rsqrt.32), dimensions={0}, metadata={op_type="aten__mul" op_name="aten__mul.1577/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.142 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %add.61.5, f32[128,1024]{1,0} %broadcast.159), metadata={op_type="aten__mul" op_name="aten__mul.1577/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_4.50 = bf16[1024]{0} parameter(4)
  %convert.251.1 = f32[1024]{0} convert(bf16[1024]{0} %param_4.50), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.117.1 = f32[128,1024]{1,0} broadcast(f32[1024]{0} %convert.251.1), dimensions={1}, metadata={op_type="aten__mul" op_name="aten__mul.1578/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.113.1 = f32[128,1024]{1,0} multiply(f32[128,1024]{1,0} %multiply.142, f32[128,1024]{1,0} %broadcast.117.1), metadata={op_type="aten__mul" op_name="aten__mul.1578/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %convert.252.1 = bf16[128,1024]{1,0} convert(f32[128,1024]{1,0} %multiply.113.1), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%wrapped_slice_computation (param_0.214: bf16[128,4096]) -> bf16[128,1024] {
  %param_0.214 = bf16[128,4096]{1,0} parameter(0)
  ROOT %slice.65.1 = bf16[128,1024]{1,0} slice(bf16[128,4096]{1,0} %param_0.214), slice={[0:128], [3072:4096]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%triton_softmax_computation.5 (param_0.136: f32[], param_1.147: bf16[128,4096]) -> f32[128,8,128] {
  %param_1.147 = bf16[128,4096]{1,0} parameter(1)
  %slice.60.1 = bf16[128,1024]{1,0} slice(bf16[128,4096]{1,0} %param_1.147), slice={[0:128], [2048:3072]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.898.3 = bf16[128,8,128]{2,1,0} bitcast(bf16[128,1024]{1,0} %slice.60.1)
  %convert.253.3 = f32[128,8,128]{2,1,0} convert(bf16[128,8,128]{2,1,0} %bitcast.898.3), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.86 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %convert.253.3, f32[128,8,128]{2,1,0} %convert.253.3), metadata={op_type="aten__pow" op_name="aten__pow" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_56 = f32[] constant(0), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %reduce.5 = f32[128,8]{1,0} reduce(f32[128,8,128]{2,1,0} %multiply.86, f32[] %constant_56), dimensions={2}, to_apply=%AddComputation.52, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %constant_60 = f32[] constant(0.0078125), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.89 = f32[128,8]{1,0} broadcast(f32[] %constant_60), dimensions={}, metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.88 = f32[128,8]{1,0} multiply(f32[128,8]{1,0} %reduce.5, f32[128,8]{1,0} %broadcast.89), metadata={op_type="aten__mean" op_name="aten__mean" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_0.136 = f32[] parameter(0)
  %broadcast.90 = f32[128,8]{1,0} broadcast(f32[] %param_0.136), dimensions={}, metadata={op_type="aten__add" op_name="aten__add.1579/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.46 = f32[128,8]{1,0} add(f32[128,8]{1,0} %multiply.88, f32[128,8]{1,0} %broadcast.90), metadata={op_type="aten__add" op_name="aten__add.1579/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %rsqrt.20 = f32[128,8]{1,0} rsqrt(f32[128,8]{1,0} %add.46), metadata={op_type="aten__rsqrt" op_name="aten__rsqrt" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.92 = f32[128,8,128]{2,1,0} broadcast(f32[128,8]{1,0} %rsqrt.20), dimensions={0,1}, metadata={op_type="aten__mul" op_name="aten__mul.1580/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  ROOT %multiply.89 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %convert.253.3, f32[128,8,128]{2,1,0} %broadcast.92), metadata={op_type="aten__mul" op_name="aten__mul.1580/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_concatenate (param_0.205: f32[128,8,128], param_1.182: bf16[128], param_2.113: bf16[40960,128], param_3.93: s32[128]) -> bf16[128,8,128] {
  %param_0.205 = f32[128,8,128]{2,1,0} parameter(0)
  %param_1.182 = bf16[128]{0} parameter(1)
  %convert.254.1 = f32[128]{0} convert(bf16[128]{0} %param_1.182), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.118.18 = f32[128,8,128]{2,1,0} broadcast(f32[128]{0} %convert.254.1), dimensions={2}, metadata={op_type="aten__mul" op_name="aten__mul.1581/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.114.18 = f32[128,8,128]{2,1,0} multiply(f32[128,8,128]{2,1,0} %param_0.205, f32[128,8,128]{2,1,0} %broadcast.118.18), metadata={op_type="aten__mul" op_name="aten__mul.1581/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.61.9 = f32[128,8,64]{2,1,0} slice(f32[128,8,128]{2,1,0} %multiply.114.18), slice={[0:128], [0:8], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %param_2.113 = bf16[40960,128]{1,0} parameter(2)
  %param_3.93 = s32[128]{0} parameter(3)
  %bitcast.914.3 = s32[128,1]{1,0} bitcast(s32[128]{0} %param_3.93)
  %gather.1.3 = bf16[128,1,128]{2,0,1} gather(bf16[40960,128]{1,0} %param_2.113, s32[128,1]{1,0} %bitcast.914.3), offset_dims={1,2}, collapsed_slice_dims={}, start_index_map={0}, index_vector_dim=1, slice_sizes={1,128}, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.917.7 = bf16[128,128]{1,0} bitcast(bf16[128,1,128]{2,0,1} %gather.1.3)
  %convert.255.7 = f32[128,128]{1,0} convert(bf16[128,128]{1,0} %bitcast.917.7), metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.62.3 = f32[128,64]{1,0} slice(f32[128,128]{1,0} %convert.255.7), slice={[0:128], [0:64]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.119.14 = f32[128,8,64]{2,1,0} broadcast(f32[128,64]{1,0} %slice.62.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.1582/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.115.7 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.61.9, f32[128,8,64]{2,1,0} %broadcast.119.14), metadata={op_type="aten__mul" op_name="aten__mul.1582/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.63.9 = f32[128,8,64]{2,1,0} slice(f32[128,8,128]{2,1,0} %multiply.114.18), slice={[0:128], [0:8], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %slice.64.3 = f32[128,64]{1,0} slice(f32[128,128]{1,0} %convert.255.7), slice={[0:128], [64:128]}, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %broadcast.121.10 = f32[128,8,64]{2,1,0} broadcast(f32[128,64]{1,0} %slice.64.3), dimensions={0,2}, metadata={op_type="aten__mul" op_name="aten__mul.1583/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.116.5 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.63.9, f32[128,8,64]{2,1,0} %broadcast.121.10), metadata={op_type="aten__mul" op_name="aten__mul.1583/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %subtract.2.5 = f32[128,8,64]{2,1,0} subtract(f32[128,8,64]{2,1,0} %multiply.115.7, f32[128,8,64]{2,1,0} %multiply.116.5), metadata={op_type="aten__sub" op_name="aten__sub.1584/aten__sub" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.256.3 = bf16[128,8,64]{2,1,0} convert(f32[128,8,64]{2,1,0} %subtract.2.5)
  %multiply.117.7 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.63.9, f32[128,8,64]{2,1,0} %broadcast.119.14), metadata={op_type="aten__mul" op_name="aten__mul.1585/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %multiply.118.5 = f32[128,8,64]{2,1,0} multiply(f32[128,8,64]{2,1,0} %slice.61.9, f32[128,8,64]{2,1,0} %broadcast.121.10), metadata={op_type="aten__mul" op_name="aten__mul.1586/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %add.62.5 = f32[128,8,64]{2,1,0} add(f32[128,8,64]{2,1,0} %multiply.117.7, f32[128,8,64]{2,1,0} %multiply.118.5), metadata={op_type="aten__add" op_name="aten__add.1587/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %convert.258.3 = bf16[128,8,64]{2,1,0} convert(f32[128,8,64]{2,1,0} %add.62.5)
  ROOT %concatenate.9.1 = bf16[128,8,128]{2,1,0} concatenate(bf16[128,8,64]{2,1,0} %convert.256.3, bf16[128,8,64]{2,1,0} %convert.258.3), dimensions={2}, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
}

%fused_slice (param_0.1: bf16[2,4233,16,8,128]) -> bf16[69353472] {
  %param_0.1 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  %bitcast.958.1 = bf16[138706944]{0} bitcast(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.1)
  ROOT %slice.67.1 = bf16[69353472]{0} slice(bf16[138706944]{0} %bitcast.958.1), slice={[69353472:138706944]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%wrapped_slice_computation.1 (param_0.215: bf16[2,4233,16,8,128]) -> bf16[1,4233,16,8,128] {
  %param_0.215 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(0)
  ROOT %slice.66.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} slice(bf16[2,4233,16,8,128]{4,3,2,1,0} %param_0.215), slice={[0:1], [0:4233], [0:16], [0:8], [0:128]}, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
}

%command_buffer (p: bf16[151936,1024], p.1: s32[128], p.2: bf16[1024,2048], p.3: bf16[1024,2048], p.4: bf16[1024,2048], p.5: f32[], p.6: bf16[1024], p.7: bf16[6144,1024], p.8: bf16[1024,3072], p.9: bf16[1024], p.10: bf16[6144,1024], p.11: bf16[1024,3072], p.12: bf16[1024], p.13: bf16[6144,1024], p.14: bf16[1024,3072], p.15: bf16[1024], p.16: bf16[4096,1024], p.17: bf16[128], p.18: bf16[40960,128], p.19: s32[128], p.20: bf16[2,4233,16,8,128]) -> (bf16[128,8,128], bf16[128,8,128], bf16[4233,16,8,128], bf16[1,4233,16,8,128]) {
  %p = bf16[151936,1024]{1,0} parameter(0)
  %p.1 = s32[128]{0} parameter(1)
  %p.2 = bf16[1024,2048]{1,0} parameter(2)
  %p.3 = bf16[1024,2048]{1,0} parameter(3)
  %p.4 = bf16[1024,2048]{1,0} parameter(4)
  %p.5 = f32[] parameter(5)
  %p.6 = bf16[1024]{0} parameter(6)
  %p.7 = bf16[6144,1024]{1,0} parameter(7)
  %p.8 = bf16[1024,3072]{1,0} parameter(8)
  %p.9 = bf16[1024]{0} parameter(9)
  %p.10 = bf16[6144,1024]{1,0} parameter(10)
  %p.11 = bf16[1024,3072]{1,0} parameter(11)
  %p.12 = bf16[1024]{0} parameter(12)
  %p.13 = bf16[6144,1024]{1,0} parameter(13)
  %p.14 = bf16[1024,3072]{1,0} parameter(14)
  %p.15 = bf16[1024]{0} parameter(15)
  %p.16 = bf16[4096,1024]{1,0} parameter(16)
  %p.17 = bf16[128]{0} parameter(17)
  %p.18 = bf16[40960,128]{1,0} parameter(18)
  %p.19 = s32[128]{0} parameter(19)
  %p.20 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(20)
  %loop_gather_fusion = bf16[128,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[128]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %gemm_fusion_dot.11.0 = bf16[128,3072]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4), kind=kCustom, calls=%gemm_fusion_dot.11_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"32","block_n":"128","block_k":"32","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
  %fusion.54 = bf16[128,1024]{1,0} fusion(f32[] %p.5, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,3072]{1,0} %gemm_fusion_dot.11.0, bf16[1024]{0} %p.6), kind=kCustom, calls=%fused_computation.43, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.7.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.54, bf16[6144,1024]{1,0} %p.7), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.7 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.7.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.7), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.8.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.8), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.1.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.8.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.52 = bf16[128,1024]{1,0} fusion(f32[] %p.5, bf16[1024]{0} %p.9, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,3072]{1,0} %gemm_fusion_dot.11.0, bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.41, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.9.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.52, bf16[6144,1024]{1,0} %p.10), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.2.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.9.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.1 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.10.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.3.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.10.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.50 = bf16[128,1024]{1,0} fusion(f32[] %p.5, bf16[128,1024]{1,0} %get-tuple-element.3.0, bf16[128,3072]{1,0} %gemm_fusion_dot.11.0, bf16[1024]{0} %p.12, bf16[128,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.39, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.11.0 = (bf16[128,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.50, bf16[6144,1024]{1,0} %p.13), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.4.0 = bf16[128,6144]{1,0} get-tuple-element((bf16[128,6144]{1,0}, s8[4194304]{0}) %custom-call.11.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %loop_convert_fusion.2 = bf16[128,3072]{1,0} fusion(bf16[128,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
  %custom-call.12.0 = (bf16[128,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[128,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"393216","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.5.0 = bf16[128,1024]{1,0} get-tuple-element((bf16[128,1024]{1,0}, s8[4194304]{0}) %custom-call.12.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %fusion.48 = bf16[128,1024]{1,0} fusion(f32[] %p.5, bf16[128,1024]{1,0} %get-tuple-element.5.0, bf16[128,1024]{1,0} %get-tuple-element.3.0, bf16[128,3072]{1,0} %gemm_fusion_dot.11.0, bf16[1024]{0} %p.15, /*index=5*/bf16[128,1,1024]{2,0,1} %loop_gather_fusion, bf16[128,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.37, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %custom-call.13.0 = (bf16[128,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[128,1024]{1,0} %fusion.48, bf16[4096,1024]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"131072","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
  %get-tuple-element.6.0 = bf16[128,4096]{1,0} get-tuple-element((bf16[128,4096]{1,0}, s8[4194304]{0}) %custom-call.13.0), index=0, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %wrapped_slice = bf16[128,1024]{1,0} fusion(bf16[128,4096]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %triton_softmax.5.0 = f32[128,8,128]{2,1,0} fusion(f32[] %p.5, bf16[128,4096]{1,0} %get-tuple-element.6.0), kind=kCustom, calls=%triton_softmax_computation.5, metadata={op_type="aten__mul" op_name="aten__mul.1580/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
  %input_concatenate_fusion = bf16[128,8,128]{2,1,0} fusion(f32[128,8,128]{2,1,0} %triton_softmax.5.0, bf16[128]{0} %p.17, bf16[40960,128]{1,0} %p.18, s32[128]{0} %p.19), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %bitcast.950.0 = bf16[128,8,128]{2,1,0} bitcast(bf16[128,1024]{1,0} %wrapped_slice)
  %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.20), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  %bitcast.962.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[69353472]{0} %loop_slice_fusion)
  %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.20), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
  ROOT %tuple = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %input_concatenate_fusion, bf16[128,8,128]{2,1,0} %bitcast.950.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.962.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
}

ENTRY %SyncTensorsGraph.367 (p0.1.0: bf16[128], p1.4.0: f32[], p2.6.0: bf16[4096,1024], p3.8.0: bf16[1024], p4.18.0: s32[128], p5.20.0: bf16[151936,1024], p6.24.0: bf16[1024,2048], p7.40.0: bf16[1024,3072], p8.42.0: bf16[6144,1024], p9.44.0: bf16[1024], p10.96.0: bf16[1024,2048], p11.112.0: bf16[1024,3072], p12.114.0: bf16[6144,1024], p13.116.0: bf16[1024], p14.168.0: bf16[1024,2048], p15.184.0: bf16[1024,3072], p16.186.0: bf16[6144,1024], p17.188.0: bf16[1024], p18.312.0: s32[128], p19.313.0: bf16[40960,128], p20.357.0: bf16[2,4233,16,8,128]) -> (bf16[128,8,128], bf16[128,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128]) {
  %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
  %p20.357.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p19.313.0 = bf16[40960,128]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p18.312.0 = s32[128]{0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p17.188.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p16.186.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p15.184.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p14.168.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p13.116.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p12.114.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p11.112.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p10.96.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p9.44.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p8.42.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p7.40.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p6.24.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p5.20.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p4.18.0 = s32[128]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
  %call = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) call(bf16[151936,1024]{1,0} %p5.20.0, s32[128]{0} %p4.18.0, bf16[1024,2048]{1,0} %p14.168.0, bf16[1024,2048]{1,0} %p10.96.0, bf16[1024,2048]{1,0} %p6.24.0, /*index=5*/f32[] %p1.4.0, bf16[1024]{0} %p9.44.0, bf16[6144,1024]{1,0} %p8.42.0, bf16[1024,3072]{1,0} %p7.40.0, bf16[1024]{0} %p13.116.0, /*index=10*/bf16[6144,1024]{1,0} %p12.114.0, bf16[1024,3072]{1,0} %p11.112.0, bf16[1024]{0} %p17.188.0, bf16[6144,1024]{1,0} %p16.186.0, bf16[1024,3072]{1,0} %p15.184.0, /*index=15*/bf16[1024]{0} %p3.8.0, bf16[4096,1024]{1,0} %p2.6.0, bf16[128]{0} %p0.1.0, bf16[40960,128]{1,0} %p19.313.0, s32[128]{0} %p18.312.0, /*index=20*/bf16[2,4233,16,8,128]{4,3,2,1,0} %p20.357.0), to_apply=%command_buffer
  %get-tuple-element.9 = bf16[128,8,128]{2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=0
  %get-tuple-element.10 = bf16[128,8,128]{2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=1
  %get-tuple-element.11 = bf16[4233,16,8,128]{3,2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=2
  %get-tuple-element.12 = bf16[1,4233,16,8,128]{4,3,2,1,0} get-tuple-element((bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) %call), index=3
  %bitcast.955.0 = bf16[4233,16,8,128]{3,2,1,0} bitcast(bf16[1,4233,16,8,128]{4,3,2,1,0} %get-tuple-element.12)
  ROOT %tuple.366.0 = (bf16[128,8,128]{2,1,0}, bf16[128,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[128,8,128]{2,1,0} %get-tuple-element.9, bf16[128,8,128]{2,1,0} %get-tuple-element.10, bf16[4233,16,8,128]{3,2,1,0} %bitcast.955.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.11)
}

