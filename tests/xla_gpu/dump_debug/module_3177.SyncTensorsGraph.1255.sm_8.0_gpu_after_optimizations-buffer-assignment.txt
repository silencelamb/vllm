BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <980 p5.44.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 68, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1048 p68.1245.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <845 gemm_fusion_dot.60.0 @0> (size=1966080,offset=0): bf16[64,15360]{1,0}
 value: <972 custom-call.61.0{0} @0> (size=524288,offset=0): bf16[64,4096]{1,0}
 value: <977 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <844 wrapped_concatenate @0> (size=62914560,offset=0): bf16[15360,2048]{1,0}
 value: <850 custom-call.31.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <854 custom-call.32.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <858 custom-call.33.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <862 custom-call.34.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <866 custom-call.35.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <870 custom-call.36.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <874 custom-call.37.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <878 custom-call.38.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <883 custom-call.39.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <887 custom-call.40.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <891 custom-call.41.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <895 custom-call.42.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <899 custom-call.43.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <903 custom-call.44.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <907 custom-call.45.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <911 custom-call.46.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <916 custom-call.47.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <920 custom-call.48.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <924 custom-call.49.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <928 custom-call.50.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <932 custom-call.51.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <936 custom-call.52.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <940 custom-call.53.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <944 custom-call.54.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <949 custom-call.55.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <953 custom-call.56.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <957 custom-call.57.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <961 custom-call.58.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <965 custom-call.59.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <969 custom-call.60.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <973 custom-call.61.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <974 triton_softmax.17.0 @0> (size=262144,offset=0): f32[64,8,128]{2,1,0}
 value: <978 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <999 p8.66.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1002 p12.138.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1005 p16.210.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 12582912, parameter 20, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1008 p20.282.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 8: size 12582912, parameter 24, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1011 p24.354.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 9: size 12582912, parameter 28, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1014 p28.426.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 10: size 12582912, parameter 32, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1017 p32.498.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 11: size 12582912, parameter 36, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1020 p36.570.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 12: size 12582912, parameter 40, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1023 p40.642.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 13: size 12582912, parameter 44, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1026 p44.714.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 14: size 12582912, parameter 48, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1029 p48.786.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 15: size 12582912, parameter 52, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1032 p52.858.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 16: size 12582912, parameter 56, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1035 p56.930.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 17: size 12582912, parameter 60, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1038 p60.1002.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 18: size 12582912, parameter 64, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1041 p64.1074.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 19: size 10485760, parameter 67, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <1046 p67.1201.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 20: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <1044 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 21: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1000 p7.64.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 22: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1003 p11.136.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 23: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1006 p15.208.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 24: size 6291456, parameter 19, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1009 p19.280.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 25: size 6291456, parameter 23, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1012 p23.352.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 26: size 6291456, parameter 27, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1015 p27.424.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 27: size 6291456, parameter 31, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1018 p31.496.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 28: size 6291456, parameter 35, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1021 p35.568.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 29: size 6291456, parameter 39, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1024 p39.640.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 30: size 6291456, parameter 43, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1027 p43.712.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 31: size 6291456, parameter 47, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1030 p47.784.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 32: size 6291456, parameter 51, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1033 p51.856.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 33: size 6291456, parameter 55, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1036 p55.928.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 34: size 6291456, parameter 59, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1039 p59.1000.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 35: size 6291456, parameter 63, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1042 p63.1072.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 36: size 4194304, parameter 62, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <982 p62.1056.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 37: size 4194304, parameter 58, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <983 p58.984.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 38: size 4194304, parameter 54, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <984 p54.912.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 39: size 4194304, parameter 50, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <985 p50.840.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 40: size 4194304, parameter 46, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <986 p46.768.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 41: size 4194304, parameter 42, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <987 p42.696.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 42: size 4194304, parameter 38, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <988 p38.624.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 43: size 4194304, parameter 34, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <989 p34.552.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 44: size 4194304, parameter 30, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <990 p30.480.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 45: size 4194304, parameter 26, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <991 p26.408.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 46: size 4194304, parameter 22, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <992 p22.336.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 47: size 4194304, parameter 18, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <993 p18.264.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 48: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <994 p14.192.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 49: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <995 p10.120.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 50: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <996 p6.48.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 51: size 131072, maybe-live-out:
 value: <847 fusion.195 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <853 custom-call.32.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <888 fusion.190 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <894 custom-call.42.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <921 fusion.186 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <927 custom-call.50.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <954 fusion.182 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <960 custom-call.58.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <975 input_concatenate_fusion @0> (size=131072,offset=0): bf16[64,8,128]{2,1,0}
allocation 52: size 131072, maybe-live-out:
 value: <846 loop_gather_fusion @0> (size=131072,offset=0): bf16[64,1,1024]{2,0,1}
 value: <880 fusion.191 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <886 custom-call.40.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <913 fusion.187 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <919 custom-call.48.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <946 fusion.183 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <952 custom-call.56.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <976 wrapped_slice @0> (size=131072,offset=0): bf16[64,1024]{1,0}
allocation 53: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <998 p9.68.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 54: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <1001 p13.140.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 55: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <1004 p17.212.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 56: size 2048, parameter 21, shape |bf16[1024]| at ShapeIndex {}:
 value: <1007 p21.284.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 57: size 2048, parameter 25, shape |bf16[1024]| at ShapeIndex {}:
 value: <1010 p25.356.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 58: size 2048, parameter 29, shape |bf16[1024]| at ShapeIndex {}:
 value: <1013 p29.428.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 59: size 2048, parameter 33, shape |bf16[1024]| at ShapeIndex {}:
 value: <1016 p33.500.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 60: size 2048, parameter 37, shape |bf16[1024]| at ShapeIndex {}:
 value: <1019 p37.572.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 61: size 2048, parameter 41, shape |bf16[1024]| at ShapeIndex {}:
 value: <1022 p41.644.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 62: size 2048, parameter 45, shape |bf16[1024]| at ShapeIndex {}:
 value: <1025 p45.716.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 63: size 2048, parameter 49, shape |bf16[1024]| at ShapeIndex {}:
 value: <1028 p49.788.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 64: size 2048, parameter 53, shape |bf16[1024]| at ShapeIndex {}:
 value: <1031 p53.860.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 65: size 2048, parameter 57, shape |bf16[1024]| at ShapeIndex {}:
 value: <1034 p57.932.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 66: size 2048, parameter 61, shape |bf16[1024]| at ShapeIndex {}:
 value: <1037 p61.1004.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 67: size 2048, parameter 65, shape |bf16[1024]| at ShapeIndex {}:
 value: <1040 p65.1076.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 68: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <1043 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 69: size 256, parameter 4, shape |s32[64]| at ShapeIndex {}:
 value: <981 p4.42.0 @0> (size=256,offset=0): s32[64]{0}
allocation 70: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <1045 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 71: size 256, parameter 66, shape |s32[64]| at ShapeIndex {}:
 value: <1047 p66.1200.0 @0> (size=256,offset=0): s32[64]{0}
allocation 72: size 32, output shape is |(bf16[64,8,128], bf16[64,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <1049 tuple.1254.0{} @0> (size=32,offset=0): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 73: size 4, thread-local:
 value: <27 add.202 @0> (size=4,offset=0): f32[]
allocation 74: size 4, thread-local:
 value: <26 y.78 @0> (size=4,offset=0): f32[]
allocation 75: size 4, thread-local:
 value: <25 x.77 @0> (size=4,offset=0): f32[]
allocation 76: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <997 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 77: size 1707936, preallocated-temp:
 value: <848 custom-call.31.0{} @0> (size=16,offset=1707776): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <849 custom-call.31.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <851 loop_convert_fusion.14 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <852 custom-call.32.0{} @0> (size=16,offset=1707648): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <855 fusion.194 @0> (size=131072,offset=790144): bf16[64,1024]{1,0}
 value: <856 custom-call.33.0{} @0> (size=16,offset=3584): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <857 custom-call.33.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <859 loop_convert_fusion.13 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <860 custom-call.34.0{} @0> (size=16,offset=0): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <861 custom-call.34.0{0} @0> (size=131072,offset=1445504): bf16[64,1024]{1,0}
 value: <863 fusion.193 @0> (size=131072,offset=790144): bf16[64,1024]{1,0}
 value: <864 custom-call.35.0{} @0> (size=16,offset=128): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <865 custom-call.35.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <867 loop_convert_fusion.12 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <868 custom-call.36.0{} @0> (size=16,offset=256): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <869 custom-call.36.0{0} @0> (size=131072,offset=1576576): bf16[64,1024]{1,0}
 value: <871 fusion.192 @0> (size=131072,offset=790144): bf16[64,1024]{1,0}
 value: <872 custom-call.37.0{} @0> (size=16,offset=384): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <873 custom-call.37.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <875 loop_convert_fusion.11 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <876 custom-call.38.0{} @0> (size=16,offset=512): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <877 custom-call.38.0{0} @0> (size=131072,offset=3712): bf16[64,1024]{1,0}
 value: <879 loop_add_fusion @0> (size=262144,offset=1183360): f32[64,1024]{1,0}
 value: <881 custom-call.39.0{} @0> (size=16,offset=640): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <882 custom-call.39.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <884 loop_convert_fusion.10 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <885 custom-call.40.0{} @0> (size=16,offset=768): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <889 custom-call.41.0{} @0> (size=16,offset=896): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <890 custom-call.41.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <892 loop_convert_fusion.9 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <893 custom-call.42.0{} @0> (size=16,offset=1024): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <896 fusion.189 @0> (size=131072,offset=790144): bf16[64,1024]{1,0}
 value: <897 custom-call.43.0{} @0> (size=16,offset=1152): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <898 custom-call.43.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <900 loop_convert_fusion.8 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <901 custom-call.44.0{} @0> (size=16,offset=1280): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <902 custom-call.44.0{0} @0> (size=131072,offset=1445504): bf16[64,1024]{1,0}
 value: <904 fusion.188 @0> (size=131072,offset=790144): bf16[64,1024]{1,0}
 value: <905 custom-call.45.0{} @0> (size=16,offset=1408): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <906 custom-call.45.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <908 loop_convert_fusion.7 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <909 custom-call.46.0{} @0> (size=16,offset=1536): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <910 custom-call.46.0{0} @0> (size=131072,offset=3712): bf16[64,1024]{1,0}
 value: <912 loop_add_fusion.1 @0> (size=262144,offset=1183360): f32[64,1024]{1,0}
 value: <914 custom-call.47.0{} @0> (size=16,offset=1664): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <915 custom-call.47.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <917 loop_convert_fusion.6 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <918 custom-call.48.0{} @0> (size=16,offset=1792): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <922 custom-call.49.0{} @0> (size=16,offset=1920): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <923 custom-call.49.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <925 loop_convert_fusion.5 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <926 custom-call.50.0{} @0> (size=16,offset=2048): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <929 fusion.185 @0> (size=131072,offset=790144): bf16[64,1024]{1,0}
 value: <930 custom-call.51.0{} @0> (size=16,offset=2176): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <931 custom-call.51.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <933 loop_convert_fusion.4 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <934 custom-call.52.0{} @0> (size=16,offset=2304): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <935 custom-call.52.0{0} @0> (size=131072,offset=1445504): bf16[64,1024]{1,0}
 value: <937 fusion.184 @0> (size=131072,offset=790144): bf16[64,1024]{1,0}
 value: <938 custom-call.53.0{} @0> (size=16,offset=2432): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <939 custom-call.53.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <941 loop_convert_fusion.3 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <942 custom-call.54.0{} @0> (size=16,offset=2560): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <943 custom-call.54.0{0} @0> (size=131072,offset=3712): bf16[64,1024]{1,0}
 value: <945 loop_add_fusion.2 @0> (size=262144,offset=1183360): f32[64,1024]{1,0}
 value: <947 custom-call.55.0{} @0> (size=16,offset=2688): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <948 custom-call.55.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <950 loop_convert_fusion.2 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <951 custom-call.56.0{} @0> (size=16,offset=2816): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <955 custom-call.57.0{} @0> (size=16,offset=2944): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <956 custom-call.57.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <958 loop_convert_fusion.1 @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <959 custom-call.58.0{} @0> (size=16,offset=3072): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <962 fusion.181 @0> (size=131072,offset=790144): bf16[64,1024]{1,0}
 value: <963 custom-call.59.0{} @0> (size=16,offset=3200): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <964 custom-call.59.0{0} @0> (size=786432,offset=3712): bf16[64,6144]{1,0}
 value: <966 loop_convert_fusion @0> (size=393216,offset=790144): bf16[64,3072]{1,0}
 value: <967 custom-call.60.0{} @0> (size=16,offset=3328): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <968 custom-call.60.0{0} @0> (size=131072,offset=3712): bf16[64,1024]{1,0}
 value: <970 fusion.179 @0> (size=131072,offset=134784): bf16[64,1024]{1,0}
 value: <971 custom-call.61.0{} @0> (size=16,offset=3456): (bf16[64,4096]{1,0}, s8[4194304]{0})
 value: <979 tuple{} @0> (size=32,offset=1707904): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 1232900816 (1.15GiB)

Used values:
<25 x.77 @0>
 positions:
  x.77
 uses:
  add.202, operand 0
 from instruction: %x.77 = f32[] parameter(0)
<26 y.78 @0>
 positions:
  y.78
 uses:
  add.202, operand 1
 from instruction: %y.78 = f32[] parameter(1)
<27 add.202 @0>
 positions:
  add.202
 uses:
 from instruction: %add.202 = f32[] add(f32[] %x.77, f32[] %y.78)
<844 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.60.0, operand 0
 from instruction: %wrapped_concatenate = bf16[15360,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9, bf16[1024,2048]{1,0} %p.10, bf16[1024,2048]{1,0} %p.11, /*index=10*/bf16[1024,2048]{1,0} %p.12, bf16[1024,2048]{1,0} %p.13, bf16[1024,2048]{1,0} %p.14, bf16[1024,2048]{1,0} %p.15, bf16[1024,2048]{1,0} %p.16), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<845 gemm_fusion_dot.60.0 @0>
 positions:
  gemm_fusion_dot.60.0
 uses:
  fusion.195, operand 2
  fusion.194, operand 2
  fusion.193, operand 3
  fusion.192, operand 2
  loop_add_fusion, operand 0
  fusion.190, operand 3
  fusion.189, operand 4
  fusion.188, operand 2
  loop_add_fusion.1, operand 0
  fusion.186, operand 3
  fusion.185, operand 4
  fusion.184, operand 2
  loop_add_fusion.2, operand 0
  fusion.182, operand 3
  fusion.181, operand 4
  fusion.179, operand 5
 from instruction: %gemm_fusion_dot.60.0 = bf16[64,15360]{1,0} fusion(bf16[15360,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.60_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"64","block_n":"128","block_k":"128","split_k":"1","num_stages":"3","num_warps":"8","num_ctas":"1"}},"force_earliest_schedule":false}
<846 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.195, operand 1
  fusion.194, operand 1
  fusion.193, operand 2
  fusion.192, operand 4
  loop_add_fusion, operand 3
 from instruction: %loop_gather_fusion = bf16[64,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[64]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<847 fusion.195 @0>
 positions:
  fusion.195
 uses:
  custom-call.31.0, operand 0
 from instruction: %fusion.195 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[1024]{0} %p.18), kind=kCustom, calls=%fused_computation.160, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<848 custom-call.31.0{} @0>
 positions:
  custom-call.31.0 {}
 uses:
  get-tuple-element.31, operand 0 {}
 from instruction: %custom-call.31.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.195, bf16[6144,1024]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<849 custom-call.31.0{0} @0>
 positions:
  custom-call.31.0 {0}
  get-tuple-element.31
 uses:
  loop_convert_fusion.14, operand 0
 from instruction: %custom-call.31.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.195, bf16[6144,1024]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<850 custom-call.31.0{1} @0>
 positions:
  custom-call.31.0 {1}
 uses:
 from instruction: %custom-call.31.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.195, bf16[6144,1024]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<851 loop_convert_fusion.14 @0>
 positions:
  loop_convert_fusion.14
 uses:
  custom-call.32.0, operand 0
 from instruction: %loop_convert_fusion.14 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.31), kind=kLoop, calls=%fused_convert.14, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<852 custom-call.32.0{} @0>
 positions:
  custom-call.32.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.32.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<853 custom-call.32.0{0} @0>
 positions:
  custom-call.32.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.194, operand 3
  fusion.193, operand 4
  fusion.192, operand 5
  loop_add_fusion, operand 4
 from instruction: %custom-call.32.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<854 custom-call.32.0{1} @0>
 positions:
  custom-call.32.0 {1}
 uses:
 from instruction: %custom-call.32.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<855 fusion.194 @0>
 positions:
  fusion.194
 uses:
  custom-call.33.0, operand 0
 from instruction: %fusion.194 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[64,1024]{1,0} %get-tuple-element.1.0, bf16[1024]{0} %p.21), kind=kCustom, calls=%fused_computation.159, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<856 custom-call.33.0{} @0>
 positions:
  custom-call.33.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.33.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.194, bf16[6144,1024]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<857 custom-call.33.0{0} @0>
 positions:
  custom-call.33.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.13, operand 0
 from instruction: %custom-call.33.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.194, bf16[6144,1024]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<858 custom-call.33.0{1} @0>
 positions:
  custom-call.33.0 {1}
 uses:
 from instruction: %custom-call.33.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.194, bf16[6144,1024]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<859 loop_convert_fusion.13 @0>
 positions:
  loop_convert_fusion.13
 uses:
  custom-call.34.0, operand 0
 from instruction: %loop_convert_fusion.13 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.13, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<860 custom-call.34.0{} @0>
 positions:
  custom-call.34.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.34.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<861 custom-call.34.0{0} @0>
 positions:
  custom-call.34.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.193, operand 5
  fusion.192, operand 6
  loop_add_fusion, operand 5
 from instruction: %custom-call.34.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<862 custom-call.34.0{1} @0>
 positions:
  custom-call.34.0 {1}
 uses:
 from instruction: %custom-call.34.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<863 fusion.193 @0>
 positions:
  fusion.193
 uses:
  custom-call.35.0, operand 0
 from instruction: %fusion.193 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[1024]{0} %p.24, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[64,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.158, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<864 custom-call.35.0{} @0>
 positions:
  custom-call.35.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.35.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.193, bf16[6144,1024]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<865 custom-call.35.0{0} @0>
 positions:
  custom-call.35.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.12, operand 0
 from instruction: %custom-call.35.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.193, bf16[6144,1024]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<866 custom-call.35.0{1} @0>
 positions:
  custom-call.35.0 {1}
 uses:
 from instruction: %custom-call.35.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.193, bf16[6144,1024]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<867 loop_convert_fusion.12 @0>
 positions:
  loop_convert_fusion.12
 uses:
  custom-call.36.0, operand 0
 from instruction: %loop_convert_fusion.12 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.12, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<868 custom-call.36.0{} @0>
 positions:
  custom-call.36.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.36.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<869 custom-call.36.0{0} @0>
 positions:
  custom-call.36.0 {0}
  get-tuple-element.5.0
 uses:
  fusion.192, operand 1
  loop_add_fusion, operand 2
 from instruction: %custom-call.36.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<870 custom-call.36.0{1} @0>
 positions:
  custom-call.36.0 {1}
 uses:
 from instruction: %custom-call.36.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<871 fusion.192 @0>
 positions:
  fusion.192
 uses:
  custom-call.37.0, operand 0
 from instruction: %fusion.192 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[64,1024]{1,0} %get-tuple-element.5.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[1024]{0} %p.27, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.1.0, bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.157, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<872 custom-call.37.0{} @0>
 positions:
  custom-call.37.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.37.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.192, bf16[6144,1024]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<873 custom-call.37.0{0} @0>
 positions:
  custom-call.37.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.11, operand 0
 from instruction: %custom-call.37.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.192, bf16[6144,1024]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<874 custom-call.37.0{1} @0>
 positions:
  custom-call.37.0 {1}
 uses:
 from instruction: %custom-call.37.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.192, bf16[6144,1024]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<875 loop_convert_fusion.11 @0>
 positions:
  loop_convert_fusion.11
 uses:
  custom-call.38.0, operand 0
 from instruction: %loop_convert_fusion.11 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.11, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<876 custom-call.38.0{} @0>
 positions:
  custom-call.38.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.38.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<877 custom-call.38.0{0} @0>
 positions:
  custom-call.38.0 {0}
  get-tuple-element.7.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.38.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<878 custom-call.38.0{1} @0>
 positions:
  custom-call.38.0 {1}
 uses:
 from instruction: %custom-call.38.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<879 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.191, operand 0
  fusion.190, operand 1
  fusion.189, operand 2
  fusion.188, operand 4
  loop_add_fusion.1, operand 3
 from instruction: %loop_add_fusion = f32[64,1024]{1,0} fusion(bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[64,1024]{1,0} %get-tuple-element.7.0, bf16[64,1024]{1,0} %get-tuple-element.5.0, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.1099/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<880 fusion.191 @0>
 positions:
  fusion.191
 uses:
  custom-call.39.0, operand 0
 from instruction: %fusion.191 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion, f32[] %p.17, bf16[1024]{0} %p.30), kind=kCustom, calls=%fused_computation.156, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.183"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<881 custom-call.39.0{} @0>
 positions:
  custom-call.39.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.39.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.191, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<882 custom-call.39.0{0} @0>
 positions:
  custom-call.39.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.10, operand 0
 from instruction: %custom-call.39.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.191, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<883 custom-call.39.0{1} @0>
 positions:
  custom-call.39.0 {1}
 uses:
 from instruction: %custom-call.39.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.191, bf16[6144,1024]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<884 loop_convert_fusion.10 @0>
 positions:
  loop_convert_fusion.10
 uses:
  custom-call.40.0, operand 0
 from instruction: %loop_convert_fusion.10 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.10, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<885 custom-call.40.0{} @0>
 positions:
  custom-call.40.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.40.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<886 custom-call.40.0{0} @0>
 positions:
  custom-call.40.0 {0}
  get-tuple-element.9.0
 uses:
  fusion.190, operand 2
  fusion.189, operand 3
  fusion.188, operand 5
  loop_add_fusion.1, operand 4
 from instruction: %custom-call.40.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<887 custom-call.40.0{1} @0>
 positions:
  custom-call.40.0 {1}
 uses:
 from instruction: %custom-call.40.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<888 fusion.190 @0>
 positions:
  fusion.190
 uses:
  custom-call.41.0, operand 0
 from instruction: %fusion.190 = bf16[64,1024]{1,0} fusion(f32[] %p.17, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[1024]{0} %p.33), kind=kCustom, calls=%fused_computation.155, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<889 custom-call.41.0{} @0>
 positions:
  custom-call.41.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.41.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.190, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<890 custom-call.41.0{0} @0>
 positions:
  custom-call.41.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.9, operand 0
 from instruction: %custom-call.41.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.190, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<891 custom-call.41.0{1} @0>
 positions:
  custom-call.41.0 {1}
 uses:
 from instruction: %custom-call.41.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.190, bf16[6144,1024]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<892 loop_convert_fusion.9 @0>
 positions:
  loop_convert_fusion.9
 uses:
  custom-call.42.0, operand 0
 from instruction: %loop_convert_fusion.9 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.9, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<893 custom-call.42.0{} @0>
 positions:
  custom-call.42.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.42.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<894 custom-call.42.0{0} @0>
 positions:
  custom-call.42.0 {0}
  get-tuple-element.11.0
 uses:
  fusion.189, operand 5
  fusion.188, operand 6
  loop_add_fusion.1, operand 5
 from instruction: %custom-call.42.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<895 custom-call.42.0{1} @0>
 positions:
  custom-call.42.0 {1}
 uses:
 from instruction: %custom-call.42.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<896 fusion.189 @0>
 positions:
  fusion.189
 uses:
  custom-call.43.0, operand 0
 from instruction: %fusion.189 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[1024]{0} %p.36, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.154, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<897 custom-call.43.0{} @0>
 positions:
  custom-call.43.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.43.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.189, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<898 custom-call.43.0{0} @0>
 positions:
  custom-call.43.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.8, operand 0
 from instruction: %custom-call.43.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.189, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<899 custom-call.43.0{1} @0>
 positions:
  custom-call.43.0 {1}
 uses:
 from instruction: %custom-call.43.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.189, bf16[6144,1024]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<900 loop_convert_fusion.8 @0>
 positions:
  loop_convert_fusion.8
 uses:
  custom-call.44.0, operand 0
 from instruction: %loop_convert_fusion.8 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.8, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<901 custom-call.44.0{} @0>
 positions:
  custom-call.44.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.44.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<902 custom-call.44.0{0} @0>
 positions:
  custom-call.44.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.188, operand 1
  loop_add_fusion.1, operand 2
 from instruction: %custom-call.44.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<903 custom-call.44.0{1} @0>
 positions:
  custom-call.44.0 {1}
 uses:
 from instruction: %custom-call.44.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<904 fusion.188 @0>
 positions:
  fusion.188
 uses:
  custom-call.45.0, operand 0
 from instruction: %fusion.188 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[64,1024]{1,0} %get-tuple-element.13.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[1024]{0} %p.39, f32[64,1024]{1,0} %loop_add_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.153, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<905 custom-call.45.0{} @0>
 positions:
  custom-call.45.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.45.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.188, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<906 custom-call.45.0{0} @0>
 positions:
  custom-call.45.0 {0}
  get-tuple-element.14.0
 uses:
  loop_convert_fusion.7, operand 0
 from instruction: %custom-call.45.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.188, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<907 custom-call.45.0{1} @0>
 positions:
  custom-call.45.0 {1}
 uses:
 from instruction: %custom-call.45.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.188, bf16[6144,1024]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<908 loop_convert_fusion.7 @0>
 positions:
  loop_convert_fusion.7
 uses:
  custom-call.46.0, operand 0
 from instruction: %loop_convert_fusion.7 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<909 custom-call.46.0{} @0>
 positions:
  custom-call.46.0 {}
 uses:
  get-tuple-element.15.0, operand 0 {}
 from instruction: %custom-call.46.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<910 custom-call.46.0{0} @0>
 positions:
  custom-call.46.0 {0}
  get-tuple-element.15.0
 uses:
  loop_add_fusion.1, operand 1
 from instruction: %custom-call.46.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<911 custom-call.46.0{1} @0>
 positions:
  custom-call.46.0 {1}
 uses:
 from instruction: %custom-call.46.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<912 loop_add_fusion.1 @0>
 positions:
  loop_add_fusion.1
 uses:
  fusion.187, operand 0
  fusion.186, operand 1
  fusion.185, operand 2
  fusion.184, operand 4
  loop_add_fusion.2, operand 3
 from instruction: %loop_add_fusion.1 = f32[64,1024]{1,0} fusion(bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[64,1024]{1,0} %get-tuple-element.15.0, bf16[64,1024]{1,0} %get-tuple-element.13.0, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kLoop, calls=%fused_add.1, metadata={op_type="aten__add" op_name="aten__add.1171/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<913 fusion.187 @0>
 positions:
  fusion.187
 uses:
  custom-call.47.0, operand 0
 from instruction: %fusion.187 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.1, f32[] %p.17, bf16[1024]{0} %p.42), kind=kCustom, calls=%fused_computation.152, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.183"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<914 custom-call.47.0{} @0>
 positions:
  custom-call.47.0 {}
 uses:
  get-tuple-element.16.0, operand 0 {}
 from instruction: %custom-call.47.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.187, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<915 custom-call.47.0{0} @0>
 positions:
  custom-call.47.0 {0}
  get-tuple-element.16.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.47.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.187, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<916 custom-call.47.0{1} @0>
 positions:
  custom-call.47.0 {1}
 uses:
 from instruction: %custom-call.47.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.187, bf16[6144,1024]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<917 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.48.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<918 custom-call.48.0{} @0>
 positions:
  custom-call.48.0 {}
 uses:
  get-tuple-element.17.0, operand 0 {}
 from instruction: %custom-call.48.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<919 custom-call.48.0{0} @0>
 positions:
  custom-call.48.0 {0}
  get-tuple-element.17.0
 uses:
  fusion.186, operand 2
  fusion.185, operand 3
  fusion.184, operand 5
  loop_add_fusion.2, operand 4
 from instruction: %custom-call.48.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<920 custom-call.48.0{1} @0>
 positions:
  custom-call.48.0 {1}
 uses:
 from instruction: %custom-call.48.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<921 fusion.186 @0>
 positions:
  fusion.186
 uses:
  custom-call.49.0, operand 0
 from instruction: %fusion.186 = bf16[64,1024]{1,0} fusion(f32[] %p.17, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[1024]{0} %p.45), kind=kCustom, calls=%fused_computation.151, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<922 custom-call.49.0{} @0>
 positions:
  custom-call.49.0 {}
 uses:
  get-tuple-element.18.0, operand 0 {}
 from instruction: %custom-call.49.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.186, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<923 custom-call.49.0{0} @0>
 positions:
  custom-call.49.0 {0}
  get-tuple-element.18.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.49.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.186, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<924 custom-call.49.0{1} @0>
 positions:
  custom-call.49.0 {1}
 uses:
 from instruction: %custom-call.49.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.186, bf16[6144,1024]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<925 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.50.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.18.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<926 custom-call.50.0{} @0>
 positions:
  custom-call.50.0 {}
 uses:
  get-tuple-element.19.0, operand 0 {}
 from instruction: %custom-call.50.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<927 custom-call.50.0{0} @0>
 positions:
  custom-call.50.0 {0}
  get-tuple-element.19.0
 uses:
  fusion.185, operand 5
  fusion.184, operand 6
  loop_add_fusion.2, operand 5
 from instruction: %custom-call.50.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<928 custom-call.50.0{1} @0>
 positions:
  custom-call.50.0 {1}
 uses:
 from instruction: %custom-call.50.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<929 fusion.185 @0>
 positions:
  fusion.185
 uses:
  custom-call.51.0, operand 0
 from instruction: %fusion.185 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[1024]{0} %p.48, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kCustom, calls=%fused_computation.150, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<930 custom-call.51.0{} @0>
 positions:
  custom-call.51.0 {}
 uses:
  get-tuple-element.20.0, operand 0 {}
 from instruction: %custom-call.51.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.185, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<931 custom-call.51.0{0} @0>
 positions:
  custom-call.51.0 {0}
  get-tuple-element.20.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.51.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.185, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<932 custom-call.51.0{1} @0>
 positions:
  custom-call.51.0 {1}
 uses:
 from instruction: %custom-call.51.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.185, bf16[6144,1024]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<933 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.52.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.20.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<934 custom-call.52.0{} @0>
 positions:
  custom-call.52.0 {}
 uses:
  get-tuple-element.21.0, operand 0 {}
 from instruction: %custom-call.52.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<935 custom-call.52.0{0} @0>
 positions:
  custom-call.52.0 {0}
  get-tuple-element.21.0
 uses:
  fusion.184, operand 1
  loop_add_fusion.2, operand 2
 from instruction: %custom-call.52.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<936 custom-call.52.0{1} @0>
 positions:
  custom-call.52.0 {1}
 uses:
 from instruction: %custom-call.52.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<937 fusion.184 @0>
 positions:
  fusion.184
 uses:
  custom-call.53.0, operand 0
 from instruction: %fusion.184 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[64,1024]{1,0} %get-tuple-element.21.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[1024]{0} %p.51, f32[64,1024]{1,0} %loop_add_fusion.1, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kCustom, calls=%fused_computation.149, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<938 custom-call.53.0{} @0>
 positions:
  custom-call.53.0 {}
 uses:
  get-tuple-element.22.0, operand 0 {}
 from instruction: %custom-call.53.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.184, bf16[6144,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<939 custom-call.53.0{0} @0>
 positions:
  custom-call.53.0 {0}
  get-tuple-element.22.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.53.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.184, bf16[6144,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<940 custom-call.53.0{1} @0>
 positions:
  custom-call.53.0 {1}
 uses:
 from instruction: %custom-call.53.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.184, bf16[6144,1024]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<941 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.54.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.22.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<942 custom-call.54.0{} @0>
 positions:
  custom-call.54.0 {}
 uses:
  get-tuple-element.23.0, operand 0 {}
 from instruction: %custom-call.54.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<943 custom-call.54.0{0} @0>
 positions:
  custom-call.54.0 {0}
  get-tuple-element.23.0
 uses:
  loop_add_fusion.2, operand 1
 from instruction: %custom-call.54.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<944 custom-call.54.0{1} @0>
 positions:
  custom-call.54.0 {1}
 uses:
 from instruction: %custom-call.54.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<945 loop_add_fusion.2 @0>
 positions:
  loop_add_fusion.2
 uses:
  fusion.183, operand 0
  fusion.182, operand 1
  fusion.181, operand 2
  fusion.179, operand 3
 from instruction: %loop_add_fusion.2 = f32[64,1024]{1,0} fusion(bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[64,1024]{1,0} %get-tuple-element.23.0, bf16[64,1024]{1,0} %get-tuple-element.21.0, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kLoop, calls=%fused_add.2, metadata={op_type="aten__add" op_name="aten__add.1243/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<946 fusion.183 @0>
 positions:
  fusion.183
 uses:
  custom-call.55.0, operand 0
 from instruction: %fusion.183 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.2, f32[] %p.17, bf16[1024]{0} %p.54), kind=kCustom, calls=%fused_computation.148, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.183"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<947 custom-call.55.0{} @0>
 positions:
  custom-call.55.0 {}
 uses:
  get-tuple-element.24.0, operand 0 {}
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.183, bf16[6144,1024]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<948 custom-call.55.0{0} @0>
 positions:
  custom-call.55.0 {0}
  get-tuple-element.24.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.183, bf16[6144,1024]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<949 custom-call.55.0{1} @0>
 positions:
  custom-call.55.0 {1}
 uses:
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.183, bf16[6144,1024]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<950 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.56.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.24.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<951 custom-call.56.0{} @0>
 positions:
  custom-call.56.0 {}
 uses:
  get-tuple-element.25.0, operand 0 {}
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<952 custom-call.56.0{0} @0>
 positions:
  custom-call.56.0 {0}
  get-tuple-element.25.0
 uses:
  fusion.182, operand 2
  fusion.181, operand 3
  fusion.179, operand 4
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<953 custom-call.56.0{1} @0>
 positions:
  custom-call.56.0 {1}
 uses:
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<954 fusion.182 @0>
 positions:
  fusion.182
 uses:
  custom-call.57.0, operand 0
 from instruction: %fusion.182 = bf16[64,1024]{1,0} fusion(f32[] %p.17, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[1024]{0} %p.57), kind=kCustom, calls=%fused_computation.147, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<955 custom-call.57.0{} @0>
 positions:
  custom-call.57.0 {}
 uses:
  get-tuple-element.26.0, operand 0 {}
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.182, bf16[6144,1024]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<956 custom-call.57.0{0} @0>
 positions:
  custom-call.57.0 {0}
  get-tuple-element.26.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.182, bf16[6144,1024]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<957 custom-call.57.0{1} @0>
 positions:
  custom-call.57.0 {1}
 uses:
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.182, bf16[6144,1024]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<958 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.58.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.26.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<959 custom-call.58.0{} @0>
 positions:
  custom-call.58.0 {}
 uses:
  get-tuple-element.27.0, operand 0 {}
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<960 custom-call.58.0{0} @0>
 positions:
  custom-call.58.0 {0}
  get-tuple-element.27.0
 uses:
  fusion.181, operand 5
  fusion.179, operand 6
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<961 custom-call.58.0{1} @0>
 positions:
  custom-call.58.0 {1}
 uses:
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.59), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<962 fusion.181 @0>
 positions:
  fusion.181
 uses:
  custom-call.59.0, operand 0
 from instruction: %fusion.181 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[1024]{0} %p.60, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.27.0), kind=kCustom, calls=%fused_computation.146, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<963 custom-call.59.0{} @0>
 positions:
  custom-call.59.0 {}
 uses:
  get-tuple-element.28.0, operand 0 {}
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.181, bf16[6144,1024]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<964 custom-call.59.0{0} @0>
 positions:
  custom-call.59.0 {0}
  get-tuple-element.28.0
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.181, bf16[6144,1024]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<965 custom-call.59.0{1} @0>
 positions:
  custom-call.59.0 {1}
 uses:
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.181, bf16[6144,1024]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<966 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.60.0, operand 0
 from instruction: %loop_convert_fusion = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.28.0), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<967 custom-call.60.0{} @0>
 positions:
  custom-call.60.0 {}
 uses:
  get-tuple-element.29.0, operand 0 {}
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<968 custom-call.60.0{0} @0>
 positions:
  custom-call.60.0 {0}
  get-tuple-element.29.0
 uses:
  fusion.179, operand 1
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<969 custom-call.60.0{1} @0>
 positions:
  custom-call.60.0 {1}
 uses:
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.62), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<970 fusion.179 @0>
 positions:
  fusion.179
 uses:
  custom-call.61.0, operand 0
 from instruction: %fusion.179 = bf16[64,1024]{1,0} fusion(f32[] %p.17, bf16[64,1024]{1,0} %get-tuple-element.29.0, bf16[1024]{0} %p.63, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, /*index=5*/bf16[64,15360]{1,0} %gemm_fusion_dot.60.0, bf16[64,1024]{1,0} %get-tuple-element.27.0), kind=kCustom, calls=%fused_computation.144, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<971 custom-call.61.0{} @0>
 positions:
  custom-call.61.0 {}
 uses:
  get-tuple-element.30.0, operand 0 {}
 from instruction: %custom-call.61.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.179, bf16[4096,1024]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<972 custom-call.61.0{0} @0>
 positions:
  custom-call.61.0 {0}
  get-tuple-element.30.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.17.0, operand 1
 from instruction: %custom-call.61.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.179, bf16[4096,1024]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<973 custom-call.61.0{1} @0>
 positions:
  custom-call.61.0 {1}
 uses:
 from instruction: %custom-call.61.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.179, bf16[4096,1024]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<974 triton_softmax.17.0 @0>
 positions:
  triton_softmax.17.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.17.0 = f32[64,8,128]{2,1,0} fusion(f32[] %p.17, bf16[64,4096]{1,0} %get-tuple-element.30.0), kind=kCustom, calls=%triton_softmax_computation.17, metadata={op_type="aten__mul" op_name="aten__mul.1289/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<975 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.1254.0 {0}
  call {0}
  get-tuple-element.33
  tuple {0}
 uses:
  tuple, operand 0
  tuple.1254.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[64,8,128]{2,1,0} fusion(f32[64,8,128]{2,1,0} %triton_softmax.17.0, bf16[128]{0} %p.65, bf16[40960,128]{1,0} %p.66, s32[64]{0} %p.67), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<976 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.1254.0 {1}
  call {1}
  get-tuple-element.34
  bitcast.3183.0
  tuple {1}
 uses:
  bitcast.3183.0, operand 0
  tuple.1254.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[64,1024]{1,0} fusion(bf16[64,4096]{1,0} %get-tuple-element.30.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<977 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.1254.0 {3}
  call {2}
  get-tuple-element.35
  bitcast.3195.0
  tuple {2}
 uses:
  bitcast.3195.0, operand 0
  tuple.1254.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.68), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<978 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.1254.0 {2}
  bitcast.3188.0
  call {3}
  get-tuple-element.36
  tuple {3}
 uses:
  tuple, operand 3
  tuple.1254.0, operand 2
  bitcast.3188.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.68), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<979 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.33, operand 0 {}
  get-tuple-element.34, operand 0 {}
  get-tuple-element.35, operand 0 {}
  get-tuple-element.36, operand 0 {}
 from instruction: %tuple = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %input_concatenate_fusion, bf16[64,8,128]{2,1,0} %bitcast.3183.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.3195.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<980 p5.44.0 @0>
 positions:
  p5.44.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.44.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<981 p4.42.0 @0>
 positions:
  p4.42.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.42.0 = s32[64]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<982 p62.1056.0 @0>
 positions:
  p62.1056.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p62.1056.0 = bf16[1024,2048]{1,0} parameter(62), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<983 p58.984.0 @0>
 positions:
  p58.984.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p58.984.0 = bf16[1024,2048]{1,0} parameter(58), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<984 p54.912.0 @0>
 positions:
  p54.912.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p54.912.0 = bf16[1024,2048]{1,0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<985 p50.840.0 @0>
 positions:
  p50.840.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p50.840.0 = bf16[1024,2048]{1,0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<986 p46.768.0 @0>
 positions:
  p46.768.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p46.768.0 = bf16[1024,2048]{1,0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<987 p42.696.0 @0>
 positions:
  p42.696.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p42.696.0 = bf16[1024,2048]{1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<988 p38.624.0 @0>
 positions:
  p38.624.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p38.624.0 = bf16[1024,2048]{1,0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<989 p34.552.0 @0>
 positions:
  p34.552.0
  p.9
 uses:
  call, operand 9
  wrapped_concatenate, operand 7
 from instruction: %p34.552.0 = bf16[1024,2048]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<990 p30.480.0 @0>
 positions:
  p30.480.0
  p.10
 uses:
  call, operand 10
  wrapped_concatenate, operand 8
 from instruction: %p30.480.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<991 p26.408.0 @0>
 positions:
  p26.408.0
  p.11
 uses:
  call, operand 11
  wrapped_concatenate, operand 9
 from instruction: %p26.408.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<992 p22.336.0 @0>
 positions:
  p22.336.0
  p.12
 uses:
  call, operand 12
  wrapped_concatenate, operand 10
 from instruction: %p22.336.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<993 p18.264.0 @0>
 positions:
  p18.264.0
  p.13
 uses:
  call, operand 13
  wrapped_concatenate, operand 11
 from instruction: %p18.264.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<994 p14.192.0 @0>
 positions:
  p14.192.0
  p.14
 uses:
  call, operand 14
  wrapped_concatenate, operand 12
 from instruction: %p14.192.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<995 p10.120.0 @0>
 positions:
  p10.120.0
  p.15
 uses:
  call, operand 15
  wrapped_concatenate, operand 13
 from instruction: %p10.120.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<996 p6.48.0 @0>
 positions:
  p6.48.0
  p.16
 uses:
  call, operand 16
  wrapped_concatenate, operand 14
 from instruction: %p6.48.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<997 p1.4.0 @0>
 positions:
  p1.4.0
  p.17
 uses:
  call, operand 17
  fusion.195, operand 0
  fusion.194, operand 0
  fusion.193, operand 0
  fusion.192, operand 0
  fusion.191, operand 1
  fusion.190, operand 0
  fusion.189, operand 0
  fusion.188, operand 0
  fusion.187, operand 1
  fusion.186, operand 0
  fusion.185, operand 0
  fusion.184, operand 0
  fusion.183, operand 1
  fusion.182, operand 0
  fusion.181, operand 0
  fusion.179, operand 0
  triton_softmax.17.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<998 p9.68.0 @0>
 positions:
  p9.68.0
  p.18
 uses:
  call, operand 18
  fusion.195, operand 3
 from instruction: %p9.68.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<999 p8.66.0 @0>
 positions:
  p8.66.0
  p.19
 uses:
  call, operand 19
  custom-call.31.0, operand 1
 from instruction: %p8.66.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1000 p7.64.0 @0>
 positions:
  p7.64.0
  p.20
 uses:
  call, operand 20
  custom-call.32.0, operand 1
 from instruction: %p7.64.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1001 p13.140.0 @0>
 positions:
  p13.140.0
  p.21
 uses:
  call, operand 21
  fusion.194, operand 4
 from instruction: %p13.140.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1002 p12.138.0 @0>
 positions:
  p12.138.0
  p.22
 uses:
  call, operand 22
  custom-call.33.0, operand 1
 from instruction: %p12.138.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1003 p11.136.0 @0>
 positions:
  p11.136.0
  p.23
 uses:
  call, operand 23
  custom-call.34.0, operand 1
 from instruction: %p11.136.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1004 p17.212.0 @0>
 positions:
  p17.212.0
  p.24
 uses:
  call, operand 24
  fusion.193, operand 1
 from instruction: %p17.212.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1005 p16.210.0 @0>
 positions:
  p16.210.0
  p.25
 uses:
  call, operand 25
  custom-call.35.0, operand 1
 from instruction: %p16.210.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1006 p15.208.0 @0>
 positions:
  p15.208.0
  p.26
 uses:
  call, operand 26
  custom-call.36.0, operand 1
 from instruction: %p15.208.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1007 p21.284.0 @0>
 positions:
  p21.284.0
  p.27
 uses:
  call, operand 27
  fusion.192, operand 3
 from instruction: %p21.284.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1008 p20.282.0 @0>
 positions:
  p20.282.0
  p.28
 uses:
  call, operand 28
  custom-call.37.0, operand 1
 from instruction: %p20.282.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1009 p19.280.0 @0>
 positions:
  p19.280.0
  p.29
 uses:
  call, operand 29
  custom-call.38.0, operand 1
 from instruction: %p19.280.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1010 p25.356.0 @0>
 positions:
  p25.356.0
  p.30
 uses:
  call, operand 30
  fusion.191, operand 2
 from instruction: %p25.356.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1011 p24.354.0 @0>
 positions:
  p24.354.0
  p.31
 uses:
  call, operand 31
  custom-call.39.0, operand 1
 from instruction: %p24.354.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1012 p23.352.0 @0>
 positions:
  p23.352.0
  p.32
 uses:
  call, operand 32
  custom-call.40.0, operand 1
 from instruction: %p23.352.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1013 p29.428.0 @0>
 positions:
  p29.428.0
  p.33
 uses:
  call, operand 33
  fusion.190, operand 4
 from instruction: %p29.428.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1014 p28.426.0 @0>
 positions:
  p28.426.0
  p.34
 uses:
  call, operand 34
  custom-call.41.0, operand 1
 from instruction: %p28.426.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1015 p27.424.0 @0>
 positions:
  p27.424.0
  p.35
 uses:
  call, operand 35
  custom-call.42.0, operand 1
 from instruction: %p27.424.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1016 p33.500.0 @0>
 positions:
  p33.500.0
  p.36
 uses:
  call, operand 36
  fusion.189, operand 1
 from instruction: %p33.500.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1017 p32.498.0 @0>
 positions:
  p32.498.0
  p.37
 uses:
  call, operand 37
  custom-call.43.0, operand 1
 from instruction: %p32.498.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1018 p31.496.0 @0>
 positions:
  p31.496.0
  p.38
 uses:
  call, operand 38
  custom-call.44.0, operand 1
 from instruction: %p31.496.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1019 p37.572.0 @0>
 positions:
  p37.572.0
  p.39
 uses:
  call, operand 39
  fusion.188, operand 3
 from instruction: %p37.572.0 = bf16[1024]{0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1020 p36.570.0 @0>
 positions:
  p36.570.0
  p.40
 uses:
  call, operand 40
  custom-call.45.0, operand 1
 from instruction: %p36.570.0 = bf16[6144,1024]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1021 p35.568.0 @0>
 positions:
  p35.568.0
  p.41
 uses:
  call, operand 41
  custom-call.46.0, operand 1
 from instruction: %p35.568.0 = bf16[1024,3072]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1022 p41.644.0 @0>
 positions:
  p41.644.0
  p.42
 uses:
  call, operand 42
  fusion.187, operand 2
 from instruction: %p41.644.0 = bf16[1024]{0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1023 p40.642.0 @0>
 positions:
  p40.642.0
  p.43
 uses:
  call, operand 43
  custom-call.47.0, operand 1
 from instruction: %p40.642.0 = bf16[6144,1024]{1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1024 p39.640.0 @0>
 positions:
  p39.640.0
  p.44
 uses:
  call, operand 44
  custom-call.48.0, operand 1
 from instruction: %p39.640.0 = bf16[1024,3072]{1,0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1025 p45.716.0 @0>
 positions:
  p45.716.0
  p.45
 uses:
  call, operand 45
  fusion.186, operand 4
 from instruction: %p45.716.0 = bf16[1024]{0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1026 p44.714.0 @0>
 positions:
  p44.714.0
  p.46
 uses:
  call, operand 46
  custom-call.49.0, operand 1
 from instruction: %p44.714.0 = bf16[6144,1024]{1,0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1027 p43.712.0 @0>
 positions:
  p43.712.0
  p.47
 uses:
  call, operand 47
  custom-call.50.0, operand 1
 from instruction: %p43.712.0 = bf16[1024,3072]{1,0} parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1028 p49.788.0 @0>
 positions:
  p49.788.0
  p.48
 uses:
  call, operand 48
  fusion.185, operand 1
 from instruction: %p49.788.0 = bf16[1024]{0} parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1029 p48.786.0 @0>
 positions:
  p48.786.0
  p.49
 uses:
  call, operand 49
  custom-call.51.0, operand 1
 from instruction: %p48.786.0 = bf16[6144,1024]{1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1030 p47.784.0 @0>
 positions:
  p47.784.0
  p.50
 uses:
  call, operand 50
  custom-call.52.0, operand 1
 from instruction: %p47.784.0 = bf16[1024,3072]{1,0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1031 p53.860.0 @0>
 positions:
  p53.860.0
  p.51
 uses:
  call, operand 51
  fusion.184, operand 3
 from instruction: %p53.860.0 = bf16[1024]{0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1032 p52.858.0 @0>
 positions:
  p52.858.0
  p.52
 uses:
  call, operand 52
  custom-call.53.0, operand 1
 from instruction: %p52.858.0 = bf16[6144,1024]{1,0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1033 p51.856.0 @0>
 positions:
  p51.856.0
  p.53
 uses:
  call, operand 53
  custom-call.54.0, operand 1
 from instruction: %p51.856.0 = bf16[1024,3072]{1,0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1034 p57.932.0 @0>
 positions:
  p57.932.0
  p.54
 uses:
  call, operand 54
  fusion.183, operand 2
 from instruction: %p57.932.0 = bf16[1024]{0} parameter(57), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1035 p56.930.0 @0>
 positions:
  p56.930.0
  p.55
 uses:
  call, operand 55
  custom-call.55.0, operand 1
 from instruction: %p56.930.0 = bf16[6144,1024]{1,0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1036 p55.928.0 @0>
 positions:
  p55.928.0
  p.56
 uses:
  call, operand 56
  custom-call.56.0, operand 1
 from instruction: %p55.928.0 = bf16[1024,3072]{1,0} parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1037 p61.1004.0 @0>
 positions:
  p61.1004.0
  p.57
 uses:
  call, operand 57
  fusion.182, operand 4
 from instruction: %p61.1004.0 = bf16[1024]{0} parameter(61), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1038 p60.1002.0 @0>
 positions:
  p60.1002.0
  p.58
 uses:
  call, operand 58
  custom-call.57.0, operand 1
 from instruction: %p60.1002.0 = bf16[6144,1024]{1,0} parameter(60), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1039 p59.1000.0 @0>
 positions:
  p59.1000.0
  p.59
 uses:
  call, operand 59
  custom-call.58.0, operand 1
 from instruction: %p59.1000.0 = bf16[1024,3072]{1,0} parameter(59), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1040 p65.1076.0 @0>
 positions:
  p65.1076.0
  p.60
 uses:
  call, operand 60
  fusion.181, operand 1
 from instruction: %p65.1076.0 = bf16[1024]{0} parameter(65), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1041 p64.1074.0 @0>
 positions:
  p64.1074.0
  p.61
 uses:
  call, operand 61
  custom-call.59.0, operand 1
 from instruction: %p64.1074.0 = bf16[6144,1024]{1,0} parameter(64), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1042 p63.1072.0 @0>
 positions:
  p63.1072.0
  p.62
 uses:
  call, operand 62
  custom-call.60.0, operand 1
 from instruction: %p63.1072.0 = bf16[1024,3072]{1,0} parameter(63), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1043 p3.8.0 @0>
 positions:
  p3.8.0
  p.63
 uses:
  call, operand 63
  fusion.179, operand 2
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1044 p2.6.0 @0>
 positions:
  p2.6.0
  p.64
 uses:
  call, operand 64
  custom-call.61.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1045 p0.1.0 @0>
 positions:
  p0.1.0
  p.65
 uses:
  call, operand 65
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1046 p67.1201.0 @0>
 positions:
  p67.1201.0
  p.66
 uses:
  call, operand 66
  input_concatenate_fusion, operand 2
 from instruction: %p67.1201.0 = bf16[40960,128]{1,0} parameter(67), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1047 p66.1200.0 @0>
 positions:
  p66.1200.0
  p.67
 uses:
  call, operand 67
  input_concatenate_fusion, operand 3
 from instruction: %p66.1200.0 = s32[64]{0} parameter(66), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1048 p68.1245.0 @0>
 positions:
  p68.1245.0
  p.68
 uses:
  call, operand 68
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p68.1245.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(68), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1049 tuple.1254.0{} @0>
 positions:
  tuple.1254.0 {}
 uses:
 from instruction: %tuple.1254.0 = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %get-tuple-element.33, bf16[64,8,128]{2,1,0} %get-tuple-element.34, bf16[4233,16,8,128]{3,2,1,0} %bitcast.3188.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.35)


HloLiveRange (max 252):
  InstructionSequence:
    0:p1.4.0
    1:p68.1245.0
    2:p67.1201.0
    3:p66.1200.0
    4:p65.1076.0
    5:p64.1074.0
    6:p63.1072.0
    7:p62.1056.0
    8:p61.1004.0
    9:p60.1002.0
    10:p59.1000.0
    11:p58.984.0
    12:p57.932.0
    13:p56.930.0
    14:p55.928.0
    15:p54.912.0
    16:p53.860.0
    17:p52.858.0
    18:p51.856.0
    19:p50.840.0
    20:p49.788.0
    21:p48.786.0
    22:p47.784.0
    23:p46.768.0
    24:p45.716.0
    25:p44.714.0
    26:p43.712.0
    27:p42.696.0
    28:p41.644.0
    29:p40.642.0
    30:p39.640.0
    31:p38.624.0
    32:p37.572.0
    33:p36.570.0
    34:p35.568.0
    35:p34.552.0
    36:p33.500.0
    37:p32.498.0
    38:p31.496.0
    39:p30.480.0
    40:p29.428.0
    41:p28.426.0
    42:p27.424.0
    43:p26.408.0
    44:p25.356.0
    45:p24.354.0
    46:p23.352.0
    47:p22.336.0
    48:p21.284.0
    49:p20.282.0
    50:p19.280.0
    51:p18.264.0
    52:p17.212.0
    53:p16.210.0
    54:p15.208.0
    55:p14.192.0
    56:p13.140.0
    57:p12.138.0
    58:p11.136.0
    59:p10.120.0
    60:p9.68.0
    61:p8.66.0
    62:p7.64.0
    63:p6.48.0
    64:p5.44.0
    65:p4.42.0
    66:p3.8.0
    67:p2.6.0
    68:p0.1.0
    69:p
    70:p.1
    71:p.2
    72:p.3
    73:p.4
    74:p.5
    75:p.6
    76:p.7
    77:p.8
    78:p.9
    79:p.10
    80:p.11
    81:p.12
    82:p.13
    83:p.14
    84:p.15
    85:p.16
    86:p.17
    87:p.18
    88:p.19
    89:p.20
    90:p.21
    91:p.22
    92:p.23
    93:p.24
    94:p.25
    95:p.26
    96:p.27
    97:p.28
    98:p.29
    99:p.30
    100:p.31
    101:p.32
    102:p.33
    103:p.34
    104:p.35
    105:p.36
    106:p.37
    107:p.38
    108:p.39
    109:p.40
    110:p.41
    111:p.42
    112:p.43
    113:p.44
    114:p.45
    115:p.46
    116:p.47
    117:p.48
    118:p.49
    119:p.50
    120:p.51
    121:p.52
    122:p.53
    123:p.54
    124:p.55
    125:p.56
    126:p.57
    127:p.58
    128:p.59
    129:p.60
    130:p.61
    131:p.62
    132:p.63
    133:p.64
    134:p.65
    135:p.66
    136:p.67
    137:p.68
    138:loop_gather_fusion
    139:wrapped_concatenate
    140:gemm_fusion_dot.60.0
    141:fusion.195
    142:custom-call.31.0
    143:get-tuple-element.31
    144:loop_convert_fusion.14
    145:custom-call.32.0
    146:get-tuple-element.1.0
    147:fusion.194
    148:custom-call.33.0
    149:get-tuple-element.2.0
    150:loop_convert_fusion.13
    151:custom-call.34.0
    152:get-tuple-element.3.0
    153:fusion.193
    154:custom-call.35.0
    155:get-tuple-element.4.0
    156:loop_convert_fusion.12
    157:custom-call.36.0
    158:get-tuple-element.5.0
    159:fusion.192
    160:custom-call.37.0
    161:get-tuple-element.6.0
    162:loop_convert_fusion.11
    163:custom-call.38.0
    164:get-tuple-element.7.0
    165:loop_add_fusion
    166:fusion.191
    167:custom-call.39.0
    168:get-tuple-element.8.0
    169:loop_convert_fusion.10
    170:custom-call.40.0
    171:get-tuple-element.9.0
    172:fusion.190
    173:custom-call.41.0
    174:get-tuple-element.10.0
    175:loop_convert_fusion.9
    176:custom-call.42.0
    177:get-tuple-element.11.0
    178:fusion.189
    179:custom-call.43.0
    180:get-tuple-element.12.0
    181:loop_convert_fusion.8
    182:custom-call.44.0
    183:get-tuple-element.13.0
    184:fusion.188
    185:custom-call.45.0
    186:get-tuple-element.14.0
    187:loop_convert_fusion.7
    188:custom-call.46.0
    189:get-tuple-element.15.0
    190:loop_add_fusion.1
    191:fusion.187
    192:custom-call.47.0
    193:get-tuple-element.16.0
    194:loop_convert_fusion.6
    195:custom-call.48.0
    196:get-tuple-element.17.0
    197:fusion.186
    198:custom-call.49.0
    199:get-tuple-element.18.0
    200:loop_convert_fusion.5
    201:custom-call.50.0
    202:get-tuple-element.19.0
    203:fusion.185
    204:custom-call.51.0
    205:get-tuple-element.20.0
    206:loop_convert_fusion.4
    207:custom-call.52.0
    208:get-tuple-element.21.0
    209:fusion.184
    210:custom-call.53.0
    211:get-tuple-element.22.0
    212:loop_convert_fusion.3
    213:custom-call.54.0
    214:get-tuple-element.23.0
    215:loop_add_fusion.2
    216:fusion.183
    217:custom-call.55.0
    218:get-tuple-element.24.0
    219:loop_convert_fusion.2
    220:custom-call.56.0
    221:get-tuple-element.25.0
    222:fusion.182
    223:custom-call.57.0
    224:get-tuple-element.26.0
    225:loop_convert_fusion.1
    226:custom-call.58.0
    227:get-tuple-element.27.0
    228:fusion.181
    229:custom-call.59.0
    230:get-tuple-element.28.0
    231:loop_convert_fusion
    232:custom-call.60.0
    233:get-tuple-element.29.0
    234:fusion.179
    235:custom-call.61.0
    236:get-tuple-element.30.0
    237:wrapped_slice
    238:triton_softmax.17.0
    239:input_concatenate_fusion
    240:bitcast.3183.0
    241:loop_slice_fusion
    242:bitcast.3195.0
    243:wrapped_slice.1
    244:tuple
    245:call
    246:get-tuple-element.33
    247:get-tuple-element.34
    248:get-tuple-element.35
    249:get-tuple-element.36
    250:bitcast.3188.0
    251:tuple.1254.0
  BufferLiveRange:
    wrapped_concatenate{}:139-140
    gemm_fusion_dot.60.0{}:140-234
    loop_gather_fusion{}:138-165
    fusion.195{}:141-142
    custom-call.31.0{}:142-143
    custom-call.31.0{0}:142-144
    custom-call.31.0{1}:142-142
    loop_convert_fusion.14{}:144-145
    custom-call.32.0{}:145-146
    custom-call.32.0{0}:145-165
    custom-call.32.0{1}:145-145
    fusion.194{}:147-148
    custom-call.33.0{}:148-149
    custom-call.33.0{0}:148-150
    custom-call.33.0{1}:148-148
    loop_convert_fusion.13{}:150-151
    custom-call.34.0{}:151-152
    custom-call.34.0{0}:151-165
    custom-call.34.0{1}:151-151
    fusion.193{}:153-154
    custom-call.35.0{}:154-155
    custom-call.35.0{0}:154-156
    custom-call.35.0{1}:154-154
    loop_convert_fusion.12{}:156-157
    custom-call.36.0{}:157-158
    custom-call.36.0{0}:157-165
    custom-call.36.0{1}:157-157
    fusion.192{}:159-160
    custom-call.37.0{}:160-161
    custom-call.37.0{0}:160-162
    custom-call.37.0{1}:160-160
    loop_convert_fusion.11{}:162-163
    custom-call.38.0{}:163-164
    custom-call.38.0{0}:163-165
    custom-call.38.0{1}:163-163
    loop_add_fusion{}:165-190
    fusion.191{}:166-167
    custom-call.39.0{}:167-168
    custom-call.39.0{0}:167-169
    custom-call.39.0{1}:167-167
    loop_convert_fusion.10{}:169-170
    custom-call.40.0{}:170-171
    custom-call.40.0{0}:170-190
    custom-call.40.0{1}:170-170
    fusion.190{}:172-173
    custom-call.41.0{}:173-174
    custom-call.41.0{0}:173-175
    custom-call.41.0{1}:173-173
    loop_convert_fusion.9{}:175-176
    custom-call.42.0{}:176-177
    custom-call.42.0{0}:176-190
    custom-call.42.0{1}:176-176
    fusion.189{}:178-179
    custom-call.43.0{}:179-180
    custom-call.43.0{0}:179-181
    custom-call.43.0{1}:179-179
    loop_convert_fusion.8{}:181-182
    custom-call.44.0{}:182-183
    custom-call.44.0{0}:182-190
    custom-call.44.0{1}:182-182
    fusion.188{}:184-185
    custom-call.45.0{}:185-186
    custom-call.45.0{0}:185-187
    custom-call.45.0{1}:185-185
    loop_convert_fusion.7{}:187-188
    custom-call.46.0{}:188-189
    custom-call.46.0{0}:188-190
    custom-call.46.0{1}:188-188
    loop_add_fusion.1{}:190-215
    fusion.187{}:191-192
    custom-call.47.0{}:192-193
    custom-call.47.0{0}:192-194
    custom-call.47.0{1}:192-192
    loop_convert_fusion.6{}:194-195
    custom-call.48.0{}:195-196
    custom-call.48.0{0}:195-215
    custom-call.48.0{1}:195-195
    fusion.186{}:197-198
    custom-call.49.0{}:198-199
    custom-call.49.0{0}:198-200
    custom-call.49.0{1}:198-198
    loop_convert_fusion.5{}:200-201
    custom-call.50.0{}:201-202
    custom-call.50.0{0}:201-215
    custom-call.50.0{1}:201-201
    fusion.185{}:203-204
    custom-call.51.0{}:204-205
    custom-call.51.0{0}:204-206
    custom-call.51.0{1}:204-204
    loop_convert_fusion.4{}:206-207
    custom-call.52.0{}:207-208
    custom-call.52.0{0}:207-215
    custom-call.52.0{1}:207-207
    fusion.184{}:209-210
    custom-call.53.0{}:210-211
    custom-call.53.0{0}:210-212
    custom-call.53.0{1}:210-210
    loop_convert_fusion.3{}:212-213
    custom-call.54.0{}:213-214
    custom-call.54.0{0}:213-215
    custom-call.54.0{1}:213-213
    loop_add_fusion.2{}:215-234
    fusion.183{}:216-217
    custom-call.55.0{}:217-218
    custom-call.55.0{0}:217-219
    custom-call.55.0{1}:217-217
    loop_convert_fusion.2{}:219-220
    custom-call.56.0{}:220-221
    custom-call.56.0{0}:220-234
    custom-call.56.0{1}:220-220
    fusion.182{}:222-223
    custom-call.57.0{}:223-224
    custom-call.57.0{0}:223-225
    custom-call.57.0{1}:223-223
    loop_convert_fusion.1{}:225-226
    custom-call.58.0{}:226-227
    custom-call.58.0{0}:226-234
    custom-call.58.0{1}:226-226
    fusion.181{}:228-229
    custom-call.59.0{}:229-230
    custom-call.59.0{0}:229-231
    custom-call.59.0{1}:229-229
    loop_convert_fusion{}:231-232
    custom-call.60.0{}:232-233
    custom-call.60.0{0}:232-234
    custom-call.60.0{1}:232-232
    fusion.179{}:234-235
    custom-call.61.0{}:235-236
    custom-call.61.0{0}:235-238
    custom-call.61.0{1}:235-235
    triton_softmax.17.0{}:238-239
    input_concatenate_fusion{}:239-252
    wrapped_slice{}:237-252
    loop_slice_fusion{}:241-252
    wrapped_slice.1{}:243-252
    tuple{}:244-249
    p5.44.0{}:0-252
    p4.42.0{}:0-252
    p62.1056.0{}:0-252
    p58.984.0{}:0-252
    p54.912.0{}:0-252
    p50.840.0{}:0-252
    p46.768.0{}:0-252
    p42.696.0{}:0-252
    p38.624.0{}:0-252
    p34.552.0{}:0-252
    p30.480.0{}:0-252
    p26.408.0{}:0-252
    p22.336.0{}:0-252
    p18.264.0{}:0-252
    p14.192.0{}:0-252
    p10.120.0{}:0-252
    p6.48.0{}:0-252
    p1.4.0{}:0-252
    p9.68.0{}:0-252
    p8.66.0{}:0-252
    p7.64.0{}:0-252
    p13.140.0{}:0-252
    p12.138.0{}:0-252
    p11.136.0{}:0-252
    p17.212.0{}:0-252
    p16.210.0{}:0-252
    p15.208.0{}:0-252
    p21.284.0{}:0-252
    p20.282.0{}:0-252
    p19.280.0{}:0-252
    p25.356.0{}:0-252
    p24.354.0{}:0-252
    p23.352.0{}:0-252
    p29.428.0{}:0-252
    p28.426.0{}:0-252
    p27.424.0{}:0-252
    p33.500.0{}:0-252
    p32.498.0{}:0-252
    p31.496.0{}:0-252
    p37.572.0{}:0-252
    p36.570.0{}:0-252
    p35.568.0{}:0-252
    p41.644.0{}:0-252
    p40.642.0{}:0-252
    p39.640.0{}:0-252
    p45.716.0{}:0-252
    p44.714.0{}:0-252
    p43.712.0{}:0-252
    p49.788.0{}:0-252
    p48.786.0{}:0-252
    p47.784.0{}:0-252
    p53.860.0{}:0-252
    p52.858.0{}:0-252
    p51.856.0{}:0-252
    p57.932.0{}:0-252
    p56.930.0{}:0-252
    p55.928.0{}:0-252
    p61.1004.0{}:0-252
    p60.1002.0{}:0-252
    p59.1000.0{}:0-252
    p65.1076.0{}:0-252
    p64.1074.0{}:0-252
    p63.1072.0{}:0-252
    p3.8.0{}:0-252
    p2.6.0{}:0-252
    p0.1.0{}:0-252
    p67.1201.0{}:0-252
    p66.1200.0{}:0-252
    p68.1245.0{}:0-252
    tuple.1254.0{}:251-252
  Live ranges at 244 (peak):
    input_concatenate_fusion: 131072 bytes
    wrapped_slice: 131072 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.44.0: 311164928 bytes
    p4.42.0: 256 bytes
    p62.1056.0: 4194304 bytes
    p58.984.0: 4194304 bytes
    p54.912.0: 4194304 bytes
    p50.840.0: 4194304 bytes
    p46.768.0: 4194304 bytes
    p42.696.0: 4194304 bytes
    p38.624.0: 4194304 bytes
    p34.552.0: 4194304 bytes
    p30.480.0: 4194304 bytes
    p26.408.0: 4194304 bytes
    p22.336.0: 4194304 bytes
    p18.264.0: 4194304 bytes
    p14.192.0: 4194304 bytes
    p10.120.0: 4194304 bytes
    p6.48.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.68.0: 2048 bytes
    p8.66.0: 12582912 bytes
    p7.64.0: 6291456 bytes
    p13.140.0: 2048 bytes
    p12.138.0: 12582912 bytes
    p11.136.0: 6291456 bytes
    p17.212.0: 2048 bytes
    p16.210.0: 12582912 bytes
    p15.208.0: 6291456 bytes
    p21.284.0: 2048 bytes
    p20.282.0: 12582912 bytes
    p19.280.0: 6291456 bytes
    p25.356.0: 2048 bytes
    p24.354.0: 12582912 bytes
    p23.352.0: 6291456 bytes
    p29.428.0: 2048 bytes
    p28.426.0: 12582912 bytes
    p27.424.0: 6291456 bytes
    p33.500.0: 2048 bytes
    p32.498.0: 12582912 bytes
    p31.496.0: 6291456 bytes
    p37.572.0: 2048 bytes
    p36.570.0: 12582912 bytes
    p35.568.0: 6291456 bytes
    p41.644.0: 2048 bytes
    p40.642.0: 12582912 bytes
    p39.640.0: 6291456 bytes
    p45.716.0: 2048 bytes
    p44.714.0: 12582912 bytes
    p43.712.0: 6291456 bytes
    p49.788.0: 2048 bytes
    p48.786.0: 12582912 bytes
    p47.784.0: 6291456 bytes
    p53.860.0: 2048 bytes
    p52.858.0: 12582912 bytes
    p51.856.0: 6291456 bytes
    p57.932.0: 2048 bytes
    p56.930.0: 12582912 bytes
    p55.928.0: 6291456 bytes
    p61.1004.0: 2048 bytes
    p60.1002.0: 12582912 bytes
    p59.1000.0: 6291456 bytes
    p65.1076.0: 2048 bytes
    p64.1074.0: 12582912 bytes
    p63.1072.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p67.1201.0: 10485760 bytes
    p66.1200.0: 256 bytes
    p68.1245.0: 277413888 bytes
