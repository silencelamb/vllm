BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <576 p5.30.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 40, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <616 p40.727.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <499 gemm_fusion_dot.31.0 @0> (size=262144,offset=0): bf16[16,8192]{1,0}
 value: <568 custom-call.33.0{0} @0> (size=131072,offset=0): bf16[16,4096]{1,0}
 value: <573 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <498 wrapped_concatenate @0> (size=33554432,offset=0): bf16[8192,2048]{1,0}
 value: <504 custom-call.17.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <508 custom-call.18.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <512 custom-call.19.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <516 custom-call.20.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <520 custom-call.21.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <524 custom-call.22.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <529 custom-call.23.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <533 custom-call.24.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <537 custom-call.25.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <541 custom-call.26.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <545 custom-call.27.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <549 custom-call.28.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <553 custom-call.29.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <557 custom-call.30.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <561 custom-call.31.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <565 custom-call.32.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <569 custom-call.33.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <570 triton_softmax.10.0 @0> (size=65536,offset=0): f32[16,8,128]{2,1,0}
 value: <574 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <588 p8.52.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <591 p12.124.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <594 p16.196.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 12582912, parameter 20, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <597 p20.268.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 8: size 12582912, parameter 24, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <600 p24.340.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 9: size 12582912, parameter 28, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <603 p28.412.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 10: size 12582912, parameter 32, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <606 p32.484.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 11: size 12582912, parameter 36, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <609 p36.556.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 12: size 10485760, parameter 39, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <614 p39.683.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 13: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <612 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 14: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <589 p7.50.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 15: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <592 p11.122.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 16: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <595 p15.194.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 17: size 6291456, parameter 19, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <598 p19.266.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 18: size 6291456, parameter 23, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <601 p23.338.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 19: size 6291456, parameter 27, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <604 p27.410.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 20: size 6291456, parameter 31, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <607 p31.482.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 21: size 6291456, parameter 35, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <610 p35.554.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 22: size 4194304, parameter 34, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <578 p34.538.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 23: size 4194304, parameter 30, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <579 p30.466.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 24: size 4194304, parameter 26, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <580 p26.394.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 25: size 4194304, parameter 22, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <581 p22.322.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 26: size 4194304, parameter 18, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <582 p18.250.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 27: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <583 p14.178.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 28: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <584 p10.106.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 29: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <585 p6.34.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 30: size 32768, maybe-live-out:
 value: <501 fusion.116 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <507 custom-call.18.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <534 fusion.108 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <540 custom-call.26.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <571 input_concatenate_fusion @0> (size=32768,offset=0): bf16[16,8,128]{2,1,0}
allocation 31: size 32768, maybe-live-out:
 value: <500 loop_gather_fusion @0> (size=32768,offset=0): bf16[16,1,1024]{2,0,1}
 value: <526 fusion.110 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <532 custom-call.24.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <572 wrapped_slice @0> (size=32768,offset=0): bf16[16,1024]{1,0}
allocation 32: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <587 p9.54.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 33: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <590 p13.126.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 34: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <593 p17.198.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 35: size 2048, parameter 21, shape |bf16[1024]| at ShapeIndex {}:
 value: <596 p21.270.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 36: size 2048, parameter 25, shape |bf16[1024]| at ShapeIndex {}:
 value: <599 p25.342.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 37: size 2048, parameter 29, shape |bf16[1024]| at ShapeIndex {}:
 value: <602 p29.414.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 38: size 2048, parameter 33, shape |bf16[1024]| at ShapeIndex {}:
 value: <605 p33.486.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 39: size 2048, parameter 37, shape |bf16[1024]| at ShapeIndex {}:
 value: <608 p37.558.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 40: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <611 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 41: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <613 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 42: size 64, parameter 4, shape |s32[16]| at ShapeIndex {}:
 value: <577 p4.28.0 @0> (size=64,offset=0): s32[16]{0}
allocation 43: size 64, parameter 38, shape |s32[16]| at ShapeIndex {}:
 value: <615 p38.682.0 @0> (size=64,offset=0): s32[16]{0}
allocation 44: size 32, output shape is |(bf16[16,8,128], bf16[16,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <617 tuple.736.0{} @0> (size=32,offset=0): (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 45: size 4, thread-local:
 value: <20 add.114 @0> (size=4,offset=0): f32[]
allocation 46: size 4, thread-local:
 value: <19 y.64 @0> (size=4,offset=0): f32[]
allocation 47: size 4, thread-local:
 value: <18 x.63 @0> (size=4,offset=0): f32[]
allocation 48: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <586 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 49: size 428192, preallocated-temp:
 value: <502 custom-call.17.0{} @0> (size=16,offset=428032): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <503 custom-call.17.0{0} @0> (size=196608,offset=1920): bf16[16,6144]{1,0}
 value: <505 loop_convert_fusion.7 @0> (size=98304,offset=198528): bf16[16,3072]{1,0}
 value: <506 custom-call.18.0{} @0> (size=16,offset=427904): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <509 fusion.114 @0> (size=32768,offset=198528): bf16[16,1024]{1,0}
 value: <510 custom-call.19.0{} @0> (size=16,offset=1792): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <511 custom-call.19.0{0} @0> (size=196608,offset=1920): bf16[16,6144]{1,0}
 value: <513 loop_convert_fusion.6 @0> (size=98304,offset=198528): bf16[16,3072]{1,0}
 value: <514 custom-call.20.0{} @0> (size=16,offset=0): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <515 custom-call.20.0{0} @0> (size=32768,offset=362368): bf16[16,1024]{1,0}
 value: <517 fusion.112 @0> (size=32768,offset=198528): bf16[16,1024]{1,0}
 value: <518 custom-call.21.0{} @0> (size=16,offset=128): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <519 custom-call.21.0{0} @0> (size=196608,offset=1920): bf16[16,6144]{1,0}
 value: <521 loop_convert_fusion.5 @0> (size=98304,offset=198528): bf16[16,3072]{1,0}
 value: <522 custom-call.22.0{} @0> (size=16,offset=256): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <523 custom-call.22.0{0} @0> (size=32768,offset=1920): bf16[16,1024]{1,0}
 value: <525 loop_add_fusion @0> (size=65536,offset=296832): f32[16,1024]{1,0}
 value: <527 custom-call.23.0{} @0> (size=16,offset=384): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <528 custom-call.23.0{0} @0> (size=196608,offset=1920): bf16[16,6144]{1,0}
 value: <530 loop_convert_fusion.4 @0> (size=98304,offset=198528): bf16[16,3072]{1,0}
 value: <531 custom-call.24.0{} @0> (size=16,offset=512): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <535 custom-call.25.0{} @0> (size=16,offset=640): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <536 custom-call.25.0{0} @0> (size=196608,offset=1920): bf16[16,6144]{1,0}
 value: <538 loop_convert_fusion.3 @0> (size=98304,offset=198528): bf16[16,3072]{1,0}
 value: <539 custom-call.26.0{} @0> (size=16,offset=768): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <542 fusion.106 @0> (size=32768,offset=198528): bf16[16,1024]{1,0}
 value: <543 custom-call.27.0{} @0> (size=16,offset=896): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <544 custom-call.27.0{0} @0> (size=196608,offset=1920): bf16[16,6144]{1,0}
 value: <546 loop_convert_fusion.2 @0> (size=98304,offset=198528): bf16[16,3072]{1,0}
 value: <547 custom-call.28.0{} @0> (size=16,offset=1024): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <548 custom-call.28.0{0} @0> (size=32768,offset=362368): bf16[16,1024]{1,0}
 value: <550 fusion.104 @0> (size=32768,offset=198528): bf16[16,1024]{1,0}
 value: <551 custom-call.29.0{} @0> (size=16,offset=1152): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <552 custom-call.29.0{0} @0> (size=196608,offset=1920): bf16[16,6144]{1,0}
 value: <554 loop_convert_fusion.1 @0> (size=98304,offset=198528): bf16[16,3072]{1,0}
 value: <555 custom-call.30.0{} @0> (size=16,offset=1280): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <556 custom-call.30.0{0} @0> (size=32768,offset=395136): bf16[16,1024]{1,0}
 value: <558 fusion.102 @0> (size=32768,offset=198528): bf16[16,1024]{1,0}
 value: <559 custom-call.31.0{} @0> (size=16,offset=1408): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <560 custom-call.31.0{0} @0> (size=196608,offset=1920): bf16[16,6144]{1,0}
 value: <562 loop_convert_fusion @0> (size=98304,offset=198528): bf16[16,3072]{1,0}
 value: <563 custom-call.32.0{} @0> (size=16,offset=1536): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <564 custom-call.32.0{0} @0> (size=32768,offset=1920): bf16[16,1024]{1,0}
 value: <566 fusion.100 @0> (size=32768,offset=34688): bf16[16,1024]{1,0}
 value: <567 custom-call.33.0{} @0> (size=16,offset=1664): (bf16[16,4096]{1,0}, s8[4194304]{0})
 value: <575 tuple{} @0> (size=32,offset=428160): (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 1069929040 (1020.36MiB)

Used values:
<18 x.63 @0>
 positions:
  x.63
 uses:
  add.114, operand 0
 from instruction: %x.63 = f32[] parameter(0)
<19 y.64 @0>
 positions:
  y.64
 uses:
  add.114, operand 1
 from instruction: %y.64 = f32[] parameter(1)
<20 add.114 @0>
 positions:
  add.114
 uses:
 from instruction: %add.114 = f32[] add(f32[] %x.63, f32[] %y.64)
<498 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.31.0, operand 0
 from instruction: %wrapped_concatenate = bf16[8192,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<499 gemm_fusion_dot.31.0 @0>
 positions:
  gemm_fusion_dot.31.0
 uses:
  fusion.116, operand 2
  fusion.114, operand 3
  fusion.112, operand 3
  loop_add_fusion, operand 0
  fusion.108, operand 4
  fusion.106, operand 4
  fusion.104, operand 3
  fusion.102, operand 3
  fusion.100, operand 4
 from instruction: %gemm_fusion_dot.31.0 = bf16[16,8192]{1,0} fusion(bf16[8192,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.31_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"128","block_n":"16","block_k":"128","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<500 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.116, operand 1
  fusion.114, operand 2
  fusion.112, operand 4
  loop_add_fusion, operand 3
 from instruction: %loop_gather_fusion = bf16[16,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[16]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<501 fusion.116 @0>
 positions:
  fusion.116
 uses:
  custom-call.17.0, operand 0
 from instruction: %fusion.116 = bf16[16,1024]{1,0} fusion(f32[] %p.10, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,8192]{1,0} %gemm_fusion_dot.31.0, bf16[1024]{0} %p.11), kind=kCustom, calls=%fused_computation.95, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","256"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<502 custom-call.17.0{} @0>
 positions:
  custom-call.17.0 {}
 uses:
  get-tuple-element.17, operand 0 {}
 from instruction: %custom-call.17.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.116, bf16[6144,1024]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<503 custom-call.17.0{0} @0>
 positions:
  custom-call.17.0 {0}
  get-tuple-element.17
 uses:
  loop_convert_fusion.7, operand 0
 from instruction: %custom-call.17.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.116, bf16[6144,1024]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<504 custom-call.17.0{1} @0>
 positions:
  custom-call.17.0 {1}
 uses:
 from instruction: %custom-call.17.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.116, bf16[6144,1024]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<505 loop_convert_fusion.7 @0>
 positions:
  loop_convert_fusion.7
 uses:
  custom-call.18.0, operand 0
 from instruction: %loop_convert_fusion.7 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.17), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<506 custom-call.18.0{} @0>
 positions:
  custom-call.18.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.18.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.13), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<507 custom-call.18.0{0} @0>
 positions:
  custom-call.18.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.114, operand 4
  fusion.112, operand 5
  loop_add_fusion, operand 4
 from instruction: %custom-call.18.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.13), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<508 custom-call.18.0{1} @0>
 positions:
  custom-call.18.0 {1}
 uses:
 from instruction: %custom-call.18.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.13), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<509 fusion.114 @0>
 positions:
  fusion.114
 uses:
  custom-call.19.0, operand 0
 from instruction: %fusion.114 = bf16[16,1024]{1,0} fusion(f32[] %p.10, bf16[1024]{0} %p.14, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,8192]{1,0} %gemm_fusion_dot.31.0, bf16[16,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.93, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","256"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<510 custom-call.19.0{} @0>
 positions:
  custom-call.19.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.19.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.114, bf16[6144,1024]{1,0} %p.15), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<511 custom-call.19.0{0} @0>
 positions:
  custom-call.19.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.19.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.114, bf16[6144,1024]{1,0} %p.15), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<512 custom-call.19.0{1} @0>
 positions:
  custom-call.19.0 {1}
 uses:
 from instruction: %custom-call.19.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.114, bf16[6144,1024]{1,0} %p.15), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<513 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.20.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<514 custom-call.20.0{} @0>
 positions:
  custom-call.20.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.20.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<515 custom-call.20.0{0} @0>
 positions:
  custom-call.20.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.112, operand 2
  loop_add_fusion, operand 2
 from instruction: %custom-call.20.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<516 custom-call.20.0{1} @0>
 positions:
  custom-call.20.0 {1}
 uses:
 from instruction: %custom-call.20.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.16), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<517 fusion.112 @0>
 positions:
  fusion.112
 uses:
  custom-call.21.0, operand 0
 from instruction: %fusion.112 = bf16[16,1024]{1,0} fusion(f32[] %p.10, bf16[1024]{0} %p.17, bf16[16,1024]{1,0} %get-tuple-element.3.0, bf16[16,8192]{1,0} %gemm_fusion_dot.31.0, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.91, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<518 custom-call.21.0{} @0>
 positions:
  custom-call.21.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.21.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.112, bf16[6144,1024]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<519 custom-call.21.0{0} @0>
 positions:
  custom-call.21.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.21.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.112, bf16[6144,1024]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<520 custom-call.21.0{1} @0>
 positions:
  custom-call.21.0 {1}
 uses:
 from instruction: %custom-call.21.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.112, bf16[6144,1024]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<521 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.22.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<522 custom-call.22.0{} @0>
 positions:
  custom-call.22.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.22.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<523 custom-call.22.0{0} @0>
 positions:
  custom-call.22.0 {0}
  get-tuple-element.5.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.22.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<524 custom-call.22.0{1} @0>
 positions:
  custom-call.22.0 {1}
 uses:
 from instruction: %custom-call.22.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.19), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<525 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.110, operand 0
  fusion.108, operand 2
  fusion.106, operand 2
  fusion.104, operand 4
  fusion.102, operand 5
  fusion.100, operand 6
 from instruction: %loop_add_fusion = f32[16,1024]{1,0} fusion(bf16[16,8192]{1,0} %gemm_fusion_dot.31.0, bf16[16,1024]{1,0} %get-tuple-element.5.0, bf16[16,1024]{1,0} %get-tuple-element.3.0, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,1024]{1,0} %get-tuple-element.1.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.67/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<526 fusion.110 @0>
 positions:
  fusion.110
 uses:
  custom-call.23.0, operand 0
 from instruction: %fusion.110 = bf16[16,1024]{1,0} fusion(f32[16,1024]{1,0} %loop_add_fusion, f32[] %p.10, bf16[1024]{0} %p.20), kind=kCustom, calls=%fused_computation.89, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<527 custom-call.23.0{} @0>
 positions:
  custom-call.23.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.23.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.110, bf16[6144,1024]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<528 custom-call.23.0{0} @0>
 positions:
  custom-call.23.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.23.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.110, bf16[6144,1024]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<529 custom-call.23.0{1} @0>
 positions:
  custom-call.23.0 {1}
 uses:
 from instruction: %custom-call.23.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.110, bf16[6144,1024]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<530 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.24.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<531 custom-call.24.0{} @0>
 positions:
  custom-call.24.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.24.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<532 custom-call.24.0{0} @0>
 positions:
  custom-call.24.0 {0}
  get-tuple-element.7.0
 uses:
  fusion.108, operand 3
  fusion.106, operand 3
  fusion.104, operand 5
  fusion.102, operand 6
  fusion.100, operand 7
 from instruction: %custom-call.24.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<533 custom-call.24.0{1} @0>
 positions:
  custom-call.24.0 {1}
 uses:
 from instruction: %custom-call.24.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.22), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<534 fusion.108 @0>
 positions:
  fusion.108
 uses:
  custom-call.25.0, operand 0
 from instruction: %fusion.108 = bf16[16,1024]{1,0} fusion(f32[] %p.10, bf16[1024]{0} %p.23, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.7.0, bf16[16,8192]{1,0} %gemm_fusion_dot.31.0), kind=kCustom, calls=%fused_computation.87, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<535 custom-call.25.0{} @0>
 positions:
  custom-call.25.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.25.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.108, bf16[6144,1024]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<536 custom-call.25.0{0} @0>
 positions:
  custom-call.25.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.25.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.108, bf16[6144,1024]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<537 custom-call.25.0{1} @0>
 positions:
  custom-call.25.0 {1}
 uses:
 from instruction: %custom-call.25.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.108, bf16[6144,1024]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<538 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.26.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<539 custom-call.26.0{} @0>
 positions:
  custom-call.26.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.26.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<540 custom-call.26.0{0} @0>
 positions:
  custom-call.26.0 {0}
  get-tuple-element.9.0
 uses:
  fusion.106, operand 5
  fusion.104, operand 6
  fusion.102, operand 7
  fusion.100, operand 8
 from instruction: %custom-call.26.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<541 custom-call.26.0{1} @0>
 positions:
  custom-call.26.0 {1}
 uses:
 from instruction: %custom-call.26.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.25), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<542 fusion.106 @0>
 positions:
  fusion.106
 uses:
  custom-call.27.0, operand 0
 from instruction: %fusion.106 = bf16[16,1024]{1,0} fusion(f32[] %p.10, bf16[1024]{0} %p.26, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.7.0, bf16[16,8192]{1,0} %gemm_fusion_dot.31.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.9.0), kind=kCustom, calls=%fused_computation.85, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<543 custom-call.27.0{} @0>
 positions:
  custom-call.27.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.27.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.106, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<544 custom-call.27.0{0} @0>
 positions:
  custom-call.27.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.27.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.106, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<545 custom-call.27.0{1} @0>
 positions:
  custom-call.27.0 {1}
 uses:
 from instruction: %custom-call.27.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.106, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<546 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.28.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<547 custom-call.28.0{} @0>
 positions:
  custom-call.28.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.28.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<548 custom-call.28.0{0} @0>
 positions:
  custom-call.28.0 {0}
  get-tuple-element.11.0
 uses:
  fusion.104, operand 2
  fusion.102, operand 2
  fusion.100, operand 3
 from instruction: %custom-call.28.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<549 custom-call.28.0{1} @0>
 positions:
  custom-call.28.0 {1}
 uses:
 from instruction: %custom-call.28.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<550 fusion.104 @0>
 positions:
  fusion.104
 uses:
  custom-call.29.0, operand 0
 from instruction: %fusion.104 = bf16[16,1024]{1,0} fusion(f32[] %p.10, bf16[1024]{0} %p.29, bf16[16,1024]{1,0} %get-tuple-element.11.0, bf16[16,8192]{1,0} %gemm_fusion_dot.31.0, f32[16,1024]{1,0} %loop_add_fusion, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.7.0, bf16[16,1024]{1,0} %get-tuple-element.9.0), kind=kCustom, calls=%fused_computation.83, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<551 custom-call.29.0{} @0>
 positions:
  custom-call.29.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.29.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.104, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<552 custom-call.29.0{0} @0>
 positions:
  custom-call.29.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.29.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.104, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<553 custom-call.29.0{1} @0>
 positions:
  custom-call.29.0 {1}
 uses:
 from instruction: %custom-call.29.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.104, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<554 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.30.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<555 custom-call.30.0{} @0>
 positions:
  custom-call.30.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.30.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<556 custom-call.30.0{0} @0>
 positions:
  custom-call.30.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.102, operand 4
  fusion.100, operand 5
 from instruction: %custom-call.30.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<557 custom-call.30.0{1} @0>
 positions:
  custom-call.30.0 {1}
 uses:
 from instruction: %custom-call.30.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<558 fusion.102 @0>
 positions:
  fusion.102
 uses:
  custom-call.31.0, operand 0
 from instruction: %fusion.102 = bf16[16,1024]{1,0} fusion(f32[] %p.10, bf16[1024]{0} %p.32, bf16[16,1024]{1,0} %get-tuple-element.11.0, bf16[16,8192]{1,0} %gemm_fusion_dot.31.0, bf16[16,1024]{1,0} %get-tuple-element.13.0, /*index=5*/f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.7.0, bf16[16,1024]{1,0} %get-tuple-element.9.0), kind=kCustom, calls=%fused_computation.81, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<559 custom-call.31.0{} @0>
 positions:
  custom-call.31.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.31.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.102, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<560 custom-call.31.0{0} @0>
 positions:
  custom-call.31.0 {0}
  get-tuple-element.14.0
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.31.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.102, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<561 custom-call.31.0{1} @0>
 positions:
  custom-call.31.0 {1}
 uses:
 from instruction: %custom-call.31.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.102, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<562 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.32.0, operand 0
 from instruction: %loop_convert_fusion = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<563 custom-call.32.0{} @0>
 positions:
  custom-call.32.0 {}
 uses:
  get-tuple-element.15.0, operand 0 {}
 from instruction: %custom-call.32.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<564 custom-call.32.0{0} @0>
 positions:
  custom-call.32.0 {0}
  get-tuple-element.15.0
 uses:
  fusion.100, operand 1
 from instruction: %custom-call.32.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<565 custom-call.32.0{1} @0>
 positions:
  custom-call.32.0 {1}
 uses:
 from instruction: %custom-call.32.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<566 fusion.100 @0>
 positions:
  fusion.100
 uses:
  custom-call.33.0, operand 0
 from instruction: %fusion.100 = bf16[16,1024]{1,0} fusion(f32[] %p.10, bf16[16,1024]{1,0} %get-tuple-element.15.0, bf16[1024]{0} %p.35, bf16[16,1024]{1,0} %get-tuple-element.11.0, bf16[16,8192]{1,0} %gemm_fusion_dot.31.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.13.0, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.7.0, bf16[16,1024]{1,0} %get-tuple-element.9.0), kind=kCustom, calls=%fused_computation.79, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<567 custom-call.33.0{} @0>
 positions:
  custom-call.33.0 {}
 uses:
  get-tuple-element.16.0, operand 0 {}
 from instruction: %custom-call.33.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.100, bf16[4096,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<568 custom-call.33.0{0} @0>
 positions:
  custom-call.33.0 {0}
  get-tuple-element.16.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.10.0, operand 1
 from instruction: %custom-call.33.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.100, bf16[4096,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<569 custom-call.33.0{1} @0>
 positions:
  custom-call.33.0 {1}
 uses:
 from instruction: %custom-call.33.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.100, bf16[4096,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<570 triton_softmax.10.0 @0>
 positions:
  triton_softmax.10.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.10.0 = f32[16,8,128]{2,1,0} fusion(f32[] %p.10, bf16[16,4096]{1,0} %get-tuple-element.16.0), kind=kCustom, calls=%triton_softmax_computation.10, metadata={op_type="aten__mul" op_name="aten__mul.149/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1","128"]}],"num_warps":"1"}},"force_earliest_schedule":false}
<571 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.736.0 {0}
  call {0}
  get-tuple-element.19
  tuple {0}
 uses:
  tuple, operand 0
  tuple.736.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[16,8,128]{2,1,0} fusion(f32[16,8,128]{2,1,0} %triton_softmax.10.0, bf16[128]{0} %p.37, bf16[40960,128]{1,0} %p.38, s32[16]{0} %p.39), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<572 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.736.0 {1}
  call {1}
  get-tuple-element.20
  bitcast.1882.0
  tuple {1}
 uses:
  bitcast.1882.0, operand 0
  tuple.736.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[16,1024]{1,0} fusion(bf16[16,4096]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<573 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.736.0 {3}
  call {2}
  get-tuple-element.21
  bitcast.1894.0
  tuple {2}
 uses:
  bitcast.1894.0, operand 0
  tuple.736.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.40), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<574 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.736.0 {2}
  bitcast.1887.0
  call {3}
  get-tuple-element.22
  tuple {3}
 uses:
  tuple, operand 3
  tuple.736.0, operand 2
  bitcast.1887.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.40), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<575 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.19, operand 0 {}
  get-tuple-element.20, operand 0 {}
  get-tuple-element.21, operand 0 {}
  get-tuple-element.22, operand 0 {}
 from instruction: %tuple = (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[16,8,128]{2,1,0} %input_concatenate_fusion, bf16[16,8,128]{2,1,0} %bitcast.1882.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.1894.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<576 p5.30.0 @0>
 positions:
  p5.30.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.30.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<577 p4.28.0 @0>
 positions:
  p4.28.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.28.0 = s32[16]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<578 p34.538.0 @0>
 positions:
  p34.538.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p34.538.0 = bf16[1024,2048]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<579 p30.466.0 @0>
 positions:
  p30.466.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p30.466.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<580 p26.394.0 @0>
 positions:
  p26.394.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p26.394.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<581 p22.322.0 @0>
 positions:
  p22.322.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p22.322.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<582 p18.250.0 @0>
 positions:
  p18.250.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p18.250.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<583 p14.178.0 @0>
 positions:
  p14.178.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p14.178.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<584 p10.106.0 @0>
 positions:
  p10.106.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p10.106.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<585 p6.34.0 @0>
 positions:
  p6.34.0
  p.9
 uses:
  call, operand 9
  wrapped_concatenate, operand 7
 from instruction: %p6.34.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<586 p1.4.0 @0>
 positions:
  p1.4.0
  p.10
 uses:
  call, operand 10
  fusion.116, operand 0
  fusion.114, operand 0
  fusion.112, operand 0
  fusion.110, operand 1
  fusion.108, operand 0
  fusion.106, operand 0
  fusion.104, operand 0
  fusion.102, operand 0
  fusion.100, operand 0
  triton_softmax.10.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<587 p9.54.0 @0>
 positions:
  p9.54.0
  p.11
 uses:
  call, operand 11
  fusion.116, operand 3
 from instruction: %p9.54.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<588 p8.52.0 @0>
 positions:
  p8.52.0
  p.12
 uses:
  call, operand 12
  custom-call.17.0, operand 1
 from instruction: %p8.52.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<589 p7.50.0 @0>
 positions:
  p7.50.0
  p.13
 uses:
  call, operand 13
  custom-call.18.0, operand 1
 from instruction: %p7.50.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<590 p13.126.0 @0>
 positions:
  p13.126.0
  p.14
 uses:
  call, operand 14
  fusion.114, operand 1
 from instruction: %p13.126.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<591 p12.124.0 @0>
 positions:
  p12.124.0
  p.15
 uses:
  call, operand 15
  custom-call.19.0, operand 1
 from instruction: %p12.124.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<592 p11.122.0 @0>
 positions:
  p11.122.0
  p.16
 uses:
  call, operand 16
  custom-call.20.0, operand 1
 from instruction: %p11.122.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<593 p17.198.0 @0>
 positions:
  p17.198.0
  p.17
 uses:
  call, operand 17
  fusion.112, operand 1
 from instruction: %p17.198.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<594 p16.196.0 @0>
 positions:
  p16.196.0
  p.18
 uses:
  call, operand 18
  custom-call.21.0, operand 1
 from instruction: %p16.196.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<595 p15.194.0 @0>
 positions:
  p15.194.0
  p.19
 uses:
  call, operand 19
  custom-call.22.0, operand 1
 from instruction: %p15.194.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<596 p21.270.0 @0>
 positions:
  p21.270.0
  p.20
 uses:
  call, operand 20
  fusion.110, operand 2
 from instruction: %p21.270.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<597 p20.268.0 @0>
 positions:
  p20.268.0
  p.21
 uses:
  call, operand 21
  custom-call.23.0, operand 1
 from instruction: %p20.268.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<598 p19.266.0 @0>
 positions:
  p19.266.0
  p.22
 uses:
  call, operand 22
  custom-call.24.0, operand 1
 from instruction: %p19.266.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<599 p25.342.0 @0>
 positions:
  p25.342.0
  p.23
 uses:
  call, operand 23
  fusion.108, operand 1
 from instruction: %p25.342.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<600 p24.340.0 @0>
 positions:
  p24.340.0
  p.24
 uses:
  call, operand 24
  custom-call.25.0, operand 1
 from instruction: %p24.340.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<601 p23.338.0 @0>
 positions:
  p23.338.0
  p.25
 uses:
  call, operand 25
  custom-call.26.0, operand 1
 from instruction: %p23.338.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<602 p29.414.0 @0>
 positions:
  p29.414.0
  p.26
 uses:
  call, operand 26
  fusion.106, operand 1
 from instruction: %p29.414.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<603 p28.412.0 @0>
 positions:
  p28.412.0
  p.27
 uses:
  call, operand 27
  custom-call.27.0, operand 1
 from instruction: %p28.412.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<604 p27.410.0 @0>
 positions:
  p27.410.0
  p.28
 uses:
  call, operand 28
  custom-call.28.0, operand 1
 from instruction: %p27.410.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<605 p33.486.0 @0>
 positions:
  p33.486.0
  p.29
 uses:
  call, operand 29
  fusion.104, operand 1
 from instruction: %p33.486.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<606 p32.484.0 @0>
 positions:
  p32.484.0
  p.30
 uses:
  call, operand 30
  custom-call.29.0, operand 1
 from instruction: %p32.484.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<607 p31.482.0 @0>
 positions:
  p31.482.0
  p.31
 uses:
  call, operand 31
  custom-call.30.0, operand 1
 from instruction: %p31.482.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<608 p37.558.0 @0>
 positions:
  p37.558.0
  p.32
 uses:
  call, operand 32
  fusion.102, operand 1
 from instruction: %p37.558.0 = bf16[1024]{0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<609 p36.556.0 @0>
 positions:
  p36.556.0
  p.33
 uses:
  call, operand 33
  custom-call.31.0, operand 1
 from instruction: %p36.556.0 = bf16[6144,1024]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<610 p35.554.0 @0>
 positions:
  p35.554.0
  p.34
 uses:
  call, operand 34
  custom-call.32.0, operand 1
 from instruction: %p35.554.0 = bf16[1024,3072]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<611 p3.8.0 @0>
 positions:
  p3.8.0
  p.35
 uses:
  call, operand 35
  fusion.100, operand 2
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<612 p2.6.0 @0>
 positions:
  p2.6.0
  p.36
 uses:
  call, operand 36
  custom-call.33.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<613 p0.1.0 @0>
 positions:
  p0.1.0
  p.37
 uses:
  call, operand 37
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<614 p39.683.0 @0>
 positions:
  p39.683.0
  p.38
 uses:
  call, operand 38
  input_concatenate_fusion, operand 2
 from instruction: %p39.683.0 = bf16[40960,128]{1,0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<615 p38.682.0 @0>
 positions:
  p38.682.0
  p.39
 uses:
  call, operand 39
  input_concatenate_fusion, operand 3
 from instruction: %p38.682.0 = s32[16]{0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<616 p40.727.0 @0>
 positions:
  p40.727.0
  p.40
 uses:
  call, operand 40
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p40.727.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<617 tuple.736.0{} @0>
 positions:
  tuple.736.0 {}
 uses:
 from instruction: %tuple.736.0 = (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[16,8,128]{2,1,0} %get-tuple-element.19, bf16[16,8,128]{2,1,0} %get-tuple-element.20, bf16[4233,16,8,128]{3,2,1,0} %bitcast.1887.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.21)


HloLiveRange (max 152):
  InstructionSequence:
    0:p1.4.0
    1:p40.727.0
    2:p39.683.0
    3:p38.682.0
    4:p37.558.0
    5:p36.556.0
    6:p35.554.0
    7:p34.538.0
    8:p33.486.0
    9:p32.484.0
    10:p31.482.0
    11:p30.466.0
    12:p29.414.0
    13:p28.412.0
    14:p27.410.0
    15:p26.394.0
    16:p25.342.0
    17:p24.340.0
    18:p23.338.0
    19:p22.322.0
    20:p21.270.0
    21:p20.268.0
    22:p19.266.0
    23:p18.250.0
    24:p17.198.0
    25:p16.196.0
    26:p15.194.0
    27:p14.178.0
    28:p13.126.0
    29:p12.124.0
    30:p11.122.0
    31:p10.106.0
    32:p9.54.0
    33:p8.52.0
    34:p7.50.0
    35:p6.34.0
    36:p5.30.0
    37:p4.28.0
    38:p3.8.0
    39:p2.6.0
    40:p0.1.0
    41:p
    42:p.1
    43:p.2
    44:p.3
    45:p.4
    46:p.5
    47:p.6
    48:p.7
    49:p.8
    50:p.9
    51:p.10
    52:p.11
    53:p.12
    54:p.13
    55:p.14
    56:p.15
    57:p.16
    58:p.17
    59:p.18
    60:p.19
    61:p.20
    62:p.21
    63:p.22
    64:p.23
    65:p.24
    66:p.25
    67:p.26
    68:p.27
    69:p.28
    70:p.29
    71:p.30
    72:p.31
    73:p.32
    74:p.33
    75:p.34
    76:p.35
    77:p.36
    78:p.37
    79:p.38
    80:p.39
    81:p.40
    82:loop_gather_fusion
    83:wrapped_concatenate
    84:gemm_fusion_dot.31.0
    85:fusion.116
    86:custom-call.17.0
    87:get-tuple-element.17
    88:loop_convert_fusion.7
    89:custom-call.18.0
    90:get-tuple-element.1.0
    91:fusion.114
    92:custom-call.19.0
    93:get-tuple-element.2.0
    94:loop_convert_fusion.6
    95:custom-call.20.0
    96:get-tuple-element.3.0
    97:fusion.112
    98:custom-call.21.0
    99:get-tuple-element.4.0
    100:loop_convert_fusion.5
    101:custom-call.22.0
    102:get-tuple-element.5.0
    103:loop_add_fusion
    104:fusion.110
    105:custom-call.23.0
    106:get-tuple-element.6.0
    107:loop_convert_fusion.4
    108:custom-call.24.0
    109:get-tuple-element.7.0
    110:fusion.108
    111:custom-call.25.0
    112:get-tuple-element.8.0
    113:loop_convert_fusion.3
    114:custom-call.26.0
    115:get-tuple-element.9.0
    116:fusion.106
    117:custom-call.27.0
    118:get-tuple-element.10.0
    119:loop_convert_fusion.2
    120:custom-call.28.0
    121:get-tuple-element.11.0
    122:fusion.104
    123:custom-call.29.0
    124:get-tuple-element.12.0
    125:loop_convert_fusion.1
    126:custom-call.30.0
    127:get-tuple-element.13.0
    128:fusion.102
    129:custom-call.31.0
    130:get-tuple-element.14.0
    131:loop_convert_fusion
    132:custom-call.32.0
    133:get-tuple-element.15.0
    134:fusion.100
    135:custom-call.33.0
    136:get-tuple-element.16.0
    137:wrapped_slice
    138:triton_softmax.10.0
    139:input_concatenate_fusion
    140:bitcast.1882.0
    141:loop_slice_fusion
    142:bitcast.1894.0
    143:wrapped_slice.1
    144:tuple
    145:call
    146:get-tuple-element.19
    147:get-tuple-element.20
    148:get-tuple-element.21
    149:get-tuple-element.22
    150:bitcast.1887.0
    151:tuple.736.0
  BufferLiveRange:
    wrapped_concatenate{}:83-84
    gemm_fusion_dot.31.0{}:84-134
    loop_gather_fusion{}:82-103
    fusion.116{}:85-86
    custom-call.17.0{}:86-87
    custom-call.17.0{0}:86-88
    custom-call.17.0{1}:86-86
    loop_convert_fusion.7{}:88-89
    custom-call.18.0{}:89-90
    custom-call.18.0{0}:89-103
    custom-call.18.0{1}:89-89
    fusion.114{}:91-92
    custom-call.19.0{}:92-93
    custom-call.19.0{0}:92-94
    custom-call.19.0{1}:92-92
    loop_convert_fusion.6{}:94-95
    custom-call.20.0{}:95-96
    custom-call.20.0{0}:95-103
    custom-call.20.0{1}:95-95
    fusion.112{}:97-98
    custom-call.21.0{}:98-99
    custom-call.21.0{0}:98-100
    custom-call.21.0{1}:98-98
    loop_convert_fusion.5{}:100-101
    custom-call.22.0{}:101-102
    custom-call.22.0{0}:101-103
    custom-call.22.0{1}:101-101
    loop_add_fusion{}:103-134
    fusion.110{}:104-105
    custom-call.23.0{}:105-106
    custom-call.23.0{0}:105-107
    custom-call.23.0{1}:105-105
    loop_convert_fusion.4{}:107-108
    custom-call.24.0{}:108-109
    custom-call.24.0{0}:108-134
    custom-call.24.0{1}:108-108
    fusion.108{}:110-111
    custom-call.25.0{}:111-112
    custom-call.25.0{0}:111-113
    custom-call.25.0{1}:111-111
    loop_convert_fusion.3{}:113-114
    custom-call.26.0{}:114-115
    custom-call.26.0{0}:114-134
    custom-call.26.0{1}:114-114
    fusion.106{}:116-117
    custom-call.27.0{}:117-118
    custom-call.27.0{0}:117-119
    custom-call.27.0{1}:117-117
    loop_convert_fusion.2{}:119-120
    custom-call.28.0{}:120-121
    custom-call.28.0{0}:120-134
    custom-call.28.0{1}:120-120
    fusion.104{}:122-123
    custom-call.29.0{}:123-124
    custom-call.29.0{0}:123-125
    custom-call.29.0{1}:123-123
    loop_convert_fusion.1{}:125-126
    custom-call.30.0{}:126-127
    custom-call.30.0{0}:126-134
    custom-call.30.0{1}:126-126
    fusion.102{}:128-129
    custom-call.31.0{}:129-130
    custom-call.31.0{0}:129-131
    custom-call.31.0{1}:129-129
    loop_convert_fusion{}:131-132
    custom-call.32.0{}:132-133
    custom-call.32.0{0}:132-134
    custom-call.32.0{1}:132-132
    fusion.100{}:134-135
    custom-call.33.0{}:135-136
    custom-call.33.0{0}:135-138
    custom-call.33.0{1}:135-135
    triton_softmax.10.0{}:138-139
    input_concatenate_fusion{}:139-152
    wrapped_slice{}:137-152
    loop_slice_fusion{}:141-152
    wrapped_slice.1{}:143-152
    tuple{}:144-149
    p5.30.0{}:0-152
    p4.28.0{}:0-152
    p34.538.0{}:0-152
    p30.466.0{}:0-152
    p26.394.0{}:0-152
    p22.322.0{}:0-152
    p18.250.0{}:0-152
    p14.178.0{}:0-152
    p10.106.0{}:0-152
    p6.34.0{}:0-152
    p1.4.0{}:0-152
    p9.54.0{}:0-152
    p8.52.0{}:0-152
    p7.50.0{}:0-152
    p13.126.0{}:0-152
    p12.124.0{}:0-152
    p11.122.0{}:0-152
    p17.198.0{}:0-152
    p16.196.0{}:0-152
    p15.194.0{}:0-152
    p21.270.0{}:0-152
    p20.268.0{}:0-152
    p19.266.0{}:0-152
    p25.342.0{}:0-152
    p24.340.0{}:0-152
    p23.338.0{}:0-152
    p29.414.0{}:0-152
    p28.412.0{}:0-152
    p27.410.0{}:0-152
    p33.486.0{}:0-152
    p32.484.0{}:0-152
    p31.482.0{}:0-152
    p37.558.0{}:0-152
    p36.556.0{}:0-152
    p35.554.0{}:0-152
    p3.8.0{}:0-152
    p2.6.0{}:0-152
    p0.1.0{}:0-152
    p39.683.0{}:0-152
    p38.682.0{}:0-152
    p40.727.0{}:0-152
    tuple.736.0{}:151-152
  Live ranges at 144 (peak):
    input_concatenate_fusion: 32768 bytes
    wrapped_slice: 32768 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.30.0: 311164928 bytes
    p4.28.0: 64 bytes
    p34.538.0: 4194304 bytes
    p30.466.0: 4194304 bytes
    p26.394.0: 4194304 bytes
    p22.322.0: 4194304 bytes
    p18.250.0: 4194304 bytes
    p14.178.0: 4194304 bytes
    p10.106.0: 4194304 bytes
    p6.34.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.54.0: 2048 bytes
    p8.52.0: 12582912 bytes
    p7.50.0: 6291456 bytes
    p13.126.0: 2048 bytes
    p12.124.0: 12582912 bytes
    p11.122.0: 6291456 bytes
    p17.198.0: 2048 bytes
    p16.196.0: 12582912 bytes
    p15.194.0: 6291456 bytes
    p21.270.0: 2048 bytes
    p20.268.0: 12582912 bytes
    p19.266.0: 6291456 bytes
    p25.342.0: 2048 bytes
    p24.340.0: 12582912 bytes
    p23.338.0: 6291456 bytes
    p29.414.0: 2048 bytes
    p28.412.0: 12582912 bytes
    p27.410.0: 6291456 bytes
    p33.486.0: 2048 bytes
    p32.484.0: 12582912 bytes
    p31.482.0: 6291456 bytes
    p37.558.0: 2048 bytes
    p36.556.0: 12582912 bytes
    p35.554.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p39.683.0: 10485760 bytes
    p38.682.0: 64 bytes
    p40.727.0: 277413888 bytes
