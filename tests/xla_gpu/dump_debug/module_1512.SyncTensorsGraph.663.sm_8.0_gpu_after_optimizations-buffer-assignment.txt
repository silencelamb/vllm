BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <508 p5.28.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 36, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <544 p36.653.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <439 gemm_fusion_dot.27.0 @0> (size=458752,offset=0): bf16[32,7168]{1,0}
 value: <500 custom-call.29.0{0} @0> (size=262144,offset=0): bf16[32,4096]{1,0}
 value: <505 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <438 wrapped_concatenate @0> (size=29360128,offset=0): bf16[7168,2048]{1,0}
 value: <444 custom-call.15.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <448 custom-call.16.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <452 custom-call.17.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <456 custom-call.18.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <460 custom-call.19.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <464 custom-call.20.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <468 custom-call.21.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <472 custom-call.22.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <477 custom-call.23.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <481 custom-call.24.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <485 custom-call.25.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <489 custom-call.26.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <493 custom-call.27.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <497 custom-call.28.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <501 custom-call.29.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <502 fusion.89 @0> (size=131072,offset=0): f32[32,8,128]{2,1,0}
 value: <506 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <519 p8.50.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <522 p12.122.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <525 p16.194.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 12582912, parameter 20, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <528 p20.266.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 8: size 12582912, parameter 24, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <531 p24.338.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 9: size 12582912, parameter 28, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <534 p28.410.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 10: size 12582912, parameter 32, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <537 p32.482.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 11: size 10485760, parameter 35, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <542 p35.609.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 12: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <540 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 13: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <520 p7.48.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 14: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <523 p11.120.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 15: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <526 p15.192.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 16: size 6291456, parameter 19, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <529 p19.264.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 17: size 6291456, parameter 23, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <532 p23.336.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 18: size 6291456, parameter 27, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <535 p27.408.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 19: size 6291456, parameter 31, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <538 p31.480.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 20: size 4194304, parameter 30, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <510 p30.464.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 21: size 4194304, parameter 26, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <511 p26.392.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 22: size 4194304, parameter 22, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <512 p22.320.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 23: size 4194304, parameter 18, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <513 p18.248.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 24: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <514 p14.176.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 25: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <515 p10.104.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 26: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <516 p6.32.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 27: size 65536, maybe-live-out:
 value: <441 fusion.98 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <447 custom-call.16.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <482 fusion.103 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <488 custom-call.26.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <503 input_concatenate_fusion @0> (size=65536,offset=0): bf16[32,8,128]{2,1,0}
allocation 28: size 65536, maybe-live-out:
 value: <440 loop_gather_fusion @0> (size=65536,offset=0): bf16[32,1,1024]{2,0,1}
 value: <474 fusion.102 @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <480 custom-call.24.0{0} @0> (size=65536,offset=0): bf16[32,1024]{1,0}
 value: <504 wrapped_slice @0> (size=65536,offset=0): bf16[32,1024]{1,0}
allocation 29: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <518 p9.52.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 30: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <521 p13.124.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 31: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <524 p17.196.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 32: size 2048, parameter 21, shape |bf16[1024]| at ShapeIndex {}:
 value: <527 p21.268.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 33: size 2048, parameter 25, shape |bf16[1024]| at ShapeIndex {}:
 value: <530 p25.340.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 34: size 2048, parameter 29, shape |bf16[1024]| at ShapeIndex {}:
 value: <533 p29.412.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 35: size 2048, parameter 33, shape |bf16[1024]| at ShapeIndex {}:
 value: <536 p33.484.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 36: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <539 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 37: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <541 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 38: size 128, parameter 4, shape |s32[32]| at ShapeIndex {}:
 value: <509 p4.26.0 @0> (size=128,offset=0): s32[32]{0}
allocation 39: size 128, parameter 34, shape |s32[32]| at ShapeIndex {}:
 value: <543 p34.608.0 @0> (size=128,offset=0): s32[32]{0}
allocation 40: size 32, output shape is |(bf16[32,8,128], bf16[32,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <545 tuple.662.0{} @0> (size=32,offset=0): (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 41: size 4, thread-local:
 value: <19 add.102 @0> (size=4,offset=0): f32[]
allocation 42: size 4, thread-local:
 value: <18 y.62 @0> (size=4,offset=0): f32[]
allocation 43: size 4, thread-local:
 value: <17 x.61 @0> (size=4,offset=0): f32[]
allocation 44: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <517 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 45: size 853920, preallocated-temp:
 value: <442 custom-call.15.0{} @0> (size=16,offset=853760): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <443 custom-call.15.0{0} @0> (size=393216,offset=1664): bf16[32,6144]{1,0}
 value: <445 loop_convert_fusion @0> (size=196608,offset=394880): bf16[32,3072]{1,0}
 value: <446 custom-call.16.0{} @0> (size=16,offset=853632): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <449 fusion.99 @0> (size=65536,offset=394880): bf16[32,1024]{1,0}
 value: <450 custom-call.17.0{} @0> (size=16,offset=1536): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <451 custom-call.17.0{0} @0> (size=393216,offset=1664): bf16[32,6144]{1,0}
 value: <453 loop_convert_fusion.1 @0> (size=196608,offset=394880): bf16[32,3072]{1,0}
 value: <454 custom-call.18.0{} @0> (size=16,offset=0): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <455 custom-call.18.0{0} @0> (size=65536,offset=722560): bf16[32,1024]{1,0}
 value: <457 fusion.100 @0> (size=65536,offset=394880): bf16[32,1024]{1,0}
 value: <458 custom-call.19.0{} @0> (size=16,offset=128): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <459 custom-call.19.0{0} @0> (size=393216,offset=1664): bf16[32,6144]{1,0}
 value: <461 loop_convert_fusion.2 @0> (size=196608,offset=394880): bf16[32,3072]{1,0}
 value: <462 custom-call.20.0{} @0> (size=16,offset=256): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <463 custom-call.20.0{0} @0> (size=65536,offset=788096): bf16[32,1024]{1,0}
 value: <465 fusion.101 @0> (size=65536,offset=394880): bf16[32,1024]{1,0}
 value: <466 custom-call.21.0{} @0> (size=16,offset=384): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <467 custom-call.21.0{0} @0> (size=393216,offset=1664): bf16[32,6144]{1,0}
 value: <469 loop_convert_fusion.3 @0> (size=196608,offset=394880): bf16[32,3072]{1,0}
 value: <470 custom-call.22.0{} @0> (size=16,offset=512): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <471 custom-call.22.0{0} @0> (size=65536,offset=1664): bf16[32,1024]{1,0}
 value: <473 loop_add_fusion @0> (size=131072,offset=591488): f32[32,1024]{1,0}
 value: <475 custom-call.23.0{} @0> (size=16,offset=640): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <476 custom-call.23.0{0} @0> (size=393216,offset=1664): bf16[32,6144]{1,0}
 value: <478 loop_convert_fusion.4 @0> (size=196608,offset=394880): bf16[32,3072]{1,0}
 value: <479 custom-call.24.0{} @0> (size=16,offset=768): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <483 custom-call.25.0{} @0> (size=16,offset=896): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <484 custom-call.25.0{0} @0> (size=393216,offset=1664): bf16[32,6144]{1,0}
 value: <486 loop_convert_fusion.5 @0> (size=196608,offset=394880): bf16[32,3072]{1,0}
 value: <487 custom-call.26.0{} @0> (size=16,offset=1024): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <490 fusion.104 @0> (size=65536,offset=394880): bf16[32,1024]{1,0}
 value: <491 custom-call.27.0{} @0> (size=16,offset=1152): (bf16[32,6144]{1,0}, s8[4194304]{0})
 value: <492 custom-call.27.0{0} @0> (size=393216,offset=1664): bf16[32,6144]{1,0}
 value: <494 loop_convert_fusion.6 @0> (size=196608,offset=394880): bf16[32,3072]{1,0}
 value: <495 custom-call.28.0{} @0> (size=16,offset=1280): (bf16[32,1024]{1,0}, s8[4194304]{0})
 value: <496 custom-call.28.0{0} @0> (size=65536,offset=1664): bf16[32,1024]{1,0}
 value: <498 fusion.105 @0> (size=65536,offset=67200): bf16[32,1024]{1,0}
 value: <499 custom-call.29.0{} @0> (size=16,offset=1408): (bf16[32,4096]{1,0}, s8[4194304]{0})
 value: <507 tuple{} @0> (size=32,offset=853888): (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 1047349712 (998.83MiB)

Used values:
<17 x.61 @0>
 positions:
  x.61
 uses:
  add.102, operand 0
 from instruction: %x.61 = f32[] parameter(0)
<18 y.62 @0>
 positions:
  y.62
 uses:
  add.102, operand 1
 from instruction: %y.62 = f32[] parameter(1)
<19 add.102 @0>
 positions:
  add.102
 uses:
 from instruction: %add.102 = f32[] add(f32[] %x.61, f32[] %y.62)
<438 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.27.0, operand 0
 from instruction: %wrapped_concatenate = bf16[7168,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<439 gemm_fusion_dot.27.0 @0>
 positions:
  gemm_fusion_dot.27.0
 uses:
  fusion.98, operand 2
  fusion.99, operand 3
  fusion.100, operand 3
  fusion.101, operand 3
  loop_add_fusion, operand 0
  fusion.103, operand 4
  fusion.104, operand 4
  fusion.105, operand 5
 from instruction: %gemm_fusion_dot.27.0 = bf16[32,7168]{1,0} fusion(bf16[7168,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.27_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"128","block_n":"32","block_k":"128","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<440 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.98, operand 1
  fusion.99, operand 2
  fusion.100, operand 2
  fusion.101, operand 4
  loop_add_fusion, operand 3
 from instruction: %loop_gather_fusion = bf16[32,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[32]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<441 fusion.98 @0>
 positions:
  fusion.98
 uses:
  custom-call.15.0, operand 0
 from instruction: %fusion.98 = bf16[32,1024]{1,0} fusion(f32[] %p.9, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,7168]{1,0} %gemm_fusion_dot.27.0, bf16[1024]{0} %p.10), kind=kCustom, calls=%fused_computation.79, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<442 custom-call.15.0{} @0>
 positions:
  custom-call.15.0 {}
 uses:
  get-tuple-element.15, operand 0 {}
 from instruction: %custom-call.15.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.98, bf16[6144,1024]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<443 custom-call.15.0{0} @0>
 positions:
  custom-call.15.0 {0}
  get-tuple-element.15
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.15.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.98, bf16[6144,1024]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<444 custom-call.15.0{1} @0>
 positions:
  custom-call.15.0 {1}
 uses:
 from instruction: %custom-call.15.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.98, bf16[6144,1024]{1,0} %p.11), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<445 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.16.0, operand 0
 from instruction: %loop_convert_fusion = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.15), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<446 custom-call.16.0{} @0>
 positions:
  custom-call.16.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.16.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<447 custom-call.16.0{0} @0>
 positions:
  custom-call.16.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.99, operand 4
  fusion.100, operand 4
  fusion.101, operand 5
  loop_add_fusion, operand 4
 from instruction: %custom-call.16.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<448 custom-call.16.0{1} @0>
 positions:
  custom-call.16.0 {1}
 uses:
 from instruction: %custom-call.16.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.12), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<449 fusion.99 @0>
 positions:
  fusion.99
 uses:
  custom-call.17.0, operand 0
 from instruction: %fusion.99 = bf16[32,1024]{1,0} fusion(f32[] %p.9, bf16[1024]{0} %p.13, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,7168]{1,0} %gemm_fusion_dot.27.0, bf16[32,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.80, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<450 custom-call.17.0{} @0>
 positions:
  custom-call.17.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.17.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.99, bf16[6144,1024]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<451 custom-call.17.0{0} @0>
 positions:
  custom-call.17.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.17.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.99, bf16[6144,1024]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<452 custom-call.17.0{1} @0>
 positions:
  custom-call.17.0 {1}
 uses:
 from instruction: %custom-call.17.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.99, bf16[6144,1024]{1,0} %p.14), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<453 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.18.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<454 custom-call.18.0{} @0>
 positions:
  custom-call.18.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.18.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.15), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<455 custom-call.18.0{0} @0>
 positions:
  custom-call.18.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.100, operand 5
  fusion.101, operand 6
  loop_add_fusion, operand 5
 from instruction: %custom-call.18.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.15), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<456 custom-call.18.0{1} @0>
 positions:
  custom-call.18.0 {1}
 uses:
 from instruction: %custom-call.18.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.15), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<457 fusion.100 @0>
 positions:
  fusion.100
 uses:
  custom-call.19.0, operand 0
 from instruction: %fusion.100 = bf16[32,1024]{1,0} fusion(f32[] %p.9, bf16[1024]{0} %p.16, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,7168]{1,0} %gemm_fusion_dot.27.0, bf16[32,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.81, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<458 custom-call.19.0{} @0>
 positions:
  custom-call.19.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.19.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.100, bf16[6144,1024]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<459 custom-call.19.0{0} @0>
 positions:
  custom-call.19.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.19.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.100, bf16[6144,1024]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<460 custom-call.19.0{1} @0>
 positions:
  custom-call.19.0 {1}
 uses:
 from instruction: %custom-call.19.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.100, bf16[6144,1024]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<461 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.20.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<462 custom-call.20.0{} @0>
 positions:
  custom-call.20.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.20.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<463 custom-call.20.0{0} @0>
 positions:
  custom-call.20.0 {0}
  get-tuple-element.5.0
 uses:
  fusion.101, operand 2
  loop_add_fusion, operand 2
 from instruction: %custom-call.20.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<464 custom-call.20.0{1} @0>
 positions:
  custom-call.20.0 {1}
 uses:
 from instruction: %custom-call.20.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<465 fusion.101 @0>
 positions:
  fusion.101
 uses:
  custom-call.21.0, operand 0
 from instruction: %fusion.101 = bf16[32,1024]{1,0} fusion(f32[] %p.9, bf16[1024]{0} %p.19, bf16[32,1024]{1,0} %get-tuple-element.5.0, bf16[32,7168]{1,0} %gemm_fusion_dot.27.0, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.1.0, bf16[32,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.82, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<466 custom-call.21.0{} @0>
 positions:
  custom-call.21.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.21.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.101, bf16[6144,1024]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<467 custom-call.21.0{0} @0>
 positions:
  custom-call.21.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.21.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.101, bf16[6144,1024]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<468 custom-call.21.0{1} @0>
 positions:
  custom-call.21.0 {1}
 uses:
 from instruction: %custom-call.21.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.101, bf16[6144,1024]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<469 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.22.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<470 custom-call.22.0{} @0>
 positions:
  custom-call.22.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.22.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<471 custom-call.22.0{0} @0>
 positions:
  custom-call.22.0 {0}
  get-tuple-element.7.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.22.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<472 custom-call.22.0{1} @0>
 positions:
  custom-call.22.0 {1}
 uses:
 from instruction: %custom-call.22.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<473 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.102, operand 0
  fusion.103, operand 2
  fusion.104, operand 2
  fusion.105, operand 3
 from instruction: %loop_add_fusion = f32[32,1024]{1,0} fusion(bf16[32,7168]{1,0} %gemm_fusion_dot.27.0, bf16[32,1024]{1,0} %get-tuple-element.7.0, bf16[32,1024]{1,0} %get-tuple-element.5.0, bf16[32,1,1024]{2,0,1} %loop_gather_fusion, bf16[32,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.3.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.592/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<474 fusion.102 @0>
 positions:
  fusion.102
 uses:
  custom-call.23.0, operand 0
 from instruction: %fusion.102 = bf16[32,1024]{1,0} fusion(f32[32,1024]{1,0} %loop_add_fusion, f32[] %p.9, bf16[1024]{0} %p.22), kind=kCustom, calls=%fused_computation.83, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<475 custom-call.23.0{} @0>
 positions:
  custom-call.23.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.23.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.102, bf16[6144,1024]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<476 custom-call.23.0{0} @0>
 positions:
  custom-call.23.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.23.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.102, bf16[6144,1024]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<477 custom-call.23.0{1} @0>
 positions:
  custom-call.23.0 {1}
 uses:
 from instruction: %custom-call.23.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.102, bf16[6144,1024]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<478 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.24.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<479 custom-call.24.0{} @0>
 positions:
  custom-call.24.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.24.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<480 custom-call.24.0{0} @0>
 positions:
  custom-call.24.0 {0}
  get-tuple-element.9.0
 uses:
  fusion.103, operand 3
  fusion.104, operand 3
  fusion.105, operand 4
 from instruction: %custom-call.24.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<481 custom-call.24.0{1} @0>
 positions:
  custom-call.24.0 {1}
 uses:
 from instruction: %custom-call.24.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<482 fusion.103 @0>
 positions:
  fusion.103
 uses:
  custom-call.25.0, operand 0
 from instruction: %fusion.103 = bf16[32,1024]{1,0} fusion(f32[] %p.9, bf16[1024]{0} %p.25, f32[32,1024]{1,0} %loop_add_fusion, bf16[32,1024]{1,0} %get-tuple-element.9.0, bf16[32,7168]{1,0} %gemm_fusion_dot.27.0), kind=kCustom, calls=%fused_computation.84, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<483 custom-call.25.0{} @0>
 positions:
  custom-call.25.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.25.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.103, bf16[6144,1024]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<484 custom-call.25.0{0} @0>
 positions:
  custom-call.25.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.25.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.103, bf16[6144,1024]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<485 custom-call.25.0{1} @0>
 positions:
  custom-call.25.0 {1}
 uses:
 from instruction: %custom-call.25.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.103, bf16[6144,1024]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<486 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.26.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<487 custom-call.26.0{} @0>
 positions:
  custom-call.26.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.26.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<488 custom-call.26.0{0} @0>
 positions:
  custom-call.26.0 {0}
  get-tuple-element.11.0
 uses:
  fusion.104, operand 5
  fusion.105, operand 6
 from instruction: %custom-call.26.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<489 custom-call.26.0{1} @0>
 positions:
  custom-call.26.0 {1}
 uses:
 from instruction: %custom-call.26.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<490 fusion.104 @0>
 positions:
  fusion.104
 uses:
  custom-call.27.0, operand 0
 from instruction: %fusion.104 = bf16[32,1024]{1,0} fusion(f32[] %p.9, bf16[1024]{0} %p.28, f32[32,1024]{1,0} %loop_add_fusion, bf16[32,1024]{1,0} %get-tuple-element.9.0, bf16[32,7168]{1,0} %gemm_fusion_dot.27.0, /*index=5*/bf16[32,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.85, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<491 custom-call.27.0{} @0>
 positions:
  custom-call.27.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.27.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.104, bf16[6144,1024]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<492 custom-call.27.0{0} @0>
 positions:
  custom-call.27.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.27.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.104, bf16[6144,1024]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<493 custom-call.27.0{1} @0>
 positions:
  custom-call.27.0 {1}
 uses:
 from instruction: %custom-call.27.0 = (bf16[32,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.104, bf16[6144,1024]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<494 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.28.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[32,3072]{1,0} fusion(bf16[32,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<495 custom-call.28.0{} @0>
 positions:
  custom-call.28.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.28.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<496 custom-call.28.0{0} @0>
 positions:
  custom-call.28.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.105, operand 1
 from instruction: %custom-call.28.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<497 custom-call.28.0{1} @0>
 positions:
  custom-call.28.0 {1}
 uses:
 from instruction: %custom-call.28.0 = (bf16[32,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[32,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"98304","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<498 fusion.105 @0>
 positions:
  fusion.105
 uses:
  custom-call.29.0, operand 0
 from instruction: %fusion.105 = bf16[32,1024]{1,0} fusion(f32[] %p.9, bf16[32,1024]{1,0} %get-tuple-element.13.0, bf16[1024]{0} %p.31, f32[32,1024]{1,0} %loop_add_fusion, bf16[32,1024]{1,0} %get-tuple-element.9.0, /*index=5*/bf16[32,7168]{1,0} %gemm_fusion_dot.27.0, bf16[32,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.86, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<499 custom-call.29.0{} @0>
 positions:
  custom-call.29.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.29.0 = (bf16[32,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.105, bf16[4096,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<500 custom-call.29.0{0} @0>
 positions:
  custom-call.29.0 {0}
  get-tuple-element.14.0
 uses:
  wrapped_slice, operand 0
  fusion.89, operand 1
 from instruction: %custom-call.29.0 = (bf16[32,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.105, bf16[4096,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<501 custom-call.29.0{1} @0>
 positions:
  custom-call.29.0 {1}
 uses:
 from instruction: %custom-call.29.0 = (bf16[32,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[32,1024]{1,0} %fusion.105, bf16[4096,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"32768","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<502 fusion.89 @0>
 positions:
  fusion.89
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %fusion.89 = f32[32,8,128]{2,1,0} fusion(f32[] %p.9, bf16[32,4096]{1,0} %get-tuple-element.14.0, bf16[128]{0} %p.33), kind=kCustom, calls=%fused_computation.70, metadata={op_type="aten__mul" op_name="aten__mul.639/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<503 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.662.0 {0}
  call {0}
  get-tuple-element.17
  tuple {0}
 uses:
  tuple, operand 0
  tuple.662.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[32,8,128]{2,1,0} fusion(f32[32,8,128]{2,1,0} %fusion.89, bf16[40960,128]{1,0} %p.34, s32[32]{0} %p.35), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<504 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.662.0 {1}
  call {1}
  get-tuple-element.18
  bitcast.1696.0
  tuple {1}
 uses:
  bitcast.1696.0, operand 0
  tuple.662.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[32,1024]{1,0} fusion(bf16[32,4096]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<505 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.662.0 {3}
  call {2}
  get-tuple-element.19
  bitcast.1708.0
  tuple {2}
 uses:
  bitcast.1708.0, operand 0
  tuple.662.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.36), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<506 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.662.0 {2}
  bitcast.1701.0
  call {3}
  get-tuple-element.20
  tuple {3}
 uses:
  tuple, operand 3
  tuple.662.0, operand 2
  bitcast.1701.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.36), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<507 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.17, operand 0 {}
  get-tuple-element.18, operand 0 {}
  get-tuple-element.19, operand 0 {}
  get-tuple-element.20, operand 0 {}
 from instruction: %tuple = (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[32,8,128]{2,1,0} %input_concatenate_fusion, bf16[32,8,128]{2,1,0} %bitcast.1696.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.1708.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<508 p5.28.0 @0>
 positions:
  p5.28.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.28.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<509 p4.26.0 @0>
 positions:
  p4.26.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.26.0 = s32[32]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<510 p30.464.0 @0>
 positions:
  p30.464.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p30.464.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<511 p26.392.0 @0>
 positions:
  p26.392.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p26.392.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<512 p22.320.0 @0>
 positions:
  p22.320.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p22.320.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<513 p18.248.0 @0>
 positions:
  p18.248.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p18.248.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<514 p14.176.0 @0>
 positions:
  p14.176.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p14.176.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<515 p10.104.0 @0>
 positions:
  p10.104.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p10.104.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<516 p6.32.0 @0>
 positions:
  p6.32.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p6.32.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<517 p1.4.0 @0>
 positions:
  p1.4.0
  p.9
 uses:
  call, operand 9
  fusion.98, operand 0
  fusion.99, operand 0
  fusion.100, operand 0
  fusion.101, operand 0
  fusion.102, operand 1
  fusion.103, operand 0
  fusion.104, operand 0
  fusion.105, operand 0
  fusion.89, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<518 p9.52.0 @0>
 positions:
  p9.52.0
  p.10
 uses:
  call, operand 10
  fusion.98, operand 3
 from instruction: %p9.52.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<519 p8.50.0 @0>
 positions:
  p8.50.0
  p.11
 uses:
  call, operand 11
  custom-call.15.0, operand 1
 from instruction: %p8.50.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<520 p7.48.0 @0>
 positions:
  p7.48.0
  p.12
 uses:
  call, operand 12
  custom-call.16.0, operand 1
 from instruction: %p7.48.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<521 p13.124.0 @0>
 positions:
  p13.124.0
  p.13
 uses:
  call, operand 13
  fusion.99, operand 1
 from instruction: %p13.124.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<522 p12.122.0 @0>
 positions:
  p12.122.0
  p.14
 uses:
  call, operand 14
  custom-call.17.0, operand 1
 from instruction: %p12.122.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<523 p11.120.0 @0>
 positions:
  p11.120.0
  p.15
 uses:
  call, operand 15
  custom-call.18.0, operand 1
 from instruction: %p11.120.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<524 p17.196.0 @0>
 positions:
  p17.196.0
  p.16
 uses:
  call, operand 16
  fusion.100, operand 1
 from instruction: %p17.196.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<525 p16.194.0 @0>
 positions:
  p16.194.0
  p.17
 uses:
  call, operand 17
  custom-call.19.0, operand 1
 from instruction: %p16.194.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<526 p15.192.0 @0>
 positions:
  p15.192.0
  p.18
 uses:
  call, operand 18
  custom-call.20.0, operand 1
 from instruction: %p15.192.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<527 p21.268.0 @0>
 positions:
  p21.268.0
  p.19
 uses:
  call, operand 19
  fusion.101, operand 1
 from instruction: %p21.268.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<528 p20.266.0 @0>
 positions:
  p20.266.0
  p.20
 uses:
  call, operand 20
  custom-call.21.0, operand 1
 from instruction: %p20.266.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<529 p19.264.0 @0>
 positions:
  p19.264.0
  p.21
 uses:
  call, operand 21
  custom-call.22.0, operand 1
 from instruction: %p19.264.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<530 p25.340.0 @0>
 positions:
  p25.340.0
  p.22
 uses:
  call, operand 22
  fusion.102, operand 2
 from instruction: %p25.340.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<531 p24.338.0 @0>
 positions:
  p24.338.0
  p.23
 uses:
  call, operand 23
  custom-call.23.0, operand 1
 from instruction: %p24.338.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<532 p23.336.0 @0>
 positions:
  p23.336.0
  p.24
 uses:
  call, operand 24
  custom-call.24.0, operand 1
 from instruction: %p23.336.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<533 p29.412.0 @0>
 positions:
  p29.412.0
  p.25
 uses:
  call, operand 25
  fusion.103, operand 1
 from instruction: %p29.412.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<534 p28.410.0 @0>
 positions:
  p28.410.0
  p.26
 uses:
  call, operand 26
  custom-call.25.0, operand 1
 from instruction: %p28.410.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<535 p27.408.0 @0>
 positions:
  p27.408.0
  p.27
 uses:
  call, operand 27
  custom-call.26.0, operand 1
 from instruction: %p27.408.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<536 p33.484.0 @0>
 positions:
  p33.484.0
  p.28
 uses:
  call, operand 28
  fusion.104, operand 1
 from instruction: %p33.484.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<537 p32.482.0 @0>
 positions:
  p32.482.0
  p.29
 uses:
  call, operand 29
  custom-call.27.0, operand 1
 from instruction: %p32.482.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<538 p31.480.0 @0>
 positions:
  p31.480.0
  p.30
 uses:
  call, operand 30
  custom-call.28.0, operand 1
 from instruction: %p31.480.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<539 p3.8.0 @0>
 positions:
  p3.8.0
  p.31
 uses:
  call, operand 31
  fusion.105, operand 2
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<540 p2.6.0 @0>
 positions:
  p2.6.0
  p.32
 uses:
  call, operand 32
  custom-call.29.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<541 p0.1.0 @0>
 positions:
  p0.1.0
  p.33
 uses:
  call, operand 33
  fusion.89, operand 2
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<542 p35.609.0 @0>
 positions:
  p35.609.0
  p.34
 uses:
  call, operand 34
  input_concatenate_fusion, operand 1
 from instruction: %p35.609.0 = bf16[40960,128]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<543 p34.608.0 @0>
 positions:
  p34.608.0
  p.35
 uses:
  call, operand 35
  input_concatenate_fusion, operand 2
 from instruction: %p34.608.0 = s32[32]{0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<544 p36.653.0 @0>
 positions:
  p36.653.0
  p.36
 uses:
  call, operand 36
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p36.653.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<545 tuple.662.0{} @0>
 positions:
  tuple.662.0 {}
 uses:
 from instruction: %tuple.662.0 = (bf16[32,8,128]{2,1,0}, bf16[32,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[32,8,128]{2,1,0} %get-tuple-element.17, bf16[32,8,128]{2,1,0} %get-tuple-element.18, bf16[4233,16,8,128]{3,2,1,0} %bitcast.1701.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.19)


HloLiveRange (max 138):
  InstructionSequence:
    0:p1.4.0
    1:p36.653.0
    2:p35.609.0
    3:p34.608.0
    4:p33.484.0
    5:p32.482.0
    6:p31.480.0
    7:p30.464.0
    8:p29.412.0
    9:p28.410.0
    10:p27.408.0
    11:p26.392.0
    12:p25.340.0
    13:p24.338.0
    14:p23.336.0
    15:p22.320.0
    16:p21.268.0
    17:p20.266.0
    18:p19.264.0
    19:p18.248.0
    20:p17.196.0
    21:p16.194.0
    22:p15.192.0
    23:p14.176.0
    24:p13.124.0
    25:p12.122.0
    26:p11.120.0
    27:p10.104.0
    28:p9.52.0
    29:p8.50.0
    30:p7.48.0
    31:p6.32.0
    32:p5.28.0
    33:p4.26.0
    34:p3.8.0
    35:p2.6.0
    36:p0.1.0
    37:p
    38:p.1
    39:p.2
    40:p.3
    41:p.4
    42:p.5
    43:p.6
    44:p.7
    45:p.8
    46:p.9
    47:p.10
    48:p.11
    49:p.12
    50:p.13
    51:p.14
    52:p.15
    53:p.16
    54:p.17
    55:p.18
    56:p.19
    57:p.20
    58:p.21
    59:p.22
    60:p.23
    61:p.24
    62:p.25
    63:p.26
    64:p.27
    65:p.28
    66:p.29
    67:p.30
    68:p.31
    69:p.32
    70:p.33
    71:p.34
    72:p.35
    73:p.36
    74:loop_gather_fusion
    75:wrapped_concatenate
    76:gemm_fusion_dot.27.0
    77:fusion.98
    78:custom-call.15.0
    79:get-tuple-element.15
    80:loop_convert_fusion
    81:custom-call.16.0
    82:get-tuple-element.1.0
    83:fusion.99
    84:custom-call.17.0
    85:get-tuple-element.2.0
    86:loop_convert_fusion.1
    87:custom-call.18.0
    88:get-tuple-element.3.0
    89:fusion.100
    90:custom-call.19.0
    91:get-tuple-element.4.0
    92:loop_convert_fusion.2
    93:custom-call.20.0
    94:get-tuple-element.5.0
    95:fusion.101
    96:custom-call.21.0
    97:get-tuple-element.6.0
    98:loop_convert_fusion.3
    99:custom-call.22.0
    100:get-tuple-element.7.0
    101:loop_add_fusion
    102:fusion.102
    103:custom-call.23.0
    104:get-tuple-element.8.0
    105:loop_convert_fusion.4
    106:custom-call.24.0
    107:get-tuple-element.9.0
    108:fusion.103
    109:custom-call.25.0
    110:get-tuple-element.10.0
    111:loop_convert_fusion.5
    112:custom-call.26.0
    113:get-tuple-element.11.0
    114:fusion.104
    115:custom-call.27.0
    116:get-tuple-element.12.0
    117:loop_convert_fusion.6
    118:custom-call.28.0
    119:get-tuple-element.13.0
    120:fusion.105
    121:custom-call.29.0
    122:get-tuple-element.14.0
    123:wrapped_slice
    124:fusion.89
    125:input_concatenate_fusion
    126:bitcast.1696.0
    127:loop_slice_fusion
    128:bitcast.1708.0
    129:wrapped_slice.1
    130:tuple
    131:call
    132:get-tuple-element.17
    133:get-tuple-element.18
    134:get-tuple-element.19
    135:get-tuple-element.20
    136:bitcast.1701.0
    137:tuple.662.0
  BufferLiveRange:
    wrapped_concatenate{}:75-76
    gemm_fusion_dot.27.0{}:76-120
    loop_gather_fusion{}:74-101
    fusion.98{}:77-78
    custom-call.15.0{}:78-79
    custom-call.15.0{0}:78-80
    custom-call.15.0{1}:78-78
    loop_convert_fusion{}:80-81
    custom-call.16.0{}:81-82
    custom-call.16.0{0}:81-101
    custom-call.16.0{1}:81-81
    fusion.99{}:83-84
    custom-call.17.0{}:84-85
    custom-call.17.0{0}:84-86
    custom-call.17.0{1}:84-84
    loop_convert_fusion.1{}:86-87
    custom-call.18.0{}:87-88
    custom-call.18.0{0}:87-101
    custom-call.18.0{1}:87-87
    fusion.100{}:89-90
    custom-call.19.0{}:90-91
    custom-call.19.0{0}:90-92
    custom-call.19.0{1}:90-90
    loop_convert_fusion.2{}:92-93
    custom-call.20.0{}:93-94
    custom-call.20.0{0}:93-101
    custom-call.20.0{1}:93-93
    fusion.101{}:95-96
    custom-call.21.0{}:96-97
    custom-call.21.0{0}:96-98
    custom-call.21.0{1}:96-96
    loop_convert_fusion.3{}:98-99
    custom-call.22.0{}:99-100
    custom-call.22.0{0}:99-101
    custom-call.22.0{1}:99-99
    loop_add_fusion{}:101-120
    fusion.102{}:102-103
    custom-call.23.0{}:103-104
    custom-call.23.0{0}:103-105
    custom-call.23.0{1}:103-103
    loop_convert_fusion.4{}:105-106
    custom-call.24.0{}:106-107
    custom-call.24.0{0}:106-120
    custom-call.24.0{1}:106-106
    fusion.103{}:108-109
    custom-call.25.0{}:109-110
    custom-call.25.0{0}:109-111
    custom-call.25.0{1}:109-109
    loop_convert_fusion.5{}:111-112
    custom-call.26.0{}:112-113
    custom-call.26.0{0}:112-120
    custom-call.26.0{1}:112-112
    fusion.104{}:114-115
    custom-call.27.0{}:115-116
    custom-call.27.0{0}:115-117
    custom-call.27.0{1}:115-115
    loop_convert_fusion.6{}:117-118
    custom-call.28.0{}:118-119
    custom-call.28.0{0}:118-120
    custom-call.28.0{1}:118-118
    fusion.105{}:120-121
    custom-call.29.0{}:121-122
    custom-call.29.0{0}:121-124
    custom-call.29.0{1}:121-121
    fusion.89{}:124-125
    input_concatenate_fusion{}:125-138
    wrapped_slice{}:123-138
    loop_slice_fusion{}:127-138
    wrapped_slice.1{}:129-138
    tuple{}:130-135
    p5.28.0{}:0-138
    p4.26.0{}:0-138
    p30.464.0{}:0-138
    p26.392.0{}:0-138
    p22.320.0{}:0-138
    p18.248.0{}:0-138
    p14.176.0{}:0-138
    p10.104.0{}:0-138
    p6.32.0{}:0-138
    p1.4.0{}:0-138
    p9.52.0{}:0-138
    p8.50.0{}:0-138
    p7.48.0{}:0-138
    p13.124.0{}:0-138
    p12.122.0{}:0-138
    p11.120.0{}:0-138
    p17.196.0{}:0-138
    p16.194.0{}:0-138
    p15.192.0{}:0-138
    p21.268.0{}:0-138
    p20.266.0{}:0-138
    p19.264.0{}:0-138
    p25.340.0{}:0-138
    p24.338.0{}:0-138
    p23.336.0{}:0-138
    p29.412.0{}:0-138
    p28.410.0{}:0-138
    p27.408.0{}:0-138
    p33.484.0{}:0-138
    p32.482.0{}:0-138
    p31.480.0{}:0-138
    p3.8.0{}:0-138
    p2.6.0{}:0-138
    p0.1.0{}:0-138
    p35.609.0{}:0-138
    p34.608.0{}:0-138
    p36.653.0{}:0-138
    tuple.662.0{}:137-138
  Live ranges at 130 (peak):
    input_concatenate_fusion: 65536 bytes
    wrapped_slice: 65536 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.28.0: 311164928 bytes
    p4.26.0: 128 bytes
    p30.464.0: 4194304 bytes
    p26.392.0: 4194304 bytes
    p22.320.0: 4194304 bytes
    p18.248.0: 4194304 bytes
    p14.176.0: 4194304 bytes
    p10.104.0: 4194304 bytes
    p6.32.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.52.0: 2048 bytes
    p8.50.0: 12582912 bytes
    p7.48.0: 6291456 bytes
    p13.124.0: 2048 bytes
    p12.122.0: 12582912 bytes
    p11.120.0: 6291456 bytes
    p17.196.0: 2048 bytes
    p16.194.0: 12582912 bytes
    p15.192.0: 6291456 bytes
    p21.268.0: 2048 bytes
    p20.266.0: 12582912 bytes
    p19.264.0: 6291456 bytes
    p25.340.0: 2048 bytes
    p24.338.0: 12582912 bytes
    p23.336.0: 6291456 bytes
    p29.412.0: 2048 bytes
    p28.410.0: 12582912 bytes
    p27.408.0: 6291456 bytes
    p33.484.0: 2048 bytes
    p32.482.0: 12582912 bytes
    p31.480.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p35.609.0: 10485760 bytes
    p34.608.0: 128 bytes
    p36.653.0: 277413888 bytes
