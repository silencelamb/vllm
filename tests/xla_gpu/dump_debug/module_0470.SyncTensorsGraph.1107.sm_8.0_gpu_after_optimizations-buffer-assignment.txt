BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <847 p5.40.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 60, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <907 p60.1097.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <728 gemm_fusion_dot.51.0 @0> (size=425984,offset=0): bf16[16,13312]{1,0}
 value: <839 custom-call.53.0{0} @0> (size=131072,offset=0): bf16[16,4096]{1,0}
 value: <844 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <727 wrapped_concatenate @0> (size=54525952,offset=0): bf16[13312,2048]{1,0}
 value: <733 custom-call.27.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <737 custom-call.28.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <741 custom-call.29.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <745 custom-call.30.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <750 custom-call.31.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <754 custom-call.32.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <758 custom-call.33.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <762 custom-call.34.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <766 custom-call.35.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <770 custom-call.36.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <774 custom-call.37.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <778 custom-call.38.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <783 custom-call.39.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <787 custom-call.40.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <791 custom-call.41.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <795 custom-call.42.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <799 custom-call.43.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <803 custom-call.44.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <807 custom-call.45.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <811 custom-call.46.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <816 custom-call.47.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <820 custom-call.48.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <824 custom-call.49.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <828 custom-call.50.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <832 custom-call.51.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <836 custom-call.52.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <840 custom-call.53.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <841 triton_softmax.15.0 @0> (size=65536,offset=0): f32[16,8,128]{2,1,0}
 value: <845 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <864 p8.62.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <867 p12.134.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <870 p16.206.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 12582912, parameter 20, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <873 p20.278.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 8: size 12582912, parameter 24, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <876 p24.350.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 9: size 12582912, parameter 28, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <879 p28.422.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 10: size 12582912, parameter 32, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <882 p32.494.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 11: size 12582912, parameter 36, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <885 p36.566.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 12: size 12582912, parameter 40, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <888 p40.638.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 13: size 12582912, parameter 44, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <891 p44.710.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 14: size 12582912, parameter 48, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <894 p48.782.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 15: size 12582912, parameter 52, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <897 p52.854.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 16: size 12582912, parameter 56, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <900 p56.926.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 17: size 10485760, parameter 59, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <905 p59.1053.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 18: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <903 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 19: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <865 p7.60.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 20: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <868 p11.132.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 21: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <871 p15.204.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 22: size 6291456, parameter 19, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <874 p19.276.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 23: size 6291456, parameter 23, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <877 p23.348.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 24: size 6291456, parameter 27, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <880 p27.420.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 25: size 6291456, parameter 31, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <883 p31.492.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 26: size 6291456, parameter 35, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <886 p35.564.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 27: size 6291456, parameter 39, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <889 p39.636.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 28: size 6291456, parameter 43, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <892 p43.708.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 29: size 6291456, parameter 47, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <895 p47.780.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 30: size 6291456, parameter 51, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <898 p51.852.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 31: size 6291456, parameter 55, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <901 p55.924.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 32: size 4194304, parameter 54, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <849 p54.908.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 33: size 4194304, parameter 50, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <850 p50.836.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 34: size 4194304, parameter 46, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <851 p46.764.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 35: size 4194304, parameter 42, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <852 p42.692.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 36: size 4194304, parameter 38, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <853 p38.620.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 37: size 4194304, parameter 34, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <854 p34.548.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 38: size 4194304, parameter 30, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <855 p30.476.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 39: size 4194304, parameter 26, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <856 p26.404.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 40: size 4194304, parameter 22, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <857 p22.332.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 41: size 4194304, parameter 18, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <858 p18.260.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 42: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <859 p14.188.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 43: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <860 p10.116.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 44: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <861 p6.44.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 45: size 32768, maybe-live-out:
 value: <730 fusion.181 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <736 custom-call.28.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <755 fusion.175 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <761 custom-call.34.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <788 fusion.167 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <794 custom-call.42.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <821 fusion.159 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <827 custom-call.50.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <842 input_concatenate_fusion @0> (size=32768,offset=0): bf16[16,8,128]{2,1,0}
allocation 46: size 32768, maybe-live-out:
 value: <729 loop_gather_fusion @0> (size=32768,offset=0): bf16[16,1,1024]{2,0,1}
 value: <747 fusion.177 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <753 custom-call.32.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <780 fusion.169 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <786 custom-call.40.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <813 fusion.161 @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <819 custom-call.48.0{0} @0> (size=32768,offset=0): bf16[16,1024]{1,0}
 value: <843 wrapped_slice @0> (size=32768,offset=0): bf16[16,1024]{1,0}
allocation 47: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <863 p9.64.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 48: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <866 p13.136.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 49: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <869 p17.208.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 50: size 2048, parameter 21, shape |bf16[1024]| at ShapeIndex {}:
 value: <872 p21.280.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 51: size 2048, parameter 25, shape |bf16[1024]| at ShapeIndex {}:
 value: <875 p25.352.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 52: size 2048, parameter 29, shape |bf16[1024]| at ShapeIndex {}:
 value: <878 p29.424.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 53: size 2048, parameter 33, shape |bf16[1024]| at ShapeIndex {}:
 value: <881 p33.496.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 54: size 2048, parameter 37, shape |bf16[1024]| at ShapeIndex {}:
 value: <884 p37.568.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 55: size 2048, parameter 41, shape |bf16[1024]| at ShapeIndex {}:
 value: <887 p41.640.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 56: size 2048, parameter 45, shape |bf16[1024]| at ShapeIndex {}:
 value: <890 p45.712.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 57: size 2048, parameter 49, shape |bf16[1024]| at ShapeIndex {}:
 value: <893 p49.784.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 58: size 2048, parameter 53, shape |bf16[1024]| at ShapeIndex {}:
 value: <896 p53.856.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 59: size 2048, parameter 57, shape |bf16[1024]| at ShapeIndex {}:
 value: <899 p57.928.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 60: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <902 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 61: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <904 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 62: size 64, parameter 4, shape |s32[16]| at ShapeIndex {}:
 value: <848 p4.38.0 @0> (size=64,offset=0): s32[16]{0}
allocation 63: size 64, parameter 58, shape |s32[16]| at ShapeIndex {}:
 value: <906 p58.1052.0 @0> (size=64,offset=0): s32[16]{0}
allocation 64: size 32, output shape is |(bf16[16,8,128], bf16[16,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <908 tuple.1106.0{} @0> (size=32,offset=0): (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 65: size 4, thread-local:
 value: <25 add.177 @0> (size=4,offset=0): f32[]
allocation 66: size 4, thread-local:
 value: <24 y.74 @0> (size=4,offset=0): f32[]
allocation 67: size 4, thread-local:
 value: <23 x.73 @0> (size=4,offset=0): f32[]
allocation 68: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <862 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 69: size 396704, preallocated-temp:
 value: <731 custom-call.27.0{} @0> (size=16,offset=396544): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <732 custom-call.27.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <734 loop_convert_fusion.12 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <735 custom-call.28.0{} @0> (size=16,offset=396416): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <738 fusion.179 @0> (size=32768,offset=199808): bf16[16,1024]{1,0}
 value: <739 custom-call.29.0{} @0> (size=16,offset=3072): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <740 custom-call.29.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <742 loop_convert_fusion.11 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <743 custom-call.30.0{} @0> (size=16,offset=0): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <744 custom-call.30.0{0} @0> (size=32768,offset=3200): bf16[16,1024]{1,0}
 value: <746 loop_add_fusion @0> (size=65536,offset=298112): f32[16,1024]{1,0}
 value: <748 custom-call.31.0{} @0> (size=16,offset=128): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <749 custom-call.31.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <751 loop_convert_fusion.10 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <752 custom-call.32.0{} @0> (size=16,offset=256): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <756 custom-call.33.0{} @0> (size=16,offset=384): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <757 custom-call.33.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <759 loop_convert_fusion.9 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <760 custom-call.34.0{} @0> (size=16,offset=512): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <763 fusion.173 @0> (size=32768,offset=199808): bf16[16,1024]{1,0}
 value: <764 custom-call.35.0{} @0> (size=16,offset=640): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <765 custom-call.35.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <767 loop_convert_fusion.8 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <768 custom-call.36.0{} @0> (size=16,offset=768): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <769 custom-call.36.0{0} @0> (size=32768,offset=363648): bf16[16,1024]{1,0}
 value: <771 fusion.171 @0> (size=32768,offset=199808): bf16[16,1024]{1,0}
 value: <772 custom-call.37.0{} @0> (size=16,offset=896): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <773 custom-call.37.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <775 loop_convert_fusion.7 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <776 custom-call.38.0{} @0> (size=16,offset=1024): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <777 custom-call.38.0{0} @0> (size=32768,offset=3200): bf16[16,1024]{1,0}
 value: <779 loop_add_fusion.1 @0> (size=65536,offset=298112): f32[16,1024]{1,0}
 value: <781 custom-call.39.0{} @0> (size=16,offset=1152): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <782 custom-call.39.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <784 loop_convert_fusion.6 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <785 custom-call.40.0{} @0> (size=16,offset=1280): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <789 custom-call.41.0{} @0> (size=16,offset=1408): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <790 custom-call.41.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <792 loop_convert_fusion.5 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <793 custom-call.42.0{} @0> (size=16,offset=1536): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <796 fusion.165 @0> (size=32768,offset=199808): bf16[16,1024]{1,0}
 value: <797 custom-call.43.0{} @0> (size=16,offset=1664): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <798 custom-call.43.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <800 loop_convert_fusion.4 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <801 custom-call.44.0{} @0> (size=16,offset=1792): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <802 custom-call.44.0{0} @0> (size=32768,offset=363648): bf16[16,1024]{1,0}
 value: <804 fusion.163 @0> (size=32768,offset=199808): bf16[16,1024]{1,0}
 value: <805 custom-call.45.0{} @0> (size=16,offset=1920): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <806 custom-call.45.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <808 loop_convert_fusion.3 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <809 custom-call.46.0{} @0> (size=16,offset=2048): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <810 custom-call.46.0{0} @0> (size=32768,offset=3200): bf16[16,1024]{1,0}
 value: <812 loop_add_fusion.2 @0> (size=65536,offset=298112): f32[16,1024]{1,0}
 value: <814 custom-call.47.0{} @0> (size=16,offset=2176): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <815 custom-call.47.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <817 loop_convert_fusion.2 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <818 custom-call.48.0{} @0> (size=16,offset=2304): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <822 custom-call.49.0{} @0> (size=16,offset=2432): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <823 custom-call.49.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <825 loop_convert_fusion.1 @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <826 custom-call.50.0{} @0> (size=16,offset=2560): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <829 fusion.157 @0> (size=32768,offset=199808): bf16[16,1024]{1,0}
 value: <830 custom-call.51.0{} @0> (size=16,offset=2688): (bf16[16,6144]{1,0}, s8[4194304]{0})
 value: <831 custom-call.51.0{0} @0> (size=196608,offset=3200): bf16[16,6144]{1,0}
 value: <833 loop_convert_fusion @0> (size=98304,offset=199808): bf16[16,3072]{1,0}
 value: <834 custom-call.52.0{} @0> (size=16,offset=2816): (bf16[16,1024]{1,0}, s8[4194304]{0})
 value: <835 custom-call.52.0{0} @0> (size=32768,offset=3200): bf16[16,1024]{1,0}
 value: <837 fusion.155 @0> (size=32768,offset=35968): bf16[16,1024]{1,0}
 value: <838 custom-call.53.0{} @0> (size=16,offset=2944): (bf16[16,4096]{1,0}, s8[4194304]{0})
 value: <846 tuple{} @0> (size=32,offset=396672): (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 1185251152 (1.10GiB)

Used values:
<23 x.73 @0>
 positions:
  x.73
 uses:
  add.177, operand 0
 from instruction: %x.73 = f32[] parameter(0)
<24 y.74 @0>
 positions:
  y.74
 uses:
  add.177, operand 1
 from instruction: %y.74 = f32[] parameter(1)
<25 add.177 @0>
 positions:
  add.177
 uses:
 from instruction: %add.177 = f32[] add(f32[] %x.73, f32[] %y.74)
<727 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.51.0, operand 0
 from instruction: %wrapped_concatenate = bf16[13312,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9, bf16[1024,2048]{1,0} %p.10, bf16[1024,2048]{1,0} %p.11, /*index=10*/bf16[1024,2048]{1,0} %p.12, bf16[1024,2048]{1,0} %p.13, bf16[1024,2048]{1,0} %p.14), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<728 gemm_fusion_dot.51.0 @0>
 positions:
  gemm_fusion_dot.51.0
 uses:
  fusion.181, operand 2
  fusion.179, operand 3
  loop_add_fusion, operand 0
  fusion.175, operand 4
  fusion.173, operand 4
  fusion.171, operand 3
  loop_add_fusion.1, operand 0
  fusion.167, operand 4
  fusion.165, operand 4
  fusion.163, operand 3
  loop_add_fusion.2, operand 0
  fusion.159, operand 4
  fusion.157, operand 4
  fusion.155, operand 5
 from instruction: %gemm_fusion_dot.51.0 = bf16[16,13312]{1,0} fusion(bf16[13312,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.51_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"128","block_n":"16","block_k":"128","split_k":"1","num_stages":"4","num_warps":"4","num_ctas":"1"}},"force_earliest_schedule":false}
<729 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.181, operand 1
  fusion.179, operand 2
  loop_add_fusion, operand 2
 from instruction: %loop_gather_fusion = bf16[16,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[16]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<730 fusion.181 @0>
 positions:
  fusion.181
 uses:
  custom-call.27.0, operand 0
 from instruction: %fusion.181 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, bf16[1024]{0} %p.16), kind=kCustom, calls=%fused_computation.150, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","256"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<731 custom-call.27.0{} @0>
 positions:
  custom-call.27.0 {}
 uses:
  get-tuple-element.27, operand 0 {}
 from instruction: %custom-call.27.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.181, bf16[6144,1024]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<732 custom-call.27.0{0} @0>
 positions:
  custom-call.27.0 {0}
  get-tuple-element.27
 uses:
  loop_convert_fusion.12, operand 0
 from instruction: %custom-call.27.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.181, bf16[6144,1024]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<733 custom-call.27.0{1} @0>
 positions:
  custom-call.27.0 {1}
 uses:
 from instruction: %custom-call.27.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.181, bf16[6144,1024]{1,0} %p.17), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<734 loop_convert_fusion.12 @0>
 positions:
  loop_convert_fusion.12
 uses:
  custom-call.28.0, operand 0
 from instruction: %loop_convert_fusion.12 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.27), kind=kLoop, calls=%fused_convert.12, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<735 custom-call.28.0{} @0>
 positions:
  custom-call.28.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.28.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<736 custom-call.28.0{0} @0>
 positions:
  custom-call.28.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.179, operand 4
  loop_add_fusion, operand 3
 from instruction: %custom-call.28.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<737 custom-call.28.0{1} @0>
 positions:
  custom-call.28.0 {1}
 uses:
 from instruction: %custom-call.28.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.18), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<738 fusion.179 @0>
 positions:
  fusion.179
 uses:
  custom-call.29.0, operand 0
 from instruction: %fusion.179 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.19, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, bf16[16,1024]{1,0} %get-tuple-element.1.0), kind=kCustom, calls=%fused_computation.148, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<739 custom-call.29.0{} @0>
 positions:
  custom-call.29.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.29.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.179, bf16[6144,1024]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<740 custom-call.29.0{0} @0>
 positions:
  custom-call.29.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.11, operand 0
 from instruction: %custom-call.29.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.179, bf16[6144,1024]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<741 custom-call.29.0{1} @0>
 positions:
  custom-call.29.0 {1}
 uses:
 from instruction: %custom-call.29.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.179, bf16[6144,1024]{1,0} %p.20), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<742 loop_convert_fusion.11 @0>
 positions:
  loop_convert_fusion.11
 uses:
  custom-call.30.0, operand 0
 from instruction: %loop_convert_fusion.11 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.11, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<743 custom-call.30.0{} @0>
 positions:
  custom-call.30.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.30.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<744 custom-call.30.0{0} @0>
 positions:
  custom-call.30.0 {0}
  get-tuple-element.3.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.30.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<745 custom-call.30.0{1} @0>
 positions:
  custom-call.30.0 {1}
 uses:
 from instruction: %custom-call.30.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.21), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<746 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.177, operand 0
  fusion.175, operand 2
  fusion.173, operand 2
  fusion.171, operand 4
  loop_add_fusion.1, operand 3
 from instruction: %loop_add_fusion = f32[16,1024]{1,0} fusion(bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, bf16[16,1024]{1,0} %get-tuple-element.3.0, bf16[16,1,1024]{2,0,1} %loop_gather_fusion, bf16[16,1024]{1,0} %get-tuple-element.1.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.49/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<747 fusion.177 @0>
 positions:
  fusion.177
 uses:
  custom-call.31.0, operand 0
 from instruction: %fusion.177 = bf16[16,1024]{1,0} fusion(f32[16,1024]{1,0} %loop_add_fusion, f32[] %p.15, bf16[1024]{0} %p.22), kind=kCustom, calls=%fused_computation.146, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.161"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<748 custom-call.31.0{} @0>
 positions:
  custom-call.31.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.31.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.177, bf16[6144,1024]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<749 custom-call.31.0{0} @0>
 positions:
  custom-call.31.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.10, operand 0
 from instruction: %custom-call.31.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.177, bf16[6144,1024]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<750 custom-call.31.0{1} @0>
 positions:
  custom-call.31.0 {1}
 uses:
 from instruction: %custom-call.31.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.177, bf16[6144,1024]{1,0} %p.23), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<751 loop_convert_fusion.10 @0>
 positions:
  loop_convert_fusion.10
 uses:
  custom-call.32.0, operand 0
 from instruction: %loop_convert_fusion.10 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.10, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<752 custom-call.32.0{} @0>
 positions:
  custom-call.32.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.32.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<753 custom-call.32.0{0} @0>
 positions:
  custom-call.32.0 {0}
  get-tuple-element.5.0
 uses:
  fusion.175, operand 3
  fusion.173, operand 3
  fusion.171, operand 5
  loop_add_fusion.1, operand 4
 from instruction: %custom-call.32.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<754 custom-call.32.0{1} @0>
 positions:
  custom-call.32.0 {1}
 uses:
 from instruction: %custom-call.32.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.24), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<755 fusion.175 @0>
 positions:
  fusion.175
 uses:
  custom-call.33.0, operand 0
 from instruction: %fusion.175 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.25, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.5.0, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0), kind=kCustom, calls=%fused_computation.144, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<756 custom-call.33.0{} @0>
 positions:
  custom-call.33.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.33.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.175, bf16[6144,1024]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<757 custom-call.33.0{0} @0>
 positions:
  custom-call.33.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.9, operand 0
 from instruction: %custom-call.33.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.175, bf16[6144,1024]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<758 custom-call.33.0{1} @0>
 positions:
  custom-call.33.0 {1}
 uses:
 from instruction: %custom-call.33.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.175, bf16[6144,1024]{1,0} %p.26), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<759 loop_convert_fusion.9 @0>
 positions:
  loop_convert_fusion.9
 uses:
  custom-call.34.0, operand 0
 from instruction: %loop_convert_fusion.9 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.9, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<760 custom-call.34.0{} @0>
 positions:
  custom-call.34.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.34.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<761 custom-call.34.0{0} @0>
 positions:
  custom-call.34.0 {0}
  get-tuple-element.7.0
 uses:
  fusion.173, operand 5
  fusion.171, operand 6
  loop_add_fusion.1, operand 5
 from instruction: %custom-call.34.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<762 custom-call.34.0{1} @0>
 positions:
  custom-call.34.0 {1}
 uses:
 from instruction: %custom-call.34.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<763 fusion.173 @0>
 positions:
  fusion.173
 uses:
  custom-call.35.0, operand 0
 from instruction: %fusion.173 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.28, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.5.0, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.7.0), kind=kCustom, calls=%fused_computation.142, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<764 custom-call.35.0{} @0>
 positions:
  custom-call.35.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.35.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.173, bf16[6144,1024]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<765 custom-call.35.0{0} @0>
 positions:
  custom-call.35.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.8, operand 0
 from instruction: %custom-call.35.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.173, bf16[6144,1024]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<766 custom-call.35.0{1} @0>
 positions:
  custom-call.35.0 {1}
 uses:
 from instruction: %custom-call.35.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.173, bf16[6144,1024]{1,0} %p.29), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<767 loop_convert_fusion.8 @0>
 positions:
  loop_convert_fusion.8
 uses:
  custom-call.36.0, operand 0
 from instruction: %loop_convert_fusion.8 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.8, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<768 custom-call.36.0{} @0>
 positions:
  custom-call.36.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.36.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<769 custom-call.36.0{0} @0>
 positions:
  custom-call.36.0 {0}
  get-tuple-element.9.0
 uses:
  fusion.171, operand 2
  loop_add_fusion.1, operand 2
 from instruction: %custom-call.36.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<770 custom-call.36.0{1} @0>
 positions:
  custom-call.36.0 {1}
 uses:
 from instruction: %custom-call.36.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<771 fusion.171 @0>
 positions:
  fusion.171
 uses:
  custom-call.37.0, operand 0
 from instruction: %fusion.171 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.31, bf16[16,1024]{1,0} %get-tuple-element.9.0, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, f32[16,1024]{1,0} %loop_add_fusion, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.5.0, bf16[16,1024]{1,0} %get-tuple-element.7.0), kind=kCustom, calls=%fused_computation.140, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<772 custom-call.37.0{} @0>
 positions:
  custom-call.37.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.37.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.171, bf16[6144,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<773 custom-call.37.0{0} @0>
 positions:
  custom-call.37.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.7, operand 0
 from instruction: %custom-call.37.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.171, bf16[6144,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<774 custom-call.37.0{1} @0>
 positions:
  custom-call.37.0 {1}
 uses:
 from instruction: %custom-call.37.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.171, bf16[6144,1024]{1,0} %p.32), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<775 loop_convert_fusion.7 @0>
 positions:
  loop_convert_fusion.7
 uses:
  custom-call.38.0, operand 0
 from instruction: %loop_convert_fusion.7 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<776 custom-call.38.0{} @0>
 positions:
  custom-call.38.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.38.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<777 custom-call.38.0{0} @0>
 positions:
  custom-call.38.0 {0}
  get-tuple-element.11.0
 uses:
  loop_add_fusion.1, operand 1
 from instruction: %custom-call.38.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<778 custom-call.38.0{1} @0>
 positions:
  custom-call.38.0 {1}
 uses:
 from instruction: %custom-call.38.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<779 loop_add_fusion.1 @0>
 positions:
  loop_add_fusion.1
 uses:
  fusion.169, operand 0
  fusion.167, operand 2
  fusion.165, operand 2
  fusion.163, operand 4
  loop_add_fusion.2, operand 3
 from instruction: %loop_add_fusion.1 = f32[16,1024]{1,0} fusion(bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, bf16[16,1024]{1,0} %get-tuple-element.11.0, bf16[16,1024]{1,0} %get-tuple-element.9.0, f32[16,1024]{1,0} %loop_add_fusion, bf16[16,1024]{1,0} %get-tuple-element.5.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.7.0), kind=kLoop, calls=%fused_add.1, metadata={op_type="aten__add" op_name="aten__add.121/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<780 fusion.169 @0>
 positions:
  fusion.169
 uses:
  custom-call.39.0, operand 0
 from instruction: %fusion.169 = bf16[16,1024]{1,0} fusion(f32[16,1024]{1,0} %loop_add_fusion.1, f32[] %p.15, bf16[1024]{0} %p.34), kind=kCustom, calls=%fused_computation.138, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.161"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<781 custom-call.39.0{} @0>
 positions:
  custom-call.39.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.39.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.169, bf16[6144,1024]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<782 custom-call.39.0{0} @0>
 positions:
  custom-call.39.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.39.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.169, bf16[6144,1024]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<783 custom-call.39.0{1} @0>
 positions:
  custom-call.39.0 {1}
 uses:
 from instruction: %custom-call.39.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.169, bf16[6144,1024]{1,0} %p.35), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<784 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.40.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<785 custom-call.40.0{} @0>
 positions:
  custom-call.40.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.40.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<786 custom-call.40.0{0} @0>
 positions:
  custom-call.40.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.167, operand 3
  fusion.165, operand 3
  fusion.163, operand 5
  loop_add_fusion.2, operand 4
 from instruction: %custom-call.40.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<787 custom-call.40.0{1} @0>
 positions:
  custom-call.40.0 {1}
 uses:
 from instruction: %custom-call.40.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<788 fusion.167 @0>
 positions:
  fusion.167
 uses:
  custom-call.41.0, operand 0
 from instruction: %fusion.167 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.37, f32[16,1024]{1,0} %loop_add_fusion.1, bf16[16,1024]{1,0} %get-tuple-element.13.0, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0), kind=kCustom, calls=%fused_computation.136, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<789 custom-call.41.0{} @0>
 positions:
  custom-call.41.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.41.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.167, bf16[6144,1024]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<790 custom-call.41.0{0} @0>
 positions:
  custom-call.41.0 {0}
  get-tuple-element.14.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.41.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.167, bf16[6144,1024]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<791 custom-call.41.0{1} @0>
 positions:
  custom-call.41.0 {1}
 uses:
 from instruction: %custom-call.41.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.167, bf16[6144,1024]{1,0} %p.38), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<792 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.42.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<793 custom-call.42.0{} @0>
 positions:
  custom-call.42.0 {}
 uses:
  get-tuple-element.15.0, operand 0 {}
 from instruction: %custom-call.42.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<794 custom-call.42.0{0} @0>
 positions:
  custom-call.42.0 {0}
  get-tuple-element.15.0
 uses:
  fusion.165, operand 5
  fusion.163, operand 6
  loop_add_fusion.2, operand 5
 from instruction: %custom-call.42.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<795 custom-call.42.0{1} @0>
 positions:
  custom-call.42.0 {1}
 uses:
 from instruction: %custom-call.42.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<796 fusion.165 @0>
 positions:
  fusion.165
 uses:
  custom-call.43.0, operand 0
 from instruction: %fusion.165 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.40, f32[16,1024]{1,0} %loop_add_fusion.1, bf16[16,1024]{1,0} %get-tuple-element.13.0, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.15.0), kind=kCustom, calls=%fused_computation.134, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<797 custom-call.43.0{} @0>
 positions:
  custom-call.43.0 {}
 uses:
  get-tuple-element.16.0, operand 0 {}
 from instruction: %custom-call.43.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.165, bf16[6144,1024]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<798 custom-call.43.0{0} @0>
 positions:
  custom-call.43.0 {0}
  get-tuple-element.16.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.43.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.165, bf16[6144,1024]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<799 custom-call.43.0{1} @0>
 positions:
  custom-call.43.0 {1}
 uses:
 from instruction: %custom-call.43.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.165, bf16[6144,1024]{1,0} %p.41), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<800 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.44.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<801 custom-call.44.0{} @0>
 positions:
  custom-call.44.0 {}
 uses:
  get-tuple-element.17.0, operand 0 {}
 from instruction: %custom-call.44.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<802 custom-call.44.0{0} @0>
 positions:
  custom-call.44.0 {0}
  get-tuple-element.17.0
 uses:
  fusion.163, operand 2
  loop_add_fusion.2, operand 2
 from instruction: %custom-call.44.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<803 custom-call.44.0{1} @0>
 positions:
  custom-call.44.0 {1}
 uses:
 from instruction: %custom-call.44.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<804 fusion.163 @0>
 positions:
  fusion.163
 uses:
  custom-call.45.0, operand 0
 from instruction: %fusion.163 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.43, bf16[16,1024]{1,0} %get-tuple-element.17.0, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, f32[16,1024]{1,0} %loop_add_fusion.1, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.13.0, bf16[16,1024]{1,0} %get-tuple-element.15.0), kind=kCustom, calls=%fused_computation.132, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<805 custom-call.45.0{} @0>
 positions:
  custom-call.45.0 {}
 uses:
  get-tuple-element.18.0, operand 0 {}
 from instruction: %custom-call.45.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.163, bf16[6144,1024]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<806 custom-call.45.0{0} @0>
 positions:
  custom-call.45.0 {0}
  get-tuple-element.18.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.45.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.163, bf16[6144,1024]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<807 custom-call.45.0{1} @0>
 positions:
  custom-call.45.0 {1}
 uses:
 from instruction: %custom-call.45.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.163, bf16[6144,1024]{1,0} %p.44), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<808 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.46.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.18.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<809 custom-call.46.0{} @0>
 positions:
  custom-call.46.0 {}
 uses:
  get-tuple-element.19.0, operand 0 {}
 from instruction: %custom-call.46.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<810 custom-call.46.0{0} @0>
 positions:
  custom-call.46.0 {0}
  get-tuple-element.19.0
 uses:
  loop_add_fusion.2, operand 1
 from instruction: %custom-call.46.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<811 custom-call.46.0{1} @0>
 positions:
  custom-call.46.0 {1}
 uses:
 from instruction: %custom-call.46.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<812 loop_add_fusion.2 @0>
 positions:
  loop_add_fusion.2
 uses:
  fusion.161, operand 0
  fusion.159, operand 2
  fusion.157, operand 2
  fusion.155, operand 3
 from instruction: %loop_add_fusion.2 = f32[16,1024]{1,0} fusion(bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, bf16[16,1024]{1,0} %get-tuple-element.19.0, bf16[16,1024]{1,0} %get-tuple-element.17.0, f32[16,1024]{1,0} %loop_add_fusion.1, bf16[16,1024]{1,0} %get-tuple-element.13.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.15.0), kind=kLoop, calls=%fused_add.2, metadata={op_type="aten__add" op_name="aten__add.193/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<813 fusion.161 @0>
 positions:
  fusion.161
 uses:
  custom-call.47.0, operand 0
 from instruction: %fusion.161 = bf16[16,1024]{1,0} fusion(f32[16,1024]{1,0} %loop_add_fusion.2, f32[] %p.15, bf16[1024]{0} %p.46), kind=kCustom, calls=%fused_computation.130, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.161"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<814 custom-call.47.0{} @0>
 positions:
  custom-call.47.0 {}
 uses:
  get-tuple-element.20.0, operand 0 {}
 from instruction: %custom-call.47.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.161, bf16[6144,1024]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<815 custom-call.47.0{0} @0>
 positions:
  custom-call.47.0 {0}
  get-tuple-element.20.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.47.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.161, bf16[6144,1024]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<816 custom-call.47.0{1} @0>
 positions:
  custom-call.47.0 {1}
 uses:
 from instruction: %custom-call.47.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.161, bf16[6144,1024]{1,0} %p.47), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<817 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.48.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.20.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<818 custom-call.48.0{} @0>
 positions:
  custom-call.48.0 {}
 uses:
  get-tuple-element.21.0, operand 0 {}
 from instruction: %custom-call.48.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<819 custom-call.48.0{0} @0>
 positions:
  custom-call.48.0 {0}
  get-tuple-element.21.0
 uses:
  fusion.159, operand 3
  fusion.157, operand 3
  fusion.155, operand 4
 from instruction: %custom-call.48.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<820 custom-call.48.0{1} @0>
 positions:
  custom-call.48.0 {1}
 uses:
 from instruction: %custom-call.48.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<821 fusion.159 @0>
 positions:
  fusion.159
 uses:
  custom-call.49.0, operand 0
 from instruction: %fusion.159 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.49, f32[16,1024]{1,0} %loop_add_fusion.2, bf16[16,1024]{1,0} %get-tuple-element.21.0, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0), kind=kCustom, calls=%fused_computation.128, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<822 custom-call.49.0{} @0>
 positions:
  custom-call.49.0 {}
 uses:
  get-tuple-element.22.0, operand 0 {}
 from instruction: %custom-call.49.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.159, bf16[6144,1024]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<823 custom-call.49.0{0} @0>
 positions:
  custom-call.49.0 {0}
  get-tuple-element.22.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.49.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.159, bf16[6144,1024]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<824 custom-call.49.0{1} @0>
 positions:
  custom-call.49.0 {1}
 uses:
 from instruction: %custom-call.49.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.159, bf16[6144,1024]{1,0} %p.50), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<825 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.50.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.22.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<826 custom-call.50.0{} @0>
 positions:
  custom-call.50.0 {}
 uses:
  get-tuple-element.23.0, operand 0 {}
 from instruction: %custom-call.50.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<827 custom-call.50.0{0} @0>
 positions:
  custom-call.50.0 {0}
  get-tuple-element.23.0
 uses:
  fusion.157, operand 5
  fusion.155, operand 6
 from instruction: %custom-call.50.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<828 custom-call.50.0{1} @0>
 positions:
  custom-call.50.0 {1}
 uses:
 from instruction: %custom-call.50.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<829 fusion.157 @0>
 positions:
  fusion.157
 uses:
  custom-call.51.0, operand 0
 from instruction: %fusion.157 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[1024]{0} %p.52, f32[16,1024]{1,0} %loop_add_fusion.2, bf16[16,1024]{1,0} %get-tuple-element.21.0, bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, /*index=5*/bf16[16,1024]{1,0} %get-tuple-element.23.0), kind=kCustom, calls=%fused_computation.126, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<830 custom-call.51.0{} @0>
 positions:
  custom-call.51.0 {}
 uses:
  get-tuple-element.24.0, operand 0 {}
 from instruction: %custom-call.51.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.157, bf16[6144,1024]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<831 custom-call.51.0{0} @0>
 positions:
  custom-call.51.0 {0}
  get-tuple-element.24.0
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.51.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.157, bf16[6144,1024]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<832 custom-call.51.0{1} @0>
 positions:
  custom-call.51.0 {1}
 uses:
 from instruction: %custom-call.51.0 = (bf16[16,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.157, bf16[6144,1024]{1,0} %p.53), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<833 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.52.0, operand 0
 from instruction: %loop_convert_fusion = bf16[16,3072]{1,0} fusion(bf16[16,6144]{1,0} %get-tuple-element.24.0), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<834 custom-call.52.0{} @0>
 positions:
  custom-call.52.0 {}
 uses:
  get-tuple-element.25.0, operand 0 {}
 from instruction: %custom-call.52.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<835 custom-call.52.0{0} @0>
 positions:
  custom-call.52.0 {0}
  get-tuple-element.25.0
 uses:
  fusion.155, operand 1
 from instruction: %custom-call.52.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<836 custom-call.52.0{1} @0>
 positions:
  custom-call.52.0 {1}
 uses:
 from instruction: %custom-call.52.0 = (bf16[16,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[16,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"49152","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<837 fusion.155 @0>
 positions:
  fusion.155
 uses:
  custom-call.53.0, operand 0
 from instruction: %fusion.155 = bf16[16,1024]{1,0} fusion(f32[] %p.15, bf16[16,1024]{1,0} %get-tuple-element.25.0, bf16[1024]{0} %p.55, f32[16,1024]{1,0} %loop_add_fusion.2, bf16[16,1024]{1,0} %get-tuple-element.21.0, /*index=5*/bf16[16,13312]{1,0} %gemm_fusion_dot.51.0, bf16[16,1024]{1,0} %get-tuple-element.23.0), kind=kCustom, calls=%fused_computation.124, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<838 custom-call.53.0{} @0>
 positions:
  custom-call.53.0 {}
 uses:
  get-tuple-element.26.0, operand 0 {}
 from instruction: %custom-call.53.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.155, bf16[4096,1024]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<839 custom-call.53.0{0} @0>
 positions:
  custom-call.53.0 {0}
  get-tuple-element.26.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.15.0, operand 1
 from instruction: %custom-call.53.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.155, bf16[4096,1024]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<840 custom-call.53.0{1} @0>
 positions:
  custom-call.53.0 {1}
 uses:
 from instruction: %custom-call.53.0 = (bf16[16,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[16,1024]{1,0} %fusion.155, bf16[4096,1024]{1,0} %p.56), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"16384","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<841 triton_softmax.15.0 @0>
 positions:
  triton_softmax.15.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.15.0 = f32[16,8,128]{2,1,0} fusion(f32[] %p.15, bf16[16,4096]{1,0} %get-tuple-element.26.0), kind=kCustom, calls=%triton_softmax_computation.15, metadata={op_type="aten__mul" op_name="aten__mul.239/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1","128"]}],"num_warps":"1"}},"force_earliest_schedule":false}
<842 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.1106.0 {0}
  call {0}
  get-tuple-element.29
  tuple {0}
 uses:
  tuple, operand 0
  tuple.1106.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[16,8,128]{2,1,0} fusion(f32[16,8,128]{2,1,0} %triton_softmax.15.0, bf16[128]{0} %p.57, bf16[40960,128]{1,0} %p.58, s32[16]{0} %p.59), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<843 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.1106.0 {1}
  call {1}
  get-tuple-element.30
  bitcast.2812.0
  tuple {1}
 uses:
  bitcast.2812.0, operand 0
  tuple.1106.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[16,1024]{1,0} fusion(bf16[16,4096]{1,0} %get-tuple-element.26.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<844 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.1106.0 {3}
  call {2}
  get-tuple-element.31
  bitcast.2824.0
  tuple {2}
 uses:
  bitcast.2824.0, operand 0
  tuple.1106.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.60), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<845 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.1106.0 {2}
  bitcast.2817.0
  call {3}
  get-tuple-element.32
  tuple {3}
 uses:
  tuple, operand 3
  tuple.1106.0, operand 2
  bitcast.2817.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.60), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<846 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.29, operand 0 {}
  get-tuple-element.30, operand 0 {}
  get-tuple-element.31, operand 0 {}
  get-tuple-element.32, operand 0 {}
 from instruction: %tuple = (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[16,8,128]{2,1,0} %input_concatenate_fusion, bf16[16,8,128]{2,1,0} %bitcast.2812.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.2824.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<847 p5.40.0 @0>
 positions:
  p5.40.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.40.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<848 p4.38.0 @0>
 positions:
  p4.38.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.38.0 = s32[16]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<849 p54.908.0 @0>
 positions:
  p54.908.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p54.908.0 = bf16[1024,2048]{1,0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<850 p50.836.0 @0>
 positions:
  p50.836.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p50.836.0 = bf16[1024,2048]{1,0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<851 p46.764.0 @0>
 positions:
  p46.764.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p46.764.0 = bf16[1024,2048]{1,0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<852 p42.692.0 @0>
 positions:
  p42.692.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p42.692.0 = bf16[1024,2048]{1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<853 p38.620.0 @0>
 positions:
  p38.620.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p38.620.0 = bf16[1024,2048]{1,0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<854 p34.548.0 @0>
 positions:
  p34.548.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p34.548.0 = bf16[1024,2048]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<855 p30.476.0 @0>
 positions:
  p30.476.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p30.476.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<856 p26.404.0 @0>
 positions:
  p26.404.0
  p.9
 uses:
  call, operand 9
  wrapped_concatenate, operand 7
 from instruction: %p26.404.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<857 p22.332.0 @0>
 positions:
  p22.332.0
  p.10
 uses:
  call, operand 10
  wrapped_concatenate, operand 8
 from instruction: %p22.332.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<858 p18.260.0 @0>
 positions:
  p18.260.0
  p.11
 uses:
  call, operand 11
  wrapped_concatenate, operand 9
 from instruction: %p18.260.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<859 p14.188.0 @0>
 positions:
  p14.188.0
  p.12
 uses:
  call, operand 12
  wrapped_concatenate, operand 10
 from instruction: %p14.188.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<860 p10.116.0 @0>
 positions:
  p10.116.0
  p.13
 uses:
  call, operand 13
  wrapped_concatenate, operand 11
 from instruction: %p10.116.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<861 p6.44.0 @0>
 positions:
  p6.44.0
  p.14
 uses:
  call, operand 14
  wrapped_concatenate, operand 12
 from instruction: %p6.44.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<862 p1.4.0 @0>
 positions:
  p1.4.0
  p.15
 uses:
  call, operand 15
  fusion.181, operand 0
  fusion.179, operand 0
  fusion.177, operand 1
  fusion.175, operand 0
  fusion.173, operand 0
  fusion.171, operand 0
  fusion.169, operand 1
  fusion.167, operand 0
  fusion.165, operand 0
  fusion.163, operand 0
  fusion.161, operand 1
  fusion.159, operand 0
  fusion.157, operand 0
  fusion.155, operand 0
  triton_softmax.15.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<863 p9.64.0 @0>
 positions:
  p9.64.0
  p.16
 uses:
  call, operand 16
  fusion.181, operand 3
 from instruction: %p9.64.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<864 p8.62.0 @0>
 positions:
  p8.62.0
  p.17
 uses:
  call, operand 17
  custom-call.27.0, operand 1
 from instruction: %p8.62.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<865 p7.60.0 @0>
 positions:
  p7.60.0
  p.18
 uses:
  call, operand 18
  custom-call.28.0, operand 1
 from instruction: %p7.60.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<866 p13.136.0 @0>
 positions:
  p13.136.0
  p.19
 uses:
  call, operand 19
  fusion.179, operand 1
 from instruction: %p13.136.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<867 p12.134.0 @0>
 positions:
  p12.134.0
  p.20
 uses:
  call, operand 20
  custom-call.29.0, operand 1
 from instruction: %p12.134.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<868 p11.132.0 @0>
 positions:
  p11.132.0
  p.21
 uses:
  call, operand 21
  custom-call.30.0, operand 1
 from instruction: %p11.132.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<869 p17.208.0 @0>
 positions:
  p17.208.0
  p.22
 uses:
  call, operand 22
  fusion.177, operand 2
 from instruction: %p17.208.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<870 p16.206.0 @0>
 positions:
  p16.206.0
  p.23
 uses:
  call, operand 23
  custom-call.31.0, operand 1
 from instruction: %p16.206.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<871 p15.204.0 @0>
 positions:
  p15.204.0
  p.24
 uses:
  call, operand 24
  custom-call.32.0, operand 1
 from instruction: %p15.204.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<872 p21.280.0 @0>
 positions:
  p21.280.0
  p.25
 uses:
  call, operand 25
  fusion.175, operand 1
 from instruction: %p21.280.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<873 p20.278.0 @0>
 positions:
  p20.278.0
  p.26
 uses:
  call, operand 26
  custom-call.33.0, operand 1
 from instruction: %p20.278.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<874 p19.276.0 @0>
 positions:
  p19.276.0
  p.27
 uses:
  call, operand 27
  custom-call.34.0, operand 1
 from instruction: %p19.276.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<875 p25.352.0 @0>
 positions:
  p25.352.0
  p.28
 uses:
  call, operand 28
  fusion.173, operand 1
 from instruction: %p25.352.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<876 p24.350.0 @0>
 positions:
  p24.350.0
  p.29
 uses:
  call, operand 29
  custom-call.35.0, operand 1
 from instruction: %p24.350.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<877 p23.348.0 @0>
 positions:
  p23.348.0
  p.30
 uses:
  call, operand 30
  custom-call.36.0, operand 1
 from instruction: %p23.348.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<878 p29.424.0 @0>
 positions:
  p29.424.0
  p.31
 uses:
  call, operand 31
  fusion.171, operand 1
 from instruction: %p29.424.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<879 p28.422.0 @0>
 positions:
  p28.422.0
  p.32
 uses:
  call, operand 32
  custom-call.37.0, operand 1
 from instruction: %p28.422.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<880 p27.420.0 @0>
 positions:
  p27.420.0
  p.33
 uses:
  call, operand 33
  custom-call.38.0, operand 1
 from instruction: %p27.420.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<881 p33.496.0 @0>
 positions:
  p33.496.0
  p.34
 uses:
  call, operand 34
  fusion.169, operand 2
 from instruction: %p33.496.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<882 p32.494.0 @0>
 positions:
  p32.494.0
  p.35
 uses:
  call, operand 35
  custom-call.39.0, operand 1
 from instruction: %p32.494.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<883 p31.492.0 @0>
 positions:
  p31.492.0
  p.36
 uses:
  call, operand 36
  custom-call.40.0, operand 1
 from instruction: %p31.492.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<884 p37.568.0 @0>
 positions:
  p37.568.0
  p.37
 uses:
  call, operand 37
  fusion.167, operand 1
 from instruction: %p37.568.0 = bf16[1024]{0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<885 p36.566.0 @0>
 positions:
  p36.566.0
  p.38
 uses:
  call, operand 38
  custom-call.41.0, operand 1
 from instruction: %p36.566.0 = bf16[6144,1024]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<886 p35.564.0 @0>
 positions:
  p35.564.0
  p.39
 uses:
  call, operand 39
  custom-call.42.0, operand 1
 from instruction: %p35.564.0 = bf16[1024,3072]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<887 p41.640.0 @0>
 positions:
  p41.640.0
  p.40
 uses:
  call, operand 40
  fusion.165, operand 1
 from instruction: %p41.640.0 = bf16[1024]{0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<888 p40.638.0 @0>
 positions:
  p40.638.0
  p.41
 uses:
  call, operand 41
  custom-call.43.0, operand 1
 from instruction: %p40.638.0 = bf16[6144,1024]{1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<889 p39.636.0 @0>
 positions:
  p39.636.0
  p.42
 uses:
  call, operand 42
  custom-call.44.0, operand 1
 from instruction: %p39.636.0 = bf16[1024,3072]{1,0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<890 p45.712.0 @0>
 positions:
  p45.712.0
  p.43
 uses:
  call, operand 43
  fusion.163, operand 1
 from instruction: %p45.712.0 = bf16[1024]{0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<891 p44.710.0 @0>
 positions:
  p44.710.0
  p.44
 uses:
  call, operand 44
  custom-call.45.0, operand 1
 from instruction: %p44.710.0 = bf16[6144,1024]{1,0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<892 p43.708.0 @0>
 positions:
  p43.708.0
  p.45
 uses:
  call, operand 45
  custom-call.46.0, operand 1
 from instruction: %p43.708.0 = bf16[1024,3072]{1,0} parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<893 p49.784.0 @0>
 positions:
  p49.784.0
  p.46
 uses:
  call, operand 46
  fusion.161, operand 2
 from instruction: %p49.784.0 = bf16[1024]{0} parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<894 p48.782.0 @0>
 positions:
  p48.782.0
  p.47
 uses:
  call, operand 47
  custom-call.47.0, operand 1
 from instruction: %p48.782.0 = bf16[6144,1024]{1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<895 p47.780.0 @0>
 positions:
  p47.780.0
  p.48
 uses:
  call, operand 48
  custom-call.48.0, operand 1
 from instruction: %p47.780.0 = bf16[1024,3072]{1,0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<896 p53.856.0 @0>
 positions:
  p53.856.0
  p.49
 uses:
  call, operand 49
  fusion.159, operand 1
 from instruction: %p53.856.0 = bf16[1024]{0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<897 p52.854.0 @0>
 positions:
  p52.854.0
  p.50
 uses:
  call, operand 50
  custom-call.49.0, operand 1
 from instruction: %p52.854.0 = bf16[6144,1024]{1,0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<898 p51.852.0 @0>
 positions:
  p51.852.0
  p.51
 uses:
  call, operand 51
  custom-call.50.0, operand 1
 from instruction: %p51.852.0 = bf16[1024,3072]{1,0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<899 p57.928.0 @0>
 positions:
  p57.928.0
  p.52
 uses:
  call, operand 52
  fusion.157, operand 1
 from instruction: %p57.928.0 = bf16[1024]{0} parameter(57), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<900 p56.926.0 @0>
 positions:
  p56.926.0
  p.53
 uses:
  call, operand 53
  custom-call.51.0, operand 1
 from instruction: %p56.926.0 = bf16[6144,1024]{1,0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<901 p55.924.0 @0>
 positions:
  p55.924.0
  p.54
 uses:
  call, operand 54
  custom-call.52.0, operand 1
 from instruction: %p55.924.0 = bf16[1024,3072]{1,0} parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<902 p3.8.0 @0>
 positions:
  p3.8.0
  p.55
 uses:
  call, operand 55
  fusion.155, operand 2
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<903 p2.6.0 @0>
 positions:
  p2.6.0
  p.56
 uses:
  call, operand 56
  custom-call.53.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<904 p0.1.0 @0>
 positions:
  p0.1.0
  p.57
 uses:
  call, operand 57
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<905 p59.1053.0 @0>
 positions:
  p59.1053.0
  p.58
 uses:
  call, operand 58
  input_concatenate_fusion, operand 2
 from instruction: %p59.1053.0 = bf16[40960,128]{1,0} parameter(59), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<906 p58.1052.0 @0>
 positions:
  p58.1052.0
  p.59
 uses:
  call, operand 59
  input_concatenate_fusion, operand 3
 from instruction: %p58.1052.0 = s32[16]{0} parameter(58), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<907 p60.1097.0 @0>
 positions:
  p60.1097.0
  p.60
 uses:
  call, operand 60
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p60.1097.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(60), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<908 tuple.1106.0{} @0>
 positions:
  tuple.1106.0 {}
 uses:
 from instruction: %tuple.1106.0 = (bf16[16,8,128]{2,1,0}, bf16[16,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[16,8,128]{2,1,0} %get-tuple-element.29, bf16[16,8,128]{2,1,0} %get-tuple-element.30, bf16[4233,16,8,128]{3,2,1,0} %bitcast.2817.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.31)


HloLiveRange (max 224):
  InstructionSequence:
    0:p1.4.0
    1:p60.1097.0
    2:p59.1053.0
    3:p58.1052.0
    4:p57.928.0
    5:p56.926.0
    6:p55.924.0
    7:p54.908.0
    8:p53.856.0
    9:p52.854.0
    10:p51.852.0
    11:p50.836.0
    12:p49.784.0
    13:p48.782.0
    14:p47.780.0
    15:p46.764.0
    16:p45.712.0
    17:p44.710.0
    18:p43.708.0
    19:p42.692.0
    20:p41.640.0
    21:p40.638.0
    22:p39.636.0
    23:p38.620.0
    24:p37.568.0
    25:p36.566.0
    26:p35.564.0
    27:p34.548.0
    28:p33.496.0
    29:p32.494.0
    30:p31.492.0
    31:p30.476.0
    32:p29.424.0
    33:p28.422.0
    34:p27.420.0
    35:p26.404.0
    36:p25.352.0
    37:p24.350.0
    38:p23.348.0
    39:p22.332.0
    40:p21.280.0
    41:p20.278.0
    42:p19.276.0
    43:p18.260.0
    44:p17.208.0
    45:p16.206.0
    46:p15.204.0
    47:p14.188.0
    48:p13.136.0
    49:p12.134.0
    50:p11.132.0
    51:p10.116.0
    52:p9.64.0
    53:p8.62.0
    54:p7.60.0
    55:p6.44.0
    56:p5.40.0
    57:p4.38.0
    58:p3.8.0
    59:p2.6.0
    60:p0.1.0
    61:p
    62:p.1
    63:p.2
    64:p.3
    65:p.4
    66:p.5
    67:p.6
    68:p.7
    69:p.8
    70:p.9
    71:p.10
    72:p.11
    73:p.12
    74:p.13
    75:p.14
    76:p.15
    77:p.16
    78:p.17
    79:p.18
    80:p.19
    81:p.20
    82:p.21
    83:p.22
    84:p.23
    85:p.24
    86:p.25
    87:p.26
    88:p.27
    89:p.28
    90:p.29
    91:p.30
    92:p.31
    93:p.32
    94:p.33
    95:p.34
    96:p.35
    97:p.36
    98:p.37
    99:p.38
    100:p.39
    101:p.40
    102:p.41
    103:p.42
    104:p.43
    105:p.44
    106:p.45
    107:p.46
    108:p.47
    109:p.48
    110:p.49
    111:p.50
    112:p.51
    113:p.52
    114:p.53
    115:p.54
    116:p.55
    117:p.56
    118:p.57
    119:p.58
    120:p.59
    121:p.60
    122:loop_gather_fusion
    123:wrapped_concatenate
    124:gemm_fusion_dot.51.0
    125:fusion.181
    126:custom-call.27.0
    127:get-tuple-element.27
    128:loop_convert_fusion.12
    129:custom-call.28.0
    130:get-tuple-element.1.0
    131:fusion.179
    132:custom-call.29.0
    133:get-tuple-element.2.0
    134:loop_convert_fusion.11
    135:custom-call.30.0
    136:get-tuple-element.3.0
    137:loop_add_fusion
    138:fusion.177
    139:custom-call.31.0
    140:get-tuple-element.4.0
    141:loop_convert_fusion.10
    142:custom-call.32.0
    143:get-tuple-element.5.0
    144:fusion.175
    145:custom-call.33.0
    146:get-tuple-element.6.0
    147:loop_convert_fusion.9
    148:custom-call.34.0
    149:get-tuple-element.7.0
    150:fusion.173
    151:custom-call.35.0
    152:get-tuple-element.8.0
    153:loop_convert_fusion.8
    154:custom-call.36.0
    155:get-tuple-element.9.0
    156:fusion.171
    157:custom-call.37.0
    158:get-tuple-element.10.0
    159:loop_convert_fusion.7
    160:custom-call.38.0
    161:get-tuple-element.11.0
    162:loop_add_fusion.1
    163:fusion.169
    164:custom-call.39.0
    165:get-tuple-element.12.0
    166:loop_convert_fusion.6
    167:custom-call.40.0
    168:get-tuple-element.13.0
    169:fusion.167
    170:custom-call.41.0
    171:get-tuple-element.14.0
    172:loop_convert_fusion.5
    173:custom-call.42.0
    174:get-tuple-element.15.0
    175:fusion.165
    176:custom-call.43.0
    177:get-tuple-element.16.0
    178:loop_convert_fusion.4
    179:custom-call.44.0
    180:get-tuple-element.17.0
    181:fusion.163
    182:custom-call.45.0
    183:get-tuple-element.18.0
    184:loop_convert_fusion.3
    185:custom-call.46.0
    186:get-tuple-element.19.0
    187:loop_add_fusion.2
    188:fusion.161
    189:custom-call.47.0
    190:get-tuple-element.20.0
    191:loop_convert_fusion.2
    192:custom-call.48.0
    193:get-tuple-element.21.0
    194:fusion.159
    195:custom-call.49.0
    196:get-tuple-element.22.0
    197:loop_convert_fusion.1
    198:custom-call.50.0
    199:get-tuple-element.23.0
    200:fusion.157
    201:custom-call.51.0
    202:get-tuple-element.24.0
    203:loop_convert_fusion
    204:custom-call.52.0
    205:get-tuple-element.25.0
    206:fusion.155
    207:custom-call.53.0
    208:get-tuple-element.26.0
    209:wrapped_slice
    210:triton_softmax.15.0
    211:input_concatenate_fusion
    212:bitcast.2812.0
    213:loop_slice_fusion
    214:bitcast.2824.0
    215:wrapped_slice.1
    216:tuple
    217:call
    218:get-tuple-element.29
    219:get-tuple-element.30
    220:get-tuple-element.31
    221:get-tuple-element.32
    222:bitcast.2817.0
    223:tuple.1106.0
  BufferLiveRange:
    wrapped_concatenate{}:123-124
    gemm_fusion_dot.51.0{}:124-206
    loop_gather_fusion{}:122-137
    fusion.181{}:125-126
    custom-call.27.0{}:126-127
    custom-call.27.0{0}:126-128
    custom-call.27.0{1}:126-126
    loop_convert_fusion.12{}:128-129
    custom-call.28.0{}:129-130
    custom-call.28.0{0}:129-137
    custom-call.28.0{1}:129-129
    fusion.179{}:131-132
    custom-call.29.0{}:132-133
    custom-call.29.0{0}:132-134
    custom-call.29.0{1}:132-132
    loop_convert_fusion.11{}:134-135
    custom-call.30.0{}:135-136
    custom-call.30.0{0}:135-137
    custom-call.30.0{1}:135-135
    loop_add_fusion{}:137-162
    fusion.177{}:138-139
    custom-call.31.0{}:139-140
    custom-call.31.0{0}:139-141
    custom-call.31.0{1}:139-139
    loop_convert_fusion.10{}:141-142
    custom-call.32.0{}:142-143
    custom-call.32.0{0}:142-162
    custom-call.32.0{1}:142-142
    fusion.175{}:144-145
    custom-call.33.0{}:145-146
    custom-call.33.0{0}:145-147
    custom-call.33.0{1}:145-145
    loop_convert_fusion.9{}:147-148
    custom-call.34.0{}:148-149
    custom-call.34.0{0}:148-162
    custom-call.34.0{1}:148-148
    fusion.173{}:150-151
    custom-call.35.0{}:151-152
    custom-call.35.0{0}:151-153
    custom-call.35.0{1}:151-151
    loop_convert_fusion.8{}:153-154
    custom-call.36.0{}:154-155
    custom-call.36.0{0}:154-162
    custom-call.36.0{1}:154-154
    fusion.171{}:156-157
    custom-call.37.0{}:157-158
    custom-call.37.0{0}:157-159
    custom-call.37.0{1}:157-157
    loop_convert_fusion.7{}:159-160
    custom-call.38.0{}:160-161
    custom-call.38.0{0}:160-162
    custom-call.38.0{1}:160-160
    loop_add_fusion.1{}:162-187
    fusion.169{}:163-164
    custom-call.39.0{}:164-165
    custom-call.39.0{0}:164-166
    custom-call.39.0{1}:164-164
    loop_convert_fusion.6{}:166-167
    custom-call.40.0{}:167-168
    custom-call.40.0{0}:167-187
    custom-call.40.0{1}:167-167
    fusion.167{}:169-170
    custom-call.41.0{}:170-171
    custom-call.41.0{0}:170-172
    custom-call.41.0{1}:170-170
    loop_convert_fusion.5{}:172-173
    custom-call.42.0{}:173-174
    custom-call.42.0{0}:173-187
    custom-call.42.0{1}:173-173
    fusion.165{}:175-176
    custom-call.43.0{}:176-177
    custom-call.43.0{0}:176-178
    custom-call.43.0{1}:176-176
    loop_convert_fusion.4{}:178-179
    custom-call.44.0{}:179-180
    custom-call.44.0{0}:179-187
    custom-call.44.0{1}:179-179
    fusion.163{}:181-182
    custom-call.45.0{}:182-183
    custom-call.45.0{0}:182-184
    custom-call.45.0{1}:182-182
    loop_convert_fusion.3{}:184-185
    custom-call.46.0{}:185-186
    custom-call.46.0{0}:185-187
    custom-call.46.0{1}:185-185
    loop_add_fusion.2{}:187-206
    fusion.161{}:188-189
    custom-call.47.0{}:189-190
    custom-call.47.0{0}:189-191
    custom-call.47.0{1}:189-189
    loop_convert_fusion.2{}:191-192
    custom-call.48.0{}:192-193
    custom-call.48.0{0}:192-206
    custom-call.48.0{1}:192-192
    fusion.159{}:194-195
    custom-call.49.0{}:195-196
    custom-call.49.0{0}:195-197
    custom-call.49.0{1}:195-195
    loop_convert_fusion.1{}:197-198
    custom-call.50.0{}:198-199
    custom-call.50.0{0}:198-206
    custom-call.50.0{1}:198-198
    fusion.157{}:200-201
    custom-call.51.0{}:201-202
    custom-call.51.0{0}:201-203
    custom-call.51.0{1}:201-201
    loop_convert_fusion{}:203-204
    custom-call.52.0{}:204-205
    custom-call.52.0{0}:204-206
    custom-call.52.0{1}:204-204
    fusion.155{}:206-207
    custom-call.53.0{}:207-208
    custom-call.53.0{0}:207-210
    custom-call.53.0{1}:207-207
    triton_softmax.15.0{}:210-211
    input_concatenate_fusion{}:211-224
    wrapped_slice{}:209-224
    loop_slice_fusion{}:213-224
    wrapped_slice.1{}:215-224
    tuple{}:216-221
    p5.40.0{}:0-224
    p4.38.0{}:0-224
    p54.908.0{}:0-224
    p50.836.0{}:0-224
    p46.764.0{}:0-224
    p42.692.0{}:0-224
    p38.620.0{}:0-224
    p34.548.0{}:0-224
    p30.476.0{}:0-224
    p26.404.0{}:0-224
    p22.332.0{}:0-224
    p18.260.0{}:0-224
    p14.188.0{}:0-224
    p10.116.0{}:0-224
    p6.44.0{}:0-224
    p1.4.0{}:0-224
    p9.64.0{}:0-224
    p8.62.0{}:0-224
    p7.60.0{}:0-224
    p13.136.0{}:0-224
    p12.134.0{}:0-224
    p11.132.0{}:0-224
    p17.208.0{}:0-224
    p16.206.0{}:0-224
    p15.204.0{}:0-224
    p21.280.0{}:0-224
    p20.278.0{}:0-224
    p19.276.0{}:0-224
    p25.352.0{}:0-224
    p24.350.0{}:0-224
    p23.348.0{}:0-224
    p29.424.0{}:0-224
    p28.422.0{}:0-224
    p27.420.0{}:0-224
    p33.496.0{}:0-224
    p32.494.0{}:0-224
    p31.492.0{}:0-224
    p37.568.0{}:0-224
    p36.566.0{}:0-224
    p35.564.0{}:0-224
    p41.640.0{}:0-224
    p40.638.0{}:0-224
    p39.636.0{}:0-224
    p45.712.0{}:0-224
    p44.710.0{}:0-224
    p43.708.0{}:0-224
    p49.784.0{}:0-224
    p48.782.0{}:0-224
    p47.780.0{}:0-224
    p53.856.0{}:0-224
    p52.854.0{}:0-224
    p51.852.0{}:0-224
    p57.928.0{}:0-224
    p56.926.0{}:0-224
    p55.924.0{}:0-224
    p3.8.0{}:0-224
    p2.6.0{}:0-224
    p0.1.0{}:0-224
    p59.1053.0{}:0-224
    p58.1052.0{}:0-224
    p60.1097.0{}:0-224
    tuple.1106.0{}:223-224
  Live ranges at 216 (peak):
    input_concatenate_fusion: 32768 bytes
    wrapped_slice: 32768 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.40.0: 311164928 bytes
    p4.38.0: 64 bytes
    p54.908.0: 4194304 bytes
    p50.836.0: 4194304 bytes
    p46.764.0: 4194304 bytes
    p42.692.0: 4194304 bytes
    p38.620.0: 4194304 bytes
    p34.548.0: 4194304 bytes
    p30.476.0: 4194304 bytes
    p26.404.0: 4194304 bytes
    p22.332.0: 4194304 bytes
    p18.260.0: 4194304 bytes
    p14.188.0: 4194304 bytes
    p10.116.0: 4194304 bytes
    p6.44.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.64.0: 2048 bytes
    p8.62.0: 12582912 bytes
    p7.60.0: 6291456 bytes
    p13.136.0: 2048 bytes
    p12.134.0: 12582912 bytes
    p11.132.0: 6291456 bytes
    p17.208.0: 2048 bytes
    p16.206.0: 12582912 bytes
    p15.204.0: 6291456 bytes
    p21.280.0: 2048 bytes
    p20.278.0: 12582912 bytes
    p19.276.0: 6291456 bytes
    p25.352.0: 2048 bytes
    p24.350.0: 12582912 bytes
    p23.348.0: 6291456 bytes
    p29.424.0: 2048 bytes
    p28.422.0: 12582912 bytes
    p27.420.0: 6291456 bytes
    p33.496.0: 2048 bytes
    p32.494.0: 12582912 bytes
    p31.492.0: 6291456 bytes
    p37.568.0: 2048 bytes
    p36.566.0: 12582912 bytes
    p35.564.0: 6291456 bytes
    p41.640.0: 2048 bytes
    p40.638.0: 12582912 bytes
    p39.636.0: 6291456 bytes
    p45.712.0: 2048 bytes
    p44.710.0: 12582912 bytes
    p43.708.0: 6291456 bytes
    p49.784.0: 2048 bytes
    p48.782.0: 12582912 bytes
    p47.780.0: 6291456 bytes
    p53.856.0: 2048 bytes
    p52.854.0: 12582912 bytes
    p51.852.0: 6291456 bytes
    p57.928.0: 2048 bytes
    p56.926.0: 12582912 bytes
    p55.924.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p59.1053.0: 10485760 bytes
    p58.1052.0: 64 bytes
    p60.1097.0: 277413888 bytes
