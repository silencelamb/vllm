BufferAssignment:
allocation 0: size 311164928, parameter 5, shape |bf16[151936,1024]| at ShapeIndex {}:
 value: <1452 p5.60.0 @0> (size=311164928,offset=0): bf16[151936,1024]{1,0}
allocation 1: size 277413888, parameter 100, shape |bf16[2,4233,16,8,128]| at ShapeIndex {}:
 value: <1552 p100.1837.0 @0> (size=277413888,offset=0): bf16[2,4233,16,8,128]{4,3,2,1,0}
allocation 2: size 138706944, maybe-live-out:
 value: <1251 gemm_fusion_dot.92.0 @0> (size=3014656,offset=0): bf16[64,23552]{1,0}
 value: <1444 custom-call.93.0{0} @0> (size=524288,offset=0): bf16[64,4096]{1,0}
 value: <1449 loop_slice_fusion @0> (size=138706944,offset=0): bf16[69353472]{0}
allocation 3: size 138706944, maybe-live-out:
 value: <1250 wrapped_concatenate @0> (size=96468992,offset=0): bf16[23552,2048]{1,0}
 value: <1256 custom-call.47.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1260 custom-call.48.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1264 custom-call.49.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1268 custom-call.50.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1272 custom-call.51.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1276 custom-call.52.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1280 custom-call.53.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1284 custom-call.54.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1289 custom-call.55.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1293 custom-call.56.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1297 custom-call.57.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1301 custom-call.58.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1305 custom-call.59.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1309 custom-call.60.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1313 custom-call.61.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1317 custom-call.62.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1322 custom-call.63.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1326 custom-call.64.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1330 custom-call.65.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1334 custom-call.66.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1338 custom-call.67.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1342 custom-call.68.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1346 custom-call.69.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1350 custom-call.70.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1355 custom-call.71.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1359 custom-call.72.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1363 custom-call.73.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1367 custom-call.74.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1371 custom-call.75.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1375 custom-call.76.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1379 custom-call.77.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1383 custom-call.78.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1388 custom-call.79.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1392 custom-call.80.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1396 custom-call.81.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1400 custom-call.82.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1404 custom-call.83.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1408 custom-call.84.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1412 custom-call.85.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1416 custom-call.86.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1421 custom-call.87.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1425 custom-call.88.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1429 custom-call.89.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1433 custom-call.90.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1437 custom-call.91.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1441 custom-call.92.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1445 custom-call.93.0{1} @0> (size=4194304,offset=0): s8[4194304]{0}
 value: <1446 triton_softmax.25.0 @0> (size=262144,offset=0): f32[64,8,128]{2,1,0}
 value: <1450 wrapped_slice.1 @0> (size=138706944,offset=0): bf16[1,4233,16,8,128]{4,3,2,1,0}
allocation 4: size 12582912, parameter 8, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1479 p8.82.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 5: size 12582912, parameter 12, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1482 p12.154.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 6: size 12582912, parameter 16, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1485 p16.226.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 7: size 12582912, parameter 20, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1488 p20.298.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 8: size 12582912, parameter 24, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1491 p24.370.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 9: size 12582912, parameter 28, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1494 p28.442.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 10: size 12582912, parameter 32, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1497 p32.514.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 11: size 12582912, parameter 36, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1500 p36.586.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 12: size 12582912, parameter 40, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1503 p40.658.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 13: size 12582912, parameter 44, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1506 p44.730.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 14: size 12582912, parameter 48, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1509 p48.802.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 15: size 12582912, parameter 52, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1512 p52.874.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 16: size 12582912, parameter 56, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1515 p56.946.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 17: size 12582912, parameter 60, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1518 p60.1018.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 18: size 12582912, parameter 64, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1521 p64.1090.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 19: size 12582912, parameter 68, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1524 p68.1162.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 20: size 12582912, parameter 72, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1527 p72.1234.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 21: size 12582912, parameter 76, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1530 p76.1306.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 22: size 12582912, parameter 80, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1533 p80.1378.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 23: size 12582912, parameter 84, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1536 p84.1450.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 24: size 12582912, parameter 88, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1539 p88.1522.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 25: size 12582912, parameter 92, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1542 p92.1594.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 26: size 12582912, parameter 96, shape |bf16[6144,1024]| at ShapeIndex {}:
 value: <1545 p96.1666.0 @0> (size=12582912,offset=0): bf16[6144,1024]{1,0}
allocation 27: size 10485760, parameter 99, shape |bf16[40960,128]| at ShapeIndex {}:
 value: <1550 p99.1793.0 @0> (size=10485760,offset=0): bf16[40960,128]{1,0}
allocation 28: size 8388608, parameter 2, shape |bf16[4096,1024]| at ShapeIndex {}:
 value: <1548 p2.6.0 @0> (size=8388608,offset=0): bf16[4096,1024]{1,0}
allocation 29: size 6291456, parameter 7, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1480 p7.80.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 30: size 6291456, parameter 11, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1483 p11.152.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 31: size 6291456, parameter 15, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1486 p15.224.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 32: size 6291456, parameter 19, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1489 p19.296.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 33: size 6291456, parameter 23, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1492 p23.368.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 34: size 6291456, parameter 27, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1495 p27.440.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 35: size 6291456, parameter 31, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1498 p31.512.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 36: size 6291456, parameter 35, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1501 p35.584.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 37: size 6291456, parameter 39, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1504 p39.656.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 38: size 6291456, parameter 43, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1507 p43.728.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 39: size 6291456, parameter 47, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1510 p47.800.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 40: size 6291456, parameter 51, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1513 p51.872.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 41: size 6291456, parameter 55, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1516 p55.944.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 42: size 6291456, parameter 59, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1519 p59.1016.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 43: size 6291456, parameter 63, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1522 p63.1088.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 44: size 6291456, parameter 67, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1525 p67.1160.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 45: size 6291456, parameter 71, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1528 p71.1232.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 46: size 6291456, parameter 75, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1531 p75.1304.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 47: size 6291456, parameter 79, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1534 p79.1376.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 48: size 6291456, parameter 83, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1537 p83.1448.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 49: size 6291456, parameter 87, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1540 p87.1520.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 50: size 6291456, parameter 91, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1543 p91.1592.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 51: size 6291456, parameter 95, shape |bf16[1024,3072]| at ShapeIndex {}:
 value: <1546 p95.1664.0 @0> (size=6291456,offset=0): bf16[1024,3072]{1,0}
allocation 52: size 4194304, parameter 94, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1454 p94.1648.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 53: size 4194304, parameter 90, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1455 p90.1576.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 54: size 4194304, parameter 86, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1456 p86.1504.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 55: size 4194304, parameter 82, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1457 p82.1432.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 56: size 4194304, parameter 78, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1458 p78.1360.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 57: size 4194304, parameter 74, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1459 p74.1288.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 58: size 4194304, parameter 70, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1460 p70.1216.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 59: size 4194304, parameter 66, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1461 p66.1144.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 60: size 4194304, parameter 62, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1462 p62.1072.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 61: size 4194304, parameter 58, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1463 p58.1000.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 62: size 4194304, parameter 54, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1464 p54.928.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 63: size 4194304, parameter 50, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1465 p50.856.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 64: size 4194304, parameter 46, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1466 p46.784.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 65: size 4194304, parameter 42, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1467 p42.712.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 66: size 4194304, parameter 38, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1468 p38.640.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 67: size 4194304, parameter 34, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1469 p34.568.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 68: size 4194304, parameter 30, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1470 p30.496.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 69: size 4194304, parameter 26, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1471 p26.424.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 70: size 4194304, parameter 22, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1472 p22.352.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 71: size 4194304, parameter 18, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1473 p18.280.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 72: size 4194304, parameter 14, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1474 p14.208.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 73: size 4194304, parameter 10, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1475 p10.136.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 74: size 4194304, parameter 6, shape |bf16[1024,2048]| at ShapeIndex {}:
 value: <1476 p6.64.0 @0> (size=4194304,offset=0): bf16[1024,2048]{1,0}
allocation 75: size 131072, maybe-live-out:
 value: <1253 fusion.291 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1259 custom-call.48.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1294 fusion.286 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1300 custom-call.58.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1327 fusion.282 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1333 custom-call.66.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1360 fusion.278 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1366 custom-call.74.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1393 fusion.274 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1399 custom-call.82.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1426 fusion.270 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1432 custom-call.90.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1447 input_concatenate_fusion @0> (size=131072,offset=0): bf16[64,8,128]{2,1,0}
allocation 76: size 131072, maybe-live-out:
 value: <1252 loop_gather_fusion @0> (size=131072,offset=0): bf16[64,1,1024]{2,0,1}
 value: <1286 fusion.287 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1292 custom-call.56.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1319 fusion.283 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1325 custom-call.64.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1352 fusion.279 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1358 custom-call.72.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1385 fusion.275 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1391 custom-call.80.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1418 fusion.271 @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1424 custom-call.88.0{0} @0> (size=131072,offset=0): bf16[64,1024]{1,0}
 value: <1448 wrapped_slice @0> (size=131072,offset=0): bf16[64,1024]{1,0}
allocation 77: size 2048, parameter 9, shape |bf16[1024]| at ShapeIndex {}:
 value: <1478 p9.84.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 78: size 2048, parameter 13, shape |bf16[1024]| at ShapeIndex {}:
 value: <1481 p13.156.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 79: size 2048, parameter 17, shape |bf16[1024]| at ShapeIndex {}:
 value: <1484 p17.228.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 80: size 2048, parameter 21, shape |bf16[1024]| at ShapeIndex {}:
 value: <1487 p21.300.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 81: size 2048, parameter 25, shape |bf16[1024]| at ShapeIndex {}:
 value: <1490 p25.372.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 82: size 2048, parameter 29, shape |bf16[1024]| at ShapeIndex {}:
 value: <1493 p29.444.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 83: size 2048, parameter 33, shape |bf16[1024]| at ShapeIndex {}:
 value: <1496 p33.516.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 84: size 2048, parameter 37, shape |bf16[1024]| at ShapeIndex {}:
 value: <1499 p37.588.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 85: size 2048, parameter 41, shape |bf16[1024]| at ShapeIndex {}:
 value: <1502 p41.660.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 86: size 2048, parameter 45, shape |bf16[1024]| at ShapeIndex {}:
 value: <1505 p45.732.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 87: size 2048, parameter 49, shape |bf16[1024]| at ShapeIndex {}:
 value: <1508 p49.804.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 88: size 2048, parameter 53, shape |bf16[1024]| at ShapeIndex {}:
 value: <1511 p53.876.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 89: size 2048, parameter 57, shape |bf16[1024]| at ShapeIndex {}:
 value: <1514 p57.948.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 90: size 2048, parameter 61, shape |bf16[1024]| at ShapeIndex {}:
 value: <1517 p61.1020.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 91: size 2048, parameter 65, shape |bf16[1024]| at ShapeIndex {}:
 value: <1520 p65.1092.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 92: size 2048, parameter 69, shape |bf16[1024]| at ShapeIndex {}:
 value: <1523 p69.1164.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 93: size 2048, parameter 73, shape |bf16[1024]| at ShapeIndex {}:
 value: <1526 p73.1236.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 94: size 2048, parameter 77, shape |bf16[1024]| at ShapeIndex {}:
 value: <1529 p77.1308.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 95: size 2048, parameter 81, shape |bf16[1024]| at ShapeIndex {}:
 value: <1532 p81.1380.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 96: size 2048, parameter 85, shape |bf16[1024]| at ShapeIndex {}:
 value: <1535 p85.1452.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 97: size 2048, parameter 89, shape |bf16[1024]| at ShapeIndex {}:
 value: <1538 p89.1524.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 98: size 2048, parameter 93, shape |bf16[1024]| at ShapeIndex {}:
 value: <1541 p93.1596.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 99: size 2048, parameter 97, shape |bf16[1024]| at ShapeIndex {}:
 value: <1544 p97.1668.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 100: size 2048, parameter 3, shape |bf16[1024]| at ShapeIndex {}:
 value: <1547 p3.8.0 @0> (size=2048,offset=0): bf16[1024]{0}
allocation 101: size 256, parameter 4, shape |s32[64]| at ShapeIndex {}:
 value: <1453 p4.58.0 @0> (size=256,offset=0): s32[64]{0}
allocation 102: size 256, parameter 0, shape |bf16[128]| at ShapeIndex {}:
 value: <1549 p0.1.0 @0> (size=256,offset=0): bf16[128]{0}
allocation 103: size 256, parameter 98, shape |s32[64]| at ShapeIndex {}:
 value: <1551 p98.1792.0 @0> (size=256,offset=0): s32[64]{0}
allocation 104: size 32, output shape is |(bf16[64,8,128], bf16[64,8,128], bf16[4233,16,8,128], bf16[4233,16,8,128])|, maybe-live-out:
 value: <1553 tuple.1846.0{} @0> (size=32,offset=0): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0})
allocation 105: size 4, thread-local:
 value: <35 add.303 @0> (size=4,offset=0): f32[]
allocation 106: size 4, thread-local:
 value: <34 y.94 @0> (size=4,offset=0): f32[]
allocation 107: size 4, thread-local:
 value: <33 x.93 @0> (size=4,offset=0): f32[]
allocation 108: size 4, parameter 1, shape |f32[]| at ShapeIndex {}:
 value: <1477 p1.4.0 @0> (size=4,offset=0): f32[]
allocation 109: size 1709984, preallocated-temp:
 value: <1254 custom-call.47.0{} @0> (size=16,offset=1709824): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1255 custom-call.47.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1257 loop_convert_fusion.22 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1258 custom-call.48.0{} @0> (size=16,offset=1709696): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1261 fusion.290 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1262 custom-call.49.0{} @0> (size=16,offset=5632): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1263 custom-call.49.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1265 loop_convert_fusion.21 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1266 custom-call.50.0{} @0> (size=16,offset=0): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1267 custom-call.50.0{0} @0> (size=131072,offset=1447552): bf16[64,1024]{1,0}
 value: <1269 fusion.289 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1270 custom-call.51.0{} @0> (size=16,offset=128): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1271 custom-call.51.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1273 loop_convert_fusion.20 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1274 custom-call.52.0{} @0> (size=16,offset=256): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1275 custom-call.52.0{0} @0> (size=131072,offset=1578624): bf16[64,1024]{1,0}
 value: <1277 fusion.288 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1278 custom-call.53.0{} @0> (size=16,offset=384): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1279 custom-call.53.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1281 loop_convert_fusion.19 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1282 custom-call.54.0{} @0> (size=16,offset=512): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1283 custom-call.54.0{0} @0> (size=131072,offset=5760): bf16[64,1024]{1,0}
 value: <1285 loop_add_fusion @0> (size=262144,offset=1185408): f32[64,1024]{1,0}
 value: <1287 custom-call.55.0{} @0> (size=16,offset=640): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1288 custom-call.55.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1290 loop_convert_fusion.18 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1291 custom-call.56.0{} @0> (size=16,offset=768): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1295 custom-call.57.0{} @0> (size=16,offset=896): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1296 custom-call.57.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1298 loop_convert_fusion.17 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1299 custom-call.58.0{} @0> (size=16,offset=1024): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1302 fusion.285 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1303 custom-call.59.0{} @0> (size=16,offset=1152): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1304 custom-call.59.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1306 loop_convert_fusion.16 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1307 custom-call.60.0{} @0> (size=16,offset=1280): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1308 custom-call.60.0{0} @0> (size=131072,offset=1447552): bf16[64,1024]{1,0}
 value: <1310 fusion.284 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1311 custom-call.61.0{} @0> (size=16,offset=1408): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1312 custom-call.61.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1314 loop_convert_fusion.15 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1315 custom-call.62.0{} @0> (size=16,offset=1536): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1316 custom-call.62.0{0} @0> (size=131072,offset=5760): bf16[64,1024]{1,0}
 value: <1318 loop_add_fusion.1 @0> (size=262144,offset=1185408): f32[64,1024]{1,0}
 value: <1320 custom-call.63.0{} @0> (size=16,offset=1664): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1321 custom-call.63.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1323 loop_convert_fusion.14 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1324 custom-call.64.0{} @0> (size=16,offset=1792): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1328 custom-call.65.0{} @0> (size=16,offset=1920): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1329 custom-call.65.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1331 loop_convert_fusion.13 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1332 custom-call.66.0{} @0> (size=16,offset=2048): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1335 fusion.281 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1336 custom-call.67.0{} @0> (size=16,offset=2176): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1337 custom-call.67.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1339 loop_convert_fusion.12 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1340 custom-call.68.0{} @0> (size=16,offset=2304): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1341 custom-call.68.0{0} @0> (size=131072,offset=1447552): bf16[64,1024]{1,0}
 value: <1343 fusion.280 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1344 custom-call.69.0{} @0> (size=16,offset=2432): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1345 custom-call.69.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1347 loop_convert_fusion.11 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1348 custom-call.70.0{} @0> (size=16,offset=2560): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1349 custom-call.70.0{0} @0> (size=131072,offset=5760): bf16[64,1024]{1,0}
 value: <1351 loop_add_fusion.2 @0> (size=262144,offset=1185408): f32[64,1024]{1,0}
 value: <1353 custom-call.71.0{} @0> (size=16,offset=2688): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1354 custom-call.71.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1356 loop_convert_fusion.10 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1357 custom-call.72.0{} @0> (size=16,offset=2816): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1361 custom-call.73.0{} @0> (size=16,offset=2944): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1362 custom-call.73.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1364 loop_convert_fusion.9 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1365 custom-call.74.0{} @0> (size=16,offset=3072): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1368 fusion.277 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1369 custom-call.75.0{} @0> (size=16,offset=3200): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1370 custom-call.75.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1372 loop_convert_fusion.8 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1373 custom-call.76.0{} @0> (size=16,offset=3328): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1374 custom-call.76.0{0} @0> (size=131072,offset=1447552): bf16[64,1024]{1,0}
 value: <1376 fusion.276 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1377 custom-call.77.0{} @0> (size=16,offset=3456): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1378 custom-call.77.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1380 loop_convert_fusion.7 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1381 custom-call.78.0{} @0> (size=16,offset=3584): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1382 custom-call.78.0{0} @0> (size=131072,offset=5760): bf16[64,1024]{1,0}
 value: <1384 loop_add_fusion.3 @0> (size=262144,offset=1185408): f32[64,1024]{1,0}
 value: <1386 custom-call.79.0{} @0> (size=16,offset=3712): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1387 custom-call.79.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1389 loop_convert_fusion.6 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1390 custom-call.80.0{} @0> (size=16,offset=3840): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1394 custom-call.81.0{} @0> (size=16,offset=3968): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1395 custom-call.81.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1397 loop_convert_fusion.5 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1398 custom-call.82.0{} @0> (size=16,offset=4096): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1401 fusion.273 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1402 custom-call.83.0{} @0> (size=16,offset=4224): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1403 custom-call.83.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1405 loop_convert_fusion.4 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1406 custom-call.84.0{} @0> (size=16,offset=4352): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1407 custom-call.84.0{0} @0> (size=131072,offset=1447552): bf16[64,1024]{1,0}
 value: <1409 fusion.272 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1410 custom-call.85.0{} @0> (size=16,offset=4480): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1411 custom-call.85.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1413 loop_convert_fusion.3 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1414 custom-call.86.0{} @0> (size=16,offset=4608): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1415 custom-call.86.0{0} @0> (size=131072,offset=5760): bf16[64,1024]{1,0}
 value: <1417 loop_add_fusion.4 @0> (size=262144,offset=1185408): f32[64,1024]{1,0}
 value: <1419 custom-call.87.0{} @0> (size=16,offset=4736): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1420 custom-call.87.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1422 loop_convert_fusion.2 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1423 custom-call.88.0{} @0> (size=16,offset=4864): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1427 custom-call.89.0{} @0> (size=16,offset=4992): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1428 custom-call.89.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1430 loop_convert_fusion.1 @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1431 custom-call.90.0{} @0> (size=16,offset=5120): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1434 fusion.269 @0> (size=131072,offset=792192): bf16[64,1024]{1,0}
 value: <1435 custom-call.91.0{} @0> (size=16,offset=5248): (bf16[64,6144]{1,0}, s8[4194304]{0})
 value: <1436 custom-call.91.0{0} @0> (size=786432,offset=5760): bf16[64,6144]{1,0}
 value: <1438 loop_convert_fusion @0> (size=393216,offset=792192): bf16[64,3072]{1,0}
 value: <1439 custom-call.92.0{} @0> (size=16,offset=5376): (bf16[64,1024]{1,0}, s8[4194304]{0})
 value: <1440 custom-call.92.0{0} @0> (size=131072,offset=5760): bf16[64,1024]{1,0}
 value: <1442 fusion.267 @0> (size=131072,offset=136832): bf16[64,1024]{1,0}
 value: <1443 custom-call.93.0{} @0> (size=16,offset=5504): (bf16[64,4096]{1,0}, s8[4194304]{0})
 value: <1451 tuple{} @0> (size=32,offset=1709952): (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0})

Total bytes used: 1417468624 (1.32GiB)

Used values:
<33 x.93 @0>
 positions:
  x.93
 uses:
  add.303, operand 0
 from instruction: %x.93 = f32[] parameter(0)
<34 y.94 @0>
 positions:
  y.94
 uses:
  add.303, operand 1
 from instruction: %y.94 = f32[] parameter(1)
<35 add.303 @0>
 positions:
  add.303
 uses:
 from instruction: %add.303 = f32[] add(f32[] %x.93, f32[] %y.94)
<1250 wrapped_concatenate @0>
 positions:
  wrapped_concatenate
 uses:
  gemm_fusion_dot.92.0, operand 0
 from instruction: %wrapped_concatenate = bf16[23552,2048]{1,0} fusion(bf16[1024,2048]{1,0} %p.2, bf16[1024,2048]{1,0} %p.3, bf16[1024,2048]{1,0} %p.4, bf16[1024,2048]{1,0} %p.5, bf16[1024,2048]{1,0} %p.6, /*index=5*/bf16[1024,2048]{1,0} %p.7, bf16[1024,2048]{1,0} %p.8, bf16[1024,2048]{1,0} %p.9, bf16[1024,2048]{1,0} %p.10, bf16[1024,2048]{1,0} %p.11, /*index=10*/bf16[1024,2048]{1,0} %p.12, bf16[1024,2048]{1,0} %p.13, bf16[1024,2048]{1,0} %p.14, bf16[1024,2048]{1,0} %p.15, bf16[1024,2048]{1,0} %p.16, /*index=15*/bf16[1024,2048]{1,0} %p.17, bf16[1024,2048]{1,0} %p.18, bf16[1024,2048]{1,0} %p.19, bf16[1024,2048]{1,0} %p.20, bf16[1024,2048]{1,0} %p.21, /*index=20*/bf16[1024,2048]{1,0} %p.22, bf16[1024,2048]{1,0} %p.23, bf16[1024,2048]{1,0} %p.24), kind=kLoop, calls=%wrapped_concatenate_computation, metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1251 gemm_fusion_dot.92.0 @0>
 positions:
  gemm_fusion_dot.92.0
 uses:
  fusion.291, operand 2
  fusion.290, operand 2
  fusion.289, operand 3
  fusion.288, operand 2
  loop_add_fusion, operand 0
  fusion.286, operand 3
  fusion.285, operand 4
  fusion.284, operand 2
  loop_add_fusion.1, operand 0
  fusion.282, operand 3
  fusion.281, operand 4
  fusion.280, operand 2
  loop_add_fusion.2, operand 0
  fusion.278, operand 3
  fusion.277, operand 4
  fusion.276, operand 2
  loop_add_fusion.3, operand 0
  fusion.274, operand 3
  fusion.273, operand 4
  fusion.272, operand 2
  loop_add_fusion.4, operand 0
  fusion.270, operand 3
  fusion.269, operand 4
  fusion.267, operand 5
 from instruction: %gemm_fusion_dot.92.0 = bf16[64,23552]{1,0} fusion(bf16[23552,2048]{1,0} %wrapped_concatenate), kind=kCustom, calls=%gemm_fusion_dot.92_computation, metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton_gemm","triton_gemm_config":{"block_m":"64","block_n":"256","block_k":"128","split_k":"1","num_stages":"3","num_warps":"8","num_ctas":"1"}},"force_earliest_schedule":false}
<1252 loop_gather_fusion @0>
 positions:
  loop_gather_fusion
 uses:
  fusion.291, operand 1
  fusion.290, operand 1
  fusion.289, operand 2
  fusion.288, operand 4
  loop_add_fusion, operand 3
 from instruction: %loop_gather_fusion = bf16[64,1,1024]{2,0,1} fusion(bf16[151936,1024]{1,0} %p, s32[64]{0} %p.1), kind=kLoop, calls=%fused_gather, metadata={op_type="aten__index_select" op_name="aten__index_select" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1253 fusion.291 @0>
 positions:
  fusion.291
 uses:
  custom-call.47.0, operand 0
 from instruction: %fusion.291 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.26), kind=kCustom, calls=%fused_computation.240, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1254 custom-call.47.0{} @0>
 positions:
  custom-call.47.0 {}
 uses:
  get-tuple-element.47, operand 0 {}
 from instruction: %custom-call.47.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.291, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1255 custom-call.47.0{0} @0>
 positions:
  custom-call.47.0 {0}
  get-tuple-element.47
 uses:
  loop_convert_fusion.22, operand 0
 from instruction: %custom-call.47.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.291, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1256 custom-call.47.0{1} @0>
 positions:
  custom-call.47.0 {1}
 uses:
 from instruction: %custom-call.47.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.291, bf16[6144,1024]{1,0} %p.27), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1257 loop_convert_fusion.22 @0>
 positions:
  loop_convert_fusion.22
 uses:
  custom-call.48.0, operand 0
 from instruction: %loop_convert_fusion.22 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.47), kind=kLoop, calls=%fused_convert.22, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1258 custom-call.48.0{} @0>
 positions:
  custom-call.48.0 {}
 uses:
  get-tuple-element.1.0, operand 0 {}
 from instruction: %custom-call.48.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1259 custom-call.48.0{0} @0>
 positions:
  custom-call.48.0 {0}
  get-tuple-element.1.0
 uses:
  fusion.290, operand 3
  fusion.289, operand 4
  fusion.288, operand 5
  loop_add_fusion, operand 4
 from instruction: %custom-call.48.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1260 custom-call.48.0{1} @0>
 positions:
  custom-call.48.0 {1}
 uses:
 from instruction: %custom-call.48.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.22, bf16[1024,3072]{1,0} %p.28), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1261 fusion.290 @0>
 positions:
  fusion.290
 uses:
  custom-call.49.0, operand 0
 from instruction: %fusion.290 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[64,1024]{1,0} %get-tuple-element.1.0, bf16[1024]{0} %p.29), kind=kCustom, calls=%fused_computation.239, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1262 custom-call.49.0{} @0>
 positions:
  custom-call.49.0 {}
 uses:
  get-tuple-element.2.0, operand 0 {}
 from instruction: %custom-call.49.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.290, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1263 custom-call.49.0{0} @0>
 positions:
  custom-call.49.0 {0}
  get-tuple-element.2.0
 uses:
  loop_convert_fusion.21, operand 0
 from instruction: %custom-call.49.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.290, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1264 custom-call.49.0{1} @0>
 positions:
  custom-call.49.0 {1}
 uses:
 from instruction: %custom-call.49.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.290, bf16[6144,1024]{1,0} %p.30), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1265 loop_convert_fusion.21 @0>
 positions:
  loop_convert_fusion.21
 uses:
  custom-call.50.0, operand 0
 from instruction: %loop_convert_fusion.21 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.2.0), kind=kLoop, calls=%fused_convert.21, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1266 custom-call.50.0{} @0>
 positions:
  custom-call.50.0 {}
 uses:
  get-tuple-element.3.0, operand 0 {}
 from instruction: %custom-call.50.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1267 custom-call.50.0{0} @0>
 positions:
  custom-call.50.0 {0}
  get-tuple-element.3.0
 uses:
  fusion.289, operand 5
  fusion.288, operand 6
  loop_add_fusion, operand 5
 from instruction: %custom-call.50.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1268 custom-call.50.0{1} @0>
 positions:
  custom-call.50.0 {1}
 uses:
 from instruction: %custom-call.50.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.21, bf16[1024,3072]{1,0} %p.31), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1269 fusion.289 @0>
 positions:
  fusion.289
 uses:
  custom-call.51.0, operand 0
 from instruction: %fusion.289 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[1024]{0} %p.32, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[64,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.238, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1270 custom-call.51.0{} @0>
 positions:
  custom-call.51.0 {}
 uses:
  get-tuple-element.4.0, operand 0 {}
 from instruction: %custom-call.51.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.289, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1271 custom-call.51.0{0} @0>
 positions:
  custom-call.51.0 {0}
  get-tuple-element.4.0
 uses:
  loop_convert_fusion.20, operand 0
 from instruction: %custom-call.51.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.289, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1272 custom-call.51.0{1} @0>
 positions:
  custom-call.51.0 {1}
 uses:
 from instruction: %custom-call.51.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.289, bf16[6144,1024]{1,0} %p.33), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1273 loop_convert_fusion.20 @0>
 positions:
  loop_convert_fusion.20
 uses:
  custom-call.52.0, operand 0
 from instruction: %loop_convert_fusion.20 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.4.0), kind=kLoop, calls=%fused_convert.20, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1274 custom-call.52.0{} @0>
 positions:
  custom-call.52.0 {}
 uses:
  get-tuple-element.5.0, operand 0 {}
 from instruction: %custom-call.52.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1275 custom-call.52.0{0} @0>
 positions:
  custom-call.52.0 {0}
  get-tuple-element.5.0
 uses:
  fusion.288, operand 1
  loop_add_fusion, operand 2
 from instruction: %custom-call.52.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1276 custom-call.52.0{1} @0>
 positions:
  custom-call.52.0 {1}
 uses:
 from instruction: %custom-call.52.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.20, bf16[1024,3072]{1,0} %p.34), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1277 fusion.288 @0>
 positions:
  fusion.288
 uses:
  custom-call.53.0, operand 0
 from instruction: %fusion.288 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[64,1024]{1,0} %get-tuple-element.5.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.35, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.1.0, bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kCustom, calls=%fused_computation.237, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1278 custom-call.53.0{} @0>
 positions:
  custom-call.53.0 {}
 uses:
  get-tuple-element.6.0, operand 0 {}
 from instruction: %custom-call.53.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.288, bf16[6144,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1279 custom-call.53.0{0} @0>
 positions:
  custom-call.53.0 {0}
  get-tuple-element.6.0
 uses:
  loop_convert_fusion.19, operand 0
 from instruction: %custom-call.53.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.288, bf16[6144,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1280 custom-call.53.0{1} @0>
 positions:
  custom-call.53.0 {1}
 uses:
 from instruction: %custom-call.53.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.288, bf16[6144,1024]{1,0} %p.36), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1281 loop_convert_fusion.19 @0>
 positions:
  loop_convert_fusion.19
 uses:
  custom-call.54.0, operand 0
 from instruction: %loop_convert_fusion.19 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.6.0), kind=kLoop, calls=%fused_convert.19, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1282 custom-call.54.0{} @0>
 positions:
  custom-call.54.0 {}
 uses:
  get-tuple-element.7.0, operand 0 {}
 from instruction: %custom-call.54.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1283 custom-call.54.0{0} @0>
 positions:
  custom-call.54.0 {0}
  get-tuple-element.7.0
 uses:
  loop_add_fusion, operand 1
 from instruction: %custom-call.54.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1284 custom-call.54.0{1} @0>
 positions:
  custom-call.54.0 {1}
 uses:
 from instruction: %custom-call.54.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.19, bf16[1024,3072]{1,0} %p.37), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1285 loop_add_fusion @0>
 positions:
  loop_add_fusion
 uses:
  fusion.287, operand 0
  fusion.286, operand 1
  fusion.285, operand 2
  fusion.284, operand 4
  loop_add_fusion.1, operand 3
 from instruction: %loop_add_fusion = f32[64,1024]{1,0} fusion(bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[64,1024]{1,0} %get-tuple-element.7.0, bf16[64,1024]{1,0} %get-tuple-element.5.0, bf16[64,1,1024]{2,0,1} %loop_gather_fusion, bf16[64,1024]{1,0} %get-tuple-element.1.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.3.0), kind=kLoop, calls=%fused_add, metadata={op_type="aten__add" op_name="aten__add.1099/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1286 fusion.287 @0>
 positions:
  fusion.287
 uses:
  custom-call.55.0, operand 0
 from instruction: %fusion.287 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion, f32[] %p.25, bf16[1024]{0} %p.38), kind=kCustom, calls=%fused_computation.236, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.271"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1287 custom-call.55.0{} @0>
 positions:
  custom-call.55.0 {}
 uses:
  get-tuple-element.8.0, operand 0 {}
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.287, bf16[6144,1024]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1288 custom-call.55.0{0} @0>
 positions:
  custom-call.55.0 {0}
  get-tuple-element.8.0
 uses:
  loop_convert_fusion.18, operand 0
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.287, bf16[6144,1024]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1289 custom-call.55.0{1} @0>
 positions:
  custom-call.55.0 {1}
 uses:
 from instruction: %custom-call.55.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.287, bf16[6144,1024]{1,0} %p.39), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1290 loop_convert_fusion.18 @0>
 positions:
  loop_convert_fusion.18
 uses:
  custom-call.56.0, operand 0
 from instruction: %loop_convert_fusion.18 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.8.0), kind=kLoop, calls=%fused_convert.18, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1291 custom-call.56.0{} @0>
 positions:
  custom-call.56.0 {}
 uses:
  get-tuple-element.9.0, operand 0 {}
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1292 custom-call.56.0{0} @0>
 positions:
  custom-call.56.0 {0}
  get-tuple-element.9.0
 uses:
  fusion.286, operand 2
  fusion.285, operand 3
  fusion.284, operand 5
  loop_add_fusion.1, operand 4
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1293 custom-call.56.0{1} @0>
 positions:
  custom-call.56.0 {1}
 uses:
 from instruction: %custom-call.56.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.18, bf16[1024,3072]{1,0} %p.40), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1294 fusion.286 @0>
 positions:
  fusion.286
 uses:
  custom-call.57.0, operand 0
 from instruction: %fusion.286 = bf16[64,1024]{1,0} fusion(f32[] %p.25, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.41), kind=kCustom, calls=%fused_computation.235, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1295 custom-call.57.0{} @0>
 positions:
  custom-call.57.0 {}
 uses:
  get-tuple-element.10.0, operand 0 {}
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.286, bf16[6144,1024]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1296 custom-call.57.0{0} @0>
 positions:
  custom-call.57.0 {0}
  get-tuple-element.10.0
 uses:
  loop_convert_fusion.17, operand 0
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.286, bf16[6144,1024]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1297 custom-call.57.0{1} @0>
 positions:
  custom-call.57.0 {1}
 uses:
 from instruction: %custom-call.57.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.286, bf16[6144,1024]{1,0} %p.42), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1298 loop_convert_fusion.17 @0>
 positions:
  loop_convert_fusion.17
 uses:
  custom-call.58.0, operand 0
 from instruction: %loop_convert_fusion.17 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.10.0), kind=kLoop, calls=%fused_convert.17, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1299 custom-call.58.0{} @0>
 positions:
  custom-call.58.0 {}
 uses:
  get-tuple-element.11.0, operand 0 {}
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1300 custom-call.58.0{0} @0>
 positions:
  custom-call.58.0 {0}
  get-tuple-element.11.0
 uses:
  fusion.285, operand 5
  fusion.284, operand 6
  loop_add_fusion.1, operand 5
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1301 custom-call.58.0{1} @0>
 positions:
  custom-call.58.0 {1}
 uses:
 from instruction: %custom-call.58.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.17, bf16[1024,3072]{1,0} %p.43), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1302 fusion.285 @0>
 positions:
  fusion.285
 uses:
  custom-call.59.0, operand 0
 from instruction: %fusion.285 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[1024]{0} %p.44, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.234, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1303 custom-call.59.0{} @0>
 positions:
  custom-call.59.0 {}
 uses:
  get-tuple-element.12.0, operand 0 {}
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.285, bf16[6144,1024]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1304 custom-call.59.0{0} @0>
 positions:
  custom-call.59.0 {0}
  get-tuple-element.12.0
 uses:
  loop_convert_fusion.16, operand 0
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.285, bf16[6144,1024]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1305 custom-call.59.0{1} @0>
 positions:
  custom-call.59.0 {1}
 uses:
 from instruction: %custom-call.59.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.285, bf16[6144,1024]{1,0} %p.45), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1306 loop_convert_fusion.16 @0>
 positions:
  loop_convert_fusion.16
 uses:
  custom-call.60.0, operand 0
 from instruction: %loop_convert_fusion.16 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.12.0), kind=kLoop, calls=%fused_convert.16, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1307 custom-call.60.0{} @0>
 positions:
  custom-call.60.0 {}
 uses:
  get-tuple-element.13.0, operand 0 {}
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1308 custom-call.60.0{0} @0>
 positions:
  custom-call.60.0 {0}
  get-tuple-element.13.0
 uses:
  fusion.284, operand 1
  loop_add_fusion.1, operand 2
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1309 custom-call.60.0{1} @0>
 positions:
  custom-call.60.0 {1}
 uses:
 from instruction: %custom-call.60.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.16, bf16[1024,3072]{1,0} %p.46), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1310 fusion.284 @0>
 positions:
  fusion.284
 uses:
  custom-call.61.0, operand 0
 from instruction: %fusion.284 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[64,1024]{1,0} %get-tuple-element.13.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.47, f32[64,1024]{1,0} %loop_add_fusion, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.9.0, bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kCustom, calls=%fused_computation.233, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1311 custom-call.61.0{} @0>
 positions:
  custom-call.61.0 {}
 uses:
  get-tuple-element.14.0, operand 0 {}
 from instruction: %custom-call.61.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.284, bf16[6144,1024]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1312 custom-call.61.0{0} @0>
 positions:
  custom-call.61.0 {0}
  get-tuple-element.14.0
 uses:
  loop_convert_fusion.15, operand 0
 from instruction: %custom-call.61.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.284, bf16[6144,1024]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1313 custom-call.61.0{1} @0>
 positions:
  custom-call.61.0 {1}
 uses:
 from instruction: %custom-call.61.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.284, bf16[6144,1024]{1,0} %p.48), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1314 loop_convert_fusion.15 @0>
 positions:
  loop_convert_fusion.15
 uses:
  custom-call.62.0, operand 0
 from instruction: %loop_convert_fusion.15 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.14.0), kind=kLoop, calls=%fused_convert.15, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1315 custom-call.62.0{} @0>
 positions:
  custom-call.62.0 {}
 uses:
  get-tuple-element.15.0, operand 0 {}
 from instruction: %custom-call.62.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1316 custom-call.62.0{0} @0>
 positions:
  custom-call.62.0 {0}
  get-tuple-element.15.0
 uses:
  loop_add_fusion.1, operand 1
 from instruction: %custom-call.62.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1317 custom-call.62.0{1} @0>
 positions:
  custom-call.62.0 {1}
 uses:
 from instruction: %custom-call.62.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.15, bf16[1024,3072]{1,0} %p.49), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1318 loop_add_fusion.1 @0>
 positions:
  loop_add_fusion.1
 uses:
  fusion.283, operand 0
  fusion.282, operand 1
  fusion.281, operand 2
  fusion.280, operand 4
  loop_add_fusion.2, operand 3
 from instruction: %loop_add_fusion.1 = f32[64,1024]{1,0} fusion(bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[64,1024]{1,0} %get-tuple-element.15.0, bf16[64,1024]{1,0} %get-tuple-element.13.0, f32[64,1024]{1,0} %loop_add_fusion, bf16[64,1024]{1,0} %get-tuple-element.9.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.11.0), kind=kLoop, calls=%fused_add.1, metadata={op_type="aten__add" op_name="aten__add.1171/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1319 fusion.283 @0>
 positions:
  fusion.283
 uses:
  custom-call.63.0, operand 0
 from instruction: %fusion.283 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.1, f32[] %p.25, bf16[1024]{0} %p.50), kind=kCustom, calls=%fused_computation.232, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.271"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1320 custom-call.63.0{} @0>
 positions:
  custom-call.63.0 {}
 uses:
  get-tuple-element.16.0, operand 0 {}
 from instruction: %custom-call.63.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.283, bf16[6144,1024]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1321 custom-call.63.0{0} @0>
 positions:
  custom-call.63.0 {0}
  get-tuple-element.16.0
 uses:
  loop_convert_fusion.14, operand 0
 from instruction: %custom-call.63.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.283, bf16[6144,1024]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1322 custom-call.63.0{1} @0>
 positions:
  custom-call.63.0 {1}
 uses:
 from instruction: %custom-call.63.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.283, bf16[6144,1024]{1,0} %p.51), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1323 loop_convert_fusion.14 @0>
 positions:
  loop_convert_fusion.14
 uses:
  custom-call.64.0, operand 0
 from instruction: %loop_convert_fusion.14 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.16.0), kind=kLoop, calls=%fused_convert.14, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1324 custom-call.64.0{} @0>
 positions:
  custom-call.64.0 {}
 uses:
  get-tuple-element.17.0, operand 0 {}
 from instruction: %custom-call.64.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1325 custom-call.64.0{0} @0>
 positions:
  custom-call.64.0 {0}
  get-tuple-element.17.0
 uses:
  fusion.282, operand 2
  fusion.281, operand 3
  fusion.280, operand 5
  loop_add_fusion.2, operand 4
 from instruction: %custom-call.64.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1326 custom-call.64.0{1} @0>
 positions:
  custom-call.64.0 {1}
 uses:
 from instruction: %custom-call.64.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.14, bf16[1024,3072]{1,0} %p.52), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1327 fusion.282 @0>
 positions:
  fusion.282
 uses:
  custom-call.65.0, operand 0
 from instruction: %fusion.282 = bf16[64,1024]{1,0} fusion(f32[] %p.25, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.53), kind=kCustom, calls=%fused_computation.231, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1328 custom-call.65.0{} @0>
 positions:
  custom-call.65.0 {}
 uses:
  get-tuple-element.18.0, operand 0 {}
 from instruction: %custom-call.65.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.282, bf16[6144,1024]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1329 custom-call.65.0{0} @0>
 positions:
  custom-call.65.0 {0}
  get-tuple-element.18.0
 uses:
  loop_convert_fusion.13, operand 0
 from instruction: %custom-call.65.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.282, bf16[6144,1024]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1330 custom-call.65.0{1} @0>
 positions:
  custom-call.65.0 {1}
 uses:
 from instruction: %custom-call.65.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.282, bf16[6144,1024]{1,0} %p.54), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1331 loop_convert_fusion.13 @0>
 positions:
  loop_convert_fusion.13
 uses:
  custom-call.66.0, operand 0
 from instruction: %loop_convert_fusion.13 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.18.0), kind=kLoop, calls=%fused_convert.13, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1332 custom-call.66.0{} @0>
 positions:
  custom-call.66.0 {}
 uses:
  get-tuple-element.19.0, operand 0 {}
 from instruction: %custom-call.66.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1333 custom-call.66.0{0} @0>
 positions:
  custom-call.66.0 {0}
  get-tuple-element.19.0
 uses:
  fusion.281, operand 5
  fusion.280, operand 6
  loop_add_fusion.2, operand 5
 from instruction: %custom-call.66.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1334 custom-call.66.0{1} @0>
 positions:
  custom-call.66.0 {1}
 uses:
 from instruction: %custom-call.66.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.13, bf16[1024,3072]{1,0} %p.55), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1335 fusion.281 @0>
 positions:
  fusion.281
 uses:
  custom-call.67.0, operand 0
 from instruction: %fusion.281 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[1024]{0} %p.56, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kCustom, calls=%fused_computation.230, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1336 custom-call.67.0{} @0>
 positions:
  custom-call.67.0 {}
 uses:
  get-tuple-element.20.0, operand 0 {}
 from instruction: %custom-call.67.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.281, bf16[6144,1024]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1337 custom-call.67.0{0} @0>
 positions:
  custom-call.67.0 {0}
  get-tuple-element.20.0
 uses:
  loop_convert_fusion.12, operand 0
 from instruction: %custom-call.67.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.281, bf16[6144,1024]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1338 custom-call.67.0{1} @0>
 positions:
  custom-call.67.0 {1}
 uses:
 from instruction: %custom-call.67.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.281, bf16[6144,1024]{1,0} %p.57), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1339 loop_convert_fusion.12 @0>
 positions:
  loop_convert_fusion.12
 uses:
  custom-call.68.0, operand 0
 from instruction: %loop_convert_fusion.12 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.20.0), kind=kLoop, calls=%fused_convert.12, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1340 custom-call.68.0{} @0>
 positions:
  custom-call.68.0 {}
 uses:
  get-tuple-element.21.0, operand 0 {}
 from instruction: %custom-call.68.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1341 custom-call.68.0{0} @0>
 positions:
  custom-call.68.0 {0}
  get-tuple-element.21.0
 uses:
  fusion.280, operand 1
  loop_add_fusion.2, operand 2
 from instruction: %custom-call.68.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1342 custom-call.68.0{1} @0>
 positions:
  custom-call.68.0 {1}
 uses:
 from instruction: %custom-call.68.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.12, bf16[1024,3072]{1,0} %p.58), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1343 fusion.280 @0>
 positions:
  fusion.280
 uses:
  custom-call.69.0, operand 0
 from instruction: %fusion.280 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[64,1024]{1,0} %get-tuple-element.21.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.59, f32[64,1024]{1,0} %loop_add_fusion.1, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.17.0, bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kCustom, calls=%fused_computation.229, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1344 custom-call.69.0{} @0>
 positions:
  custom-call.69.0 {}
 uses:
  get-tuple-element.22.0, operand 0 {}
 from instruction: %custom-call.69.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.280, bf16[6144,1024]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1345 custom-call.69.0{0} @0>
 positions:
  custom-call.69.0 {0}
  get-tuple-element.22.0
 uses:
  loop_convert_fusion.11, operand 0
 from instruction: %custom-call.69.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.280, bf16[6144,1024]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1346 custom-call.69.0{1} @0>
 positions:
  custom-call.69.0 {1}
 uses:
 from instruction: %custom-call.69.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.280, bf16[6144,1024]{1,0} %p.60), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1347 loop_convert_fusion.11 @0>
 positions:
  loop_convert_fusion.11
 uses:
  custom-call.70.0, operand 0
 from instruction: %loop_convert_fusion.11 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.22.0), kind=kLoop, calls=%fused_convert.11, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1348 custom-call.70.0{} @0>
 positions:
  custom-call.70.0 {}
 uses:
  get-tuple-element.23.0, operand 0 {}
 from instruction: %custom-call.70.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1349 custom-call.70.0{0} @0>
 positions:
  custom-call.70.0 {0}
  get-tuple-element.23.0
 uses:
  loop_add_fusion.2, operand 1
 from instruction: %custom-call.70.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1350 custom-call.70.0{1} @0>
 positions:
  custom-call.70.0 {1}
 uses:
 from instruction: %custom-call.70.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.11, bf16[1024,3072]{1,0} %p.61), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1351 loop_add_fusion.2 @0>
 positions:
  loop_add_fusion.2
 uses:
  fusion.279, operand 0
  fusion.278, operand 1
  fusion.277, operand 2
  fusion.276, operand 4
  loop_add_fusion.3, operand 3
 from instruction: %loop_add_fusion.2 = f32[64,1024]{1,0} fusion(bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[64,1024]{1,0} %get-tuple-element.23.0, bf16[64,1024]{1,0} %get-tuple-element.21.0, f32[64,1024]{1,0} %loop_add_fusion.1, bf16[64,1024]{1,0} %get-tuple-element.17.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.19.0), kind=kLoop, calls=%fused_add.2, metadata={op_type="aten__add" op_name="aten__add.1243/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1352 fusion.279 @0>
 positions:
  fusion.279
 uses:
  custom-call.71.0, operand 0
 from instruction: %fusion.279 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.2, f32[] %p.25, bf16[1024]{0} %p.62), kind=kCustom, calls=%fused_computation.228, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.271"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1353 custom-call.71.0{} @0>
 positions:
  custom-call.71.0 {}
 uses:
  get-tuple-element.24.0, operand 0 {}
 from instruction: %custom-call.71.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.279, bf16[6144,1024]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1354 custom-call.71.0{0} @0>
 positions:
  custom-call.71.0 {0}
  get-tuple-element.24.0
 uses:
  loop_convert_fusion.10, operand 0
 from instruction: %custom-call.71.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.279, bf16[6144,1024]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1355 custom-call.71.0{1} @0>
 positions:
  custom-call.71.0 {1}
 uses:
 from instruction: %custom-call.71.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.279, bf16[6144,1024]{1,0} %p.63), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1356 loop_convert_fusion.10 @0>
 positions:
  loop_convert_fusion.10
 uses:
  custom-call.72.0, operand 0
 from instruction: %loop_convert_fusion.10 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.24.0), kind=kLoop, calls=%fused_convert.10, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1357 custom-call.72.0{} @0>
 positions:
  custom-call.72.0 {}
 uses:
  get-tuple-element.25.0, operand 0 {}
 from instruction: %custom-call.72.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1358 custom-call.72.0{0} @0>
 positions:
  custom-call.72.0 {0}
  get-tuple-element.25.0
 uses:
  fusion.278, operand 2
  fusion.277, operand 3
  fusion.276, operand 5
  loop_add_fusion.3, operand 4
 from instruction: %custom-call.72.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1359 custom-call.72.0{1} @0>
 positions:
  custom-call.72.0 {1}
 uses:
 from instruction: %custom-call.72.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.10, bf16[1024,3072]{1,0} %p.64), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1360 fusion.278 @0>
 positions:
  fusion.278
 uses:
  custom-call.73.0, operand 0
 from instruction: %fusion.278 = bf16[64,1024]{1,0} fusion(f32[] %p.25, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.65), kind=kCustom, calls=%fused_computation.227, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1361 custom-call.73.0{} @0>
 positions:
  custom-call.73.0 {}
 uses:
  get-tuple-element.26.0, operand 0 {}
 from instruction: %custom-call.73.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.278, bf16[6144,1024]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1362 custom-call.73.0{0} @0>
 positions:
  custom-call.73.0 {0}
  get-tuple-element.26.0
 uses:
  loop_convert_fusion.9, operand 0
 from instruction: %custom-call.73.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.278, bf16[6144,1024]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1363 custom-call.73.0{1} @0>
 positions:
  custom-call.73.0 {1}
 uses:
 from instruction: %custom-call.73.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.278, bf16[6144,1024]{1,0} %p.66), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1364 loop_convert_fusion.9 @0>
 positions:
  loop_convert_fusion.9
 uses:
  custom-call.74.0, operand 0
 from instruction: %loop_convert_fusion.9 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.26.0), kind=kLoop, calls=%fused_convert.9, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1365 custom-call.74.0{} @0>
 positions:
  custom-call.74.0 {}
 uses:
  get-tuple-element.27.0, operand 0 {}
 from instruction: %custom-call.74.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1366 custom-call.74.0{0} @0>
 positions:
  custom-call.74.0 {0}
  get-tuple-element.27.0
 uses:
  fusion.277, operand 5
  fusion.276, operand 6
  loop_add_fusion.3, operand 5
 from instruction: %custom-call.74.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1367 custom-call.74.0{1} @0>
 positions:
  custom-call.74.0 {1}
 uses:
 from instruction: %custom-call.74.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.9, bf16[1024,3072]{1,0} %p.67), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1368 fusion.277 @0>
 positions:
  fusion.277
 uses:
  custom-call.75.0, operand 0
 from instruction: %fusion.277 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[1024]{0} %p.68, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.27.0), kind=kCustom, calls=%fused_computation.226, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1369 custom-call.75.0{} @0>
 positions:
  custom-call.75.0 {}
 uses:
  get-tuple-element.28.0, operand 0 {}
 from instruction: %custom-call.75.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.277, bf16[6144,1024]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1370 custom-call.75.0{0} @0>
 positions:
  custom-call.75.0 {0}
  get-tuple-element.28.0
 uses:
  loop_convert_fusion.8, operand 0
 from instruction: %custom-call.75.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.277, bf16[6144,1024]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1371 custom-call.75.0{1} @0>
 positions:
  custom-call.75.0 {1}
 uses:
 from instruction: %custom-call.75.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.277, bf16[6144,1024]{1,0} %p.69), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1372 loop_convert_fusion.8 @0>
 positions:
  loop_convert_fusion.8
 uses:
  custom-call.76.0, operand 0
 from instruction: %loop_convert_fusion.8 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.28.0), kind=kLoop, calls=%fused_convert.8, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1373 custom-call.76.0{} @0>
 positions:
  custom-call.76.0 {}
 uses:
  get-tuple-element.29.0, operand 0 {}
 from instruction: %custom-call.76.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1374 custom-call.76.0{0} @0>
 positions:
  custom-call.76.0 {0}
  get-tuple-element.29.0
 uses:
  fusion.276, operand 1
  loop_add_fusion.3, operand 2
 from instruction: %custom-call.76.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1375 custom-call.76.0{1} @0>
 positions:
  custom-call.76.0 {1}
 uses:
 from instruction: %custom-call.76.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.8, bf16[1024,3072]{1,0} %p.70), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1376 fusion.276 @0>
 positions:
  fusion.276
 uses:
  custom-call.77.0, operand 0
 from instruction: %fusion.276 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[64,1024]{1,0} %get-tuple-element.29.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.71, f32[64,1024]{1,0} %loop_add_fusion.2, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.25.0, bf16[64,1024]{1,0} %get-tuple-element.27.0), kind=kCustom, calls=%fused_computation.225, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1377 custom-call.77.0{} @0>
 positions:
  custom-call.77.0 {}
 uses:
  get-tuple-element.30.0, operand 0 {}
 from instruction: %custom-call.77.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.276, bf16[6144,1024]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1378 custom-call.77.0{0} @0>
 positions:
  custom-call.77.0 {0}
  get-tuple-element.30.0
 uses:
  loop_convert_fusion.7, operand 0
 from instruction: %custom-call.77.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.276, bf16[6144,1024]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1379 custom-call.77.0{1} @0>
 positions:
  custom-call.77.0 {1}
 uses:
 from instruction: %custom-call.77.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.276, bf16[6144,1024]{1,0} %p.72), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1380 loop_convert_fusion.7 @0>
 positions:
  loop_convert_fusion.7
 uses:
  custom-call.78.0, operand 0
 from instruction: %loop_convert_fusion.7 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.30.0), kind=kLoop, calls=%fused_convert.7, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1381 custom-call.78.0{} @0>
 positions:
  custom-call.78.0 {}
 uses:
  get-tuple-element.31.0, operand 0 {}
 from instruction: %custom-call.78.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1382 custom-call.78.0{0} @0>
 positions:
  custom-call.78.0 {0}
  get-tuple-element.31.0
 uses:
  loop_add_fusion.3, operand 1
 from instruction: %custom-call.78.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1383 custom-call.78.0{1} @0>
 positions:
  custom-call.78.0 {1}
 uses:
 from instruction: %custom-call.78.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.7, bf16[1024,3072]{1,0} %p.73), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1384 loop_add_fusion.3 @0>
 positions:
  loop_add_fusion.3
 uses:
  fusion.275, operand 0
  fusion.274, operand 1
  fusion.273, operand 2
  fusion.272, operand 4
  loop_add_fusion.4, operand 3
 from instruction: %loop_add_fusion.3 = f32[64,1024]{1,0} fusion(bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[64,1024]{1,0} %get-tuple-element.31.0, bf16[64,1024]{1,0} %get-tuple-element.29.0, f32[64,1024]{1,0} %loop_add_fusion.2, bf16[64,1024]{1,0} %get-tuple-element.25.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.27.0), kind=kLoop, calls=%fused_add.3, metadata={op_type="aten__add" op_name="aten__add.1315/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1385 fusion.275 @0>
 positions:
  fusion.275
 uses:
  custom-call.79.0, operand 0
 from instruction: %fusion.275 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.3, f32[] %p.25, bf16[1024]{0} %p.74), kind=kCustom, calls=%fused_computation.224, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.271"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1386 custom-call.79.0{} @0>
 positions:
  custom-call.79.0 {}
 uses:
  get-tuple-element.32.0, operand 0 {}
 from instruction: %custom-call.79.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.275, bf16[6144,1024]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1387 custom-call.79.0{0} @0>
 positions:
  custom-call.79.0 {0}
  get-tuple-element.32.0
 uses:
  loop_convert_fusion.6, operand 0
 from instruction: %custom-call.79.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.275, bf16[6144,1024]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1388 custom-call.79.0{1} @0>
 positions:
  custom-call.79.0 {1}
 uses:
 from instruction: %custom-call.79.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.275, bf16[6144,1024]{1,0} %p.75), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1389 loop_convert_fusion.6 @0>
 positions:
  loop_convert_fusion.6
 uses:
  custom-call.80.0, operand 0
 from instruction: %loop_convert_fusion.6 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.32.0), kind=kLoop, calls=%fused_convert.6, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1390 custom-call.80.0{} @0>
 positions:
  custom-call.80.0 {}
 uses:
  get-tuple-element.33.0, operand 0 {}
 from instruction: %custom-call.80.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1391 custom-call.80.0{0} @0>
 positions:
  custom-call.80.0 {0}
  get-tuple-element.33.0
 uses:
  fusion.274, operand 2
  fusion.273, operand 3
  fusion.272, operand 5
  loop_add_fusion.4, operand 4
 from instruction: %custom-call.80.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1392 custom-call.80.0{1} @0>
 positions:
  custom-call.80.0 {1}
 uses:
 from instruction: %custom-call.80.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.6, bf16[1024,3072]{1,0} %p.76), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1393 fusion.274 @0>
 positions:
  fusion.274
 uses:
  custom-call.81.0, operand 0
 from instruction: %fusion.274 = bf16[64,1024]{1,0} fusion(f32[] %p.25, f32[64,1024]{1,0} %loop_add_fusion.3, bf16[64,1024]{1,0} %get-tuple-element.33.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.77), kind=kCustom, calls=%fused_computation.223, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1394 custom-call.81.0{} @0>
 positions:
  custom-call.81.0 {}
 uses:
  get-tuple-element.34.0, operand 0 {}
 from instruction: %custom-call.81.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.274, bf16[6144,1024]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1395 custom-call.81.0{0} @0>
 positions:
  custom-call.81.0 {0}
  get-tuple-element.34.0
 uses:
  loop_convert_fusion.5, operand 0
 from instruction: %custom-call.81.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.274, bf16[6144,1024]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1396 custom-call.81.0{1} @0>
 positions:
  custom-call.81.0 {1}
 uses:
 from instruction: %custom-call.81.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.274, bf16[6144,1024]{1,0} %p.78), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1397 loop_convert_fusion.5 @0>
 positions:
  loop_convert_fusion.5
 uses:
  custom-call.82.0, operand 0
 from instruction: %loop_convert_fusion.5 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.34.0), kind=kLoop, calls=%fused_convert.5, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1398 custom-call.82.0{} @0>
 positions:
  custom-call.82.0 {}
 uses:
  get-tuple-element.35.0, operand 0 {}
 from instruction: %custom-call.82.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1399 custom-call.82.0{0} @0>
 positions:
  custom-call.82.0 {0}
  get-tuple-element.35.0
 uses:
  fusion.273, operand 5
  fusion.272, operand 6
  loop_add_fusion.4, operand 5
 from instruction: %custom-call.82.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1400 custom-call.82.0{1} @0>
 positions:
  custom-call.82.0 {1}
 uses:
 from instruction: %custom-call.82.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.5, bf16[1024,3072]{1,0} %p.79), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1401 fusion.273 @0>
 positions:
  fusion.273
 uses:
  custom-call.83.0, operand 0
 from instruction: %fusion.273 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[1024]{0} %p.80, f32[64,1024]{1,0} %loop_add_fusion.3, bf16[64,1024]{1,0} %get-tuple-element.33.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.35.0), kind=kCustom, calls=%fused_computation.222, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1402 custom-call.83.0{} @0>
 positions:
  custom-call.83.0 {}
 uses:
  get-tuple-element.36.0, operand 0 {}
 from instruction: %custom-call.83.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.273, bf16[6144,1024]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1403 custom-call.83.0{0} @0>
 positions:
  custom-call.83.0 {0}
  get-tuple-element.36.0
 uses:
  loop_convert_fusion.4, operand 0
 from instruction: %custom-call.83.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.273, bf16[6144,1024]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1404 custom-call.83.0{1} @0>
 positions:
  custom-call.83.0 {1}
 uses:
 from instruction: %custom-call.83.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.273, bf16[6144,1024]{1,0} %p.81), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1405 loop_convert_fusion.4 @0>
 positions:
  loop_convert_fusion.4
 uses:
  custom-call.84.0, operand 0
 from instruction: %loop_convert_fusion.4 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.36.0), kind=kLoop, calls=%fused_convert.4, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1406 custom-call.84.0{} @0>
 positions:
  custom-call.84.0 {}
 uses:
  get-tuple-element.37.0, operand 0 {}
 from instruction: %custom-call.84.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1407 custom-call.84.0{0} @0>
 positions:
  custom-call.84.0 {0}
  get-tuple-element.37.0
 uses:
  fusion.272, operand 1
  loop_add_fusion.4, operand 2
 from instruction: %custom-call.84.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1408 custom-call.84.0{1} @0>
 positions:
  custom-call.84.0 {1}
 uses:
 from instruction: %custom-call.84.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.4, bf16[1024,3072]{1,0} %p.82), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1409 fusion.272 @0>
 positions:
  fusion.272
 uses:
  custom-call.85.0, operand 0
 from instruction: %fusion.272 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[64,1024]{1,0} %get-tuple-element.37.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.83, f32[64,1024]{1,0} %loop_add_fusion.3, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.33.0, bf16[64,1024]{1,0} %get-tuple-element.35.0), kind=kCustom, calls=%fused_computation.221, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1410 custom-call.85.0{} @0>
 positions:
  custom-call.85.0 {}
 uses:
  get-tuple-element.38.0, operand 0 {}
 from instruction: %custom-call.85.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.272, bf16[6144,1024]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1411 custom-call.85.0{0} @0>
 positions:
  custom-call.85.0 {0}
  get-tuple-element.38.0
 uses:
  loop_convert_fusion.3, operand 0
 from instruction: %custom-call.85.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.272, bf16[6144,1024]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1412 custom-call.85.0{1} @0>
 positions:
  custom-call.85.0 {1}
 uses:
 from instruction: %custom-call.85.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.272, bf16[6144,1024]{1,0} %p.84), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1413 loop_convert_fusion.3 @0>
 positions:
  loop_convert_fusion.3
 uses:
  custom-call.86.0, operand 0
 from instruction: %loop_convert_fusion.3 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.38.0), kind=kLoop, calls=%fused_convert.3, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1414 custom-call.86.0{} @0>
 positions:
  custom-call.86.0 {}
 uses:
  get-tuple-element.39.0, operand 0 {}
 from instruction: %custom-call.86.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.85), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1415 custom-call.86.0{0} @0>
 positions:
  custom-call.86.0 {0}
  get-tuple-element.39.0
 uses:
  loop_add_fusion.4, operand 1
 from instruction: %custom-call.86.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.85), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1416 custom-call.86.0{1} @0>
 positions:
  custom-call.86.0 {1}
 uses:
 from instruction: %custom-call.86.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.3, bf16[1024,3072]{1,0} %p.85), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1417 loop_add_fusion.4 @0>
 positions:
  loop_add_fusion.4
 uses:
  fusion.271, operand 0
  fusion.270, operand 1
  fusion.269, operand 2
  fusion.267, operand 3
 from instruction: %loop_add_fusion.4 = f32[64,1024]{1,0} fusion(bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[64,1024]{1,0} %get-tuple-element.39.0, bf16[64,1024]{1,0} %get-tuple-element.37.0, f32[64,1024]{1,0} %loop_add_fusion.3, bf16[64,1024]{1,0} %get-tuple-element.33.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.35.0), kind=kLoop, calls=%fused_add.4, metadata={op_type="aten__add" op_name="aten__add.1387/aten__add" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1418 fusion.271 @0>
 positions:
  fusion.271
 uses:
  custom-call.87.0, operand 0
 from instruction: %fusion.271 = bf16[64,1024]{1,0} fusion(f32[64,1024]{1,0} %loop_add_fusion.4, f32[] %p.25, bf16[1024]{0} %p.86), kind=kCustom, calls=%fused_computation.220, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="fusion.271"}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1419 custom-call.87.0{} @0>
 positions:
  custom-call.87.0 {}
 uses:
  get-tuple-element.40.0, operand 0 {}
 from instruction: %custom-call.87.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.271, bf16[6144,1024]{1,0} %p.87), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1420 custom-call.87.0{0} @0>
 positions:
  custom-call.87.0 {0}
  get-tuple-element.40.0
 uses:
  loop_convert_fusion.2, operand 0
 from instruction: %custom-call.87.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.271, bf16[6144,1024]{1,0} %p.87), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1421 custom-call.87.0{1} @0>
 positions:
  custom-call.87.0 {1}
 uses:
 from instruction: %custom-call.87.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.271, bf16[6144,1024]{1,0} %p.87), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1422 loop_convert_fusion.2 @0>
 positions:
  loop_convert_fusion.2
 uses:
  custom-call.88.0, operand 0
 from instruction: %loop_convert_fusion.2 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.40.0), kind=kLoop, calls=%fused_convert.2, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1423 custom-call.88.0{} @0>
 positions:
  custom-call.88.0 {}
 uses:
  get-tuple-element.41.0, operand 0 {}
 from instruction: %custom-call.88.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.88), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1424 custom-call.88.0{0} @0>
 positions:
  custom-call.88.0 {0}
  get-tuple-element.41.0
 uses:
  fusion.270, operand 2
  fusion.269, operand 3
  fusion.267, operand 4
 from instruction: %custom-call.88.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.88), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1425 custom-call.88.0{1} @0>
 positions:
  custom-call.88.0 {1}
 uses:
 from instruction: %custom-call.88.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.2, bf16[1024,3072]{1,0} %p.88), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1426 fusion.270 @0>
 positions:
  fusion.270
 uses:
  custom-call.89.0, operand 0
 from instruction: %fusion.270 = bf16[64,1024]{1,0} fusion(f32[] %p.25, f32[64,1024]{1,0} %loop_add_fusion.4, bf16[64,1024]{1,0} %get-tuple-element.41.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[1024]{0} %p.89), kind=kCustom, calls=%fused_computation.219, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1427 custom-call.89.0{} @0>
 positions:
  custom-call.89.0 {}
 uses:
  get-tuple-element.42.0, operand 0 {}
 from instruction: %custom-call.89.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.270, bf16[6144,1024]{1,0} %p.90), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1428 custom-call.89.0{0} @0>
 positions:
  custom-call.89.0 {0}
  get-tuple-element.42.0
 uses:
  loop_convert_fusion.1, operand 0
 from instruction: %custom-call.89.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.270, bf16[6144,1024]{1,0} %p.90), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1429 custom-call.89.0{1} @0>
 positions:
  custom-call.89.0 {1}
 uses:
 from instruction: %custom-call.89.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.270, bf16[6144,1024]{1,0} %p.90), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1430 loop_convert_fusion.1 @0>
 positions:
  loop_convert_fusion.1
 uses:
  custom-call.90.0, operand 0
 from instruction: %loop_convert_fusion.1 = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.42.0), kind=kLoop, calls=%fused_convert.1, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1431 custom-call.90.0{} @0>
 positions:
  custom-call.90.0 {}
 uses:
  get-tuple-element.43.0, operand 0 {}
 from instruction: %custom-call.90.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.91), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1432 custom-call.90.0{0} @0>
 positions:
  custom-call.90.0 {0}
  get-tuple-element.43.0
 uses:
  fusion.269, operand 5
  fusion.267, operand 6
 from instruction: %custom-call.90.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.91), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1433 custom-call.90.0{1} @0>
 positions:
  custom-call.90.0 {1}
 uses:
 from instruction: %custom-call.90.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion.1, bf16[1024,3072]{1,0} %p.91), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1434 fusion.269 @0>
 positions:
  fusion.269
 uses:
  custom-call.91.0, operand 0
 from instruction: %fusion.269 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[1024]{0} %p.92, f32[64,1024]{1,0} %loop_add_fusion.4, bf16[64,1024]{1,0} %get-tuple-element.41.0, bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, /*index=5*/bf16[64,1024]{1,0} %get-tuple-element.43.0), kind=kCustom, calls=%fused_computation.218, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1435 custom-call.91.0{} @0>
 positions:
  custom-call.91.0 {}
 uses:
  get-tuple-element.44.0, operand 0 {}
 from instruction: %custom-call.91.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.269, bf16[6144,1024]{1,0} %p.93), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1436 custom-call.91.0{0} @0>
 positions:
  custom-call.91.0 {0}
  get-tuple-element.44.0
 uses:
  loop_convert_fusion, operand 0
 from instruction: %custom-call.91.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.269, bf16[6144,1024]{1,0} %p.93), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1437 custom-call.91.0{1} @0>
 positions:
  custom-call.91.0 {1}
 uses:
 from instruction: %custom-call.91.0 = (bf16[64,6144]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.269, bf16[6144,1024]{1,0} %p.93), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"6291456","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1438 loop_convert_fusion @0>
 positions:
  loop_convert_fusion
 uses:
  custom-call.92.0, operand 0
 from instruction: %loop_convert_fusion = bf16[64,3072]{1,0} fusion(bf16[64,6144]{1,0} %get-tuple-element.44.0), kind=kLoop, calls=%fused_convert, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756 deduplicated_name="loop_convert_fusion"}
<1439 custom-call.92.0{} @0>
 positions:
  custom-call.92.0 {}
 uses:
  get-tuple-element.45.0, operand 0 {}
 from instruction: %custom-call.92.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.94), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1440 custom-call.92.0{0} @0>
 positions:
  custom-call.92.0 {0}
  get-tuple-element.45.0
 uses:
  fusion.267, operand 1
 from instruction: %custom-call.92.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.94), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1441 custom-call.92.0{1} @0>
 positions:
  custom-call.92.0 {1}
 uses:
 from instruction: %custom-call.92.0 = (bf16[64,1024]{1,0}, s8[4194304]{0}) custom-call(bf16[64,3072]{1,0} %loop_convert_fusion, bf16[1024,3072]{1,0} %p.94), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"196608","rhs_stride":"3145728","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1442 fusion.267 @0>
 positions:
  fusion.267
 uses:
  custom-call.93.0, operand 0
 from instruction: %fusion.267 = bf16[64,1024]{1,0} fusion(f32[] %p.25, bf16[64,1024]{1,0} %get-tuple-element.45.0, bf16[1024]{0} %p.95, f32[64,1024]{1,0} %loop_add_fusion.4, bf16[64,1024]{1,0} %get-tuple-element.41.0, /*index=5*/bf16[64,23552]{1,0} %gemm_fusion_dot.92.0, bf16[64,1024]{1,0} %get-tuple-element.43.0), kind=kCustom, calls=%fused_computation.216, metadata={op_type="xla__cast" op_name="xla__cast" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","1024"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1443 custom-call.93.0{} @0>
 positions:
  custom-call.93.0 {}
 uses:
  get-tuple-element.46.0, operand 0 {}
 from instruction: %custom-call.93.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.267, bf16[4096,1024]{1,0} %p.96), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1444 custom-call.93.0{0} @0>
 positions:
  custom-call.93.0 {0}
  get-tuple-element.46.0
 uses:
  wrapped_slice, operand 0
  triton_softmax.25.0, operand 1
 from instruction: %custom-call.93.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.267, bf16[4096,1024]{1,0} %p.96), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1445 custom-call.93.0{1} @0>
 positions:
  custom-call.93.0 {1}
 uses:
 from instruction: %custom-call.93.0 = (bf16[64,4096]{1,0}, s8[4194304]{0}) custom-call(bf16[64,1024]{1,0} %fusion.267, bf16[4096,1024]{1,0} %p.96), custom_call_target="__cublas$gemm", metadata={op_type="aten__mm" op_name="aten__mm" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"gemm_backend_config":{"alpha_real":1,"alpha_imag":0,"beta":0,"dot_dimension_numbers":{"lhs_contracting_dimensions":["1"],"rhs_contracting_dimensions":["1"],"lhs_batch_dimensions":[],"rhs_batch_dimensions":[]},"precision_config":{"operand_precision":["DEFAULT","DEFAULT"],"algorithm":"ALG_UNSET"},"epilogue":"DEFAULT","damax_output":false,"lhs_stride":"65536","rhs_stride":"4194304","grad_x":false,"grad_y":false},"force_earliest_schedule":false}
<1446 triton_softmax.25.0 @0>
 positions:
  triton_softmax.25.0
 uses:
  input_concatenate_fusion, operand 0
 from instruction: %triton_softmax.25.0 = f32[64,8,128]{2,1,0} fusion(f32[] %p.25, bf16[64,4096]{1,0} %get-tuple-element.46.0), kind=kCustom, calls=%triton_softmax_computation.25, metadata={op_type="aten__mul" op_name="aten__mul.1433/aten__mul" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}, backend_config={"operation_queue_id":"0","wait_on_operation_queues":[],"fusion_backend_config":{"kind":"__triton","block_level_fusion_config":{"output_tiles":[{"sizes":["1","4","128"]}],"num_warps":"2"}},"force_earliest_schedule":false}
<1447 input_concatenate_fusion @0>
 positions:
  input_concatenate_fusion
  tuple.1846.0 {0}
  call {0}
  get-tuple-element.49
  tuple {0}
 uses:
  tuple, operand 0
  tuple.1846.0, operand 0
 from instruction: %input_concatenate_fusion = bf16[64,8,128]{2,1,0} fusion(f32[64,8,128]{2,1,0} %triton_softmax.25.0, bf16[128]{0} %p.97, bf16[40960,128]{1,0} %p.98, s32[64]{0} %p.99), kind=kInput, calls=%fused_concatenate, metadata={op_type="aten__cat" op_name="aten__cat" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1448 wrapped_slice @0>
 positions:
  wrapped_slice
  tuple.1846.0 {1}
  call {1}
  get-tuple-element.50
  bitcast.4671.0
  tuple {1}
 uses:
  bitcast.4671.0, operand 0
  tuple.1846.0, operand 1
  tuple, operand 1
 from instruction: %wrapped_slice = bf16[64,1024]{1,0} fusion(bf16[64,4096]{1,0} %get-tuple-element.46.0), kind=kLoop, calls=%wrapped_slice_computation, metadata={op_type="aten__split" op_name="aten__split" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1449 loop_slice_fusion @0>
 positions:
  loop_slice_fusion
  tuple.1846.0 {3}
  call {2}
  get-tuple-element.51
  bitcast.4683.0
  tuple {2}
 uses:
  bitcast.4683.0, operand 0
  tuple.1846.0, operand 3
  tuple, operand 2
 from instruction: %loop_slice_fusion = bf16[69353472]{0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.100), kind=kLoop, calls=%fused_slice, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<1450 wrapped_slice.1 @0>
 positions:
  wrapped_slice.1
  tuple.1846.0 {2}
  bitcast.4676.0
  call {3}
  get-tuple-element.52
  tuple {3}
 uses:
  tuple, operand 3
  tuple.1846.0, operand 2
  bitcast.4676.0, operand 0
 from instruction: %wrapped_slice.1 = bf16[1,4233,16,8,128]{4,3,2,1,0} fusion(bf16[2,4233,16,8,128]{4,3,2,1,0} %p.100), kind=kLoop, calls=%wrapped_slice_computation.1, metadata={op_type="aten__as_strided" op_name="aten__as_strided" source_file="/usr/local/lib/python3.10/site-packages/torch/_higher_order_ops/auto_functionalize.py" source_line=54}
<1451 tuple{} @0>
 positions:
  tuple {}
  call {}
 uses:
  get-tuple-element.49, operand 0 {}
  get-tuple-element.50, operand 0 {}
  get-tuple-element.51, operand 0 {}
  get-tuple-element.52, operand 0 {}
 from instruction: %tuple = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[1,4233,16,8,128]{4,3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %input_concatenate_fusion, bf16[64,8,128]{2,1,0} %bitcast.4671.0, bf16[4233,16,8,128]{3,2,1,0} %bitcast.4683.0, bf16[1,4233,16,8,128]{4,3,2,1,0} %wrapped_slice.1)
<1452 p5.60.0 @0>
 positions:
  p5.60.0
  p
 uses:
  call, operand 0
  loop_gather_fusion, operand 0
 from instruction: %p5.60.0 = bf16[151936,1024]{1,0} parameter(5), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1453 p4.58.0 @0>
 positions:
  p4.58.0
  p.1
 uses:
  call, operand 1
  loop_gather_fusion, operand 1
 from instruction: %p4.58.0 = s32[64]{0} parameter(4), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1454 p94.1648.0 @0>
 positions:
  p94.1648.0
  p.2
 uses:
  call, operand 2
  wrapped_concatenate, operand 0
 from instruction: %p94.1648.0 = bf16[1024,2048]{1,0} parameter(94), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1455 p90.1576.0 @0>
 positions:
  p90.1576.0
  p.3
 uses:
  call, operand 3
  wrapped_concatenate, operand 1
 from instruction: %p90.1576.0 = bf16[1024,2048]{1,0} parameter(90), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1456 p86.1504.0 @0>
 positions:
  p86.1504.0
  p.4
 uses:
  call, operand 4
  wrapped_concatenate, operand 2
 from instruction: %p86.1504.0 = bf16[1024,2048]{1,0} parameter(86), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1457 p82.1432.0 @0>
 positions:
  p82.1432.0
  p.5
 uses:
  call, operand 5
  wrapped_concatenate, operand 3
 from instruction: %p82.1432.0 = bf16[1024,2048]{1,0} parameter(82), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1458 p78.1360.0 @0>
 positions:
  p78.1360.0
  p.6
 uses:
  call, operand 6
  wrapped_concatenate, operand 4
 from instruction: %p78.1360.0 = bf16[1024,2048]{1,0} parameter(78), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1459 p74.1288.0 @0>
 positions:
  p74.1288.0
  p.7
 uses:
  call, operand 7
  wrapped_concatenate, operand 5
 from instruction: %p74.1288.0 = bf16[1024,2048]{1,0} parameter(74), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1460 p70.1216.0 @0>
 positions:
  p70.1216.0
  p.8
 uses:
  call, operand 8
  wrapped_concatenate, operand 6
 from instruction: %p70.1216.0 = bf16[1024,2048]{1,0} parameter(70), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1461 p66.1144.0 @0>
 positions:
  p66.1144.0
  p.9
 uses:
  call, operand 9
  wrapped_concatenate, operand 7
 from instruction: %p66.1144.0 = bf16[1024,2048]{1,0} parameter(66), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1462 p62.1072.0 @0>
 positions:
  p62.1072.0
  p.10
 uses:
  call, operand 10
  wrapped_concatenate, operand 8
 from instruction: %p62.1072.0 = bf16[1024,2048]{1,0} parameter(62), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1463 p58.1000.0 @0>
 positions:
  p58.1000.0
  p.11
 uses:
  call, operand 11
  wrapped_concatenate, operand 9
 from instruction: %p58.1000.0 = bf16[1024,2048]{1,0} parameter(58), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1464 p54.928.0 @0>
 positions:
  p54.928.0
  p.12
 uses:
  call, operand 12
  wrapped_concatenate, operand 10
 from instruction: %p54.928.0 = bf16[1024,2048]{1,0} parameter(54), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1465 p50.856.0 @0>
 positions:
  p50.856.0
  p.13
 uses:
  call, operand 13
  wrapped_concatenate, operand 11
 from instruction: %p50.856.0 = bf16[1024,2048]{1,0} parameter(50), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1466 p46.784.0 @0>
 positions:
  p46.784.0
  p.14
 uses:
  call, operand 14
  wrapped_concatenate, operand 12
 from instruction: %p46.784.0 = bf16[1024,2048]{1,0} parameter(46), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1467 p42.712.0 @0>
 positions:
  p42.712.0
  p.15
 uses:
  call, operand 15
  wrapped_concatenate, operand 13
 from instruction: %p42.712.0 = bf16[1024,2048]{1,0} parameter(42), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1468 p38.640.0 @0>
 positions:
  p38.640.0
  p.16
 uses:
  call, operand 16
  wrapped_concatenate, operand 14
 from instruction: %p38.640.0 = bf16[1024,2048]{1,0} parameter(38), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1469 p34.568.0 @0>
 positions:
  p34.568.0
  p.17
 uses:
  call, operand 17
  wrapped_concatenate, operand 15
 from instruction: %p34.568.0 = bf16[1024,2048]{1,0} parameter(34), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1470 p30.496.0 @0>
 positions:
  p30.496.0
  p.18
 uses:
  call, operand 18
  wrapped_concatenate, operand 16
 from instruction: %p30.496.0 = bf16[1024,2048]{1,0} parameter(30), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1471 p26.424.0 @0>
 positions:
  p26.424.0
  p.19
 uses:
  call, operand 19
  wrapped_concatenate, operand 17
 from instruction: %p26.424.0 = bf16[1024,2048]{1,0} parameter(26), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1472 p22.352.0 @0>
 positions:
  p22.352.0
  p.20
 uses:
  call, operand 20
  wrapped_concatenate, operand 18
 from instruction: %p22.352.0 = bf16[1024,2048]{1,0} parameter(22), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1473 p18.280.0 @0>
 positions:
  p18.280.0
  p.21
 uses:
  call, operand 21
  wrapped_concatenate, operand 19
 from instruction: %p18.280.0 = bf16[1024,2048]{1,0} parameter(18), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1474 p14.208.0 @0>
 positions:
  p14.208.0
  p.22
 uses:
  call, operand 22
  wrapped_concatenate, operand 20
 from instruction: %p14.208.0 = bf16[1024,2048]{1,0} parameter(14), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1475 p10.136.0 @0>
 positions:
  p10.136.0
  p.23
 uses:
  call, operand 23
  wrapped_concatenate, operand 21
 from instruction: %p10.136.0 = bf16[1024,2048]{1,0} parameter(10), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1476 p6.64.0 @0>
 positions:
  p6.64.0
  p.24
 uses:
  call, operand 24
  wrapped_concatenate, operand 22
 from instruction: %p6.64.0 = bf16[1024,2048]{1,0} parameter(6), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1477 p1.4.0 @0>
 positions:
  p1.4.0
  p.25
 uses:
  call, operand 25
  fusion.291, operand 0
  fusion.290, operand 0
  fusion.289, operand 0
  fusion.288, operand 0
  fusion.287, operand 1
  fusion.286, operand 0
  fusion.285, operand 0
  fusion.284, operand 0
  fusion.283, operand 1
  fusion.282, operand 0
  fusion.281, operand 0
  fusion.280, operand 0
  fusion.279, operand 1
  fusion.278, operand 0
  fusion.277, operand 0
  fusion.276, operand 0
  fusion.275, operand 1
  fusion.274, operand 0
  fusion.273, operand 0
  fusion.272, operand 0
  fusion.271, operand 1
  fusion.270, operand 0
  fusion.269, operand 0
  fusion.267, operand 0
  triton_softmax.25.0, operand 0
 from instruction: %p1.4.0 = f32[] parameter(1), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch/_ops.py" source_line=756}
<1478 p9.84.0 @0>
 positions:
  p9.84.0
  p.26
 uses:
  call, operand 26
  fusion.291, operand 3
 from instruction: %p9.84.0 = bf16[1024]{0} parameter(9), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1479 p8.82.0 @0>
 positions:
  p8.82.0
  p.27
 uses:
  call, operand 27
  custom-call.47.0, operand 1
 from instruction: %p8.82.0 = bf16[6144,1024]{1,0} parameter(8), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1480 p7.80.0 @0>
 positions:
  p7.80.0
  p.28
 uses:
  call, operand 28
  custom-call.48.0, operand 1
 from instruction: %p7.80.0 = bf16[1024,3072]{1,0} parameter(7), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1481 p13.156.0 @0>
 positions:
  p13.156.0
  p.29
 uses:
  call, operand 29
  fusion.290, operand 4
 from instruction: %p13.156.0 = bf16[1024]{0} parameter(13), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1482 p12.154.0 @0>
 positions:
  p12.154.0
  p.30
 uses:
  call, operand 30
  custom-call.49.0, operand 1
 from instruction: %p12.154.0 = bf16[6144,1024]{1,0} parameter(12), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1483 p11.152.0 @0>
 positions:
  p11.152.0
  p.31
 uses:
  call, operand 31
  custom-call.50.0, operand 1
 from instruction: %p11.152.0 = bf16[1024,3072]{1,0} parameter(11), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1484 p17.228.0 @0>
 positions:
  p17.228.0
  p.32
 uses:
  call, operand 32
  fusion.289, operand 1
 from instruction: %p17.228.0 = bf16[1024]{0} parameter(17), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1485 p16.226.0 @0>
 positions:
  p16.226.0
  p.33
 uses:
  call, operand 33
  custom-call.51.0, operand 1
 from instruction: %p16.226.0 = bf16[6144,1024]{1,0} parameter(16), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1486 p15.224.0 @0>
 positions:
  p15.224.0
  p.34
 uses:
  call, operand 34
  custom-call.52.0, operand 1
 from instruction: %p15.224.0 = bf16[1024,3072]{1,0} parameter(15), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1487 p21.300.0 @0>
 positions:
  p21.300.0
  p.35
 uses:
  call, operand 35
  fusion.288, operand 3
 from instruction: %p21.300.0 = bf16[1024]{0} parameter(21), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1488 p20.298.0 @0>
 positions:
  p20.298.0
  p.36
 uses:
  call, operand 36
  custom-call.53.0, operand 1
 from instruction: %p20.298.0 = bf16[6144,1024]{1,0} parameter(20), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1489 p19.296.0 @0>
 positions:
  p19.296.0
  p.37
 uses:
  call, operand 37
  custom-call.54.0, operand 1
 from instruction: %p19.296.0 = bf16[1024,3072]{1,0} parameter(19), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1490 p25.372.0 @0>
 positions:
  p25.372.0
  p.38
 uses:
  call, operand 38
  fusion.287, operand 2
 from instruction: %p25.372.0 = bf16[1024]{0} parameter(25), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1491 p24.370.0 @0>
 positions:
  p24.370.0
  p.39
 uses:
  call, operand 39
  custom-call.55.0, operand 1
 from instruction: %p24.370.0 = bf16[6144,1024]{1,0} parameter(24), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1492 p23.368.0 @0>
 positions:
  p23.368.0
  p.40
 uses:
  call, operand 40
  custom-call.56.0, operand 1
 from instruction: %p23.368.0 = bf16[1024,3072]{1,0} parameter(23), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1493 p29.444.0 @0>
 positions:
  p29.444.0
  p.41
 uses:
  call, operand 41
  fusion.286, operand 4
 from instruction: %p29.444.0 = bf16[1024]{0} parameter(29), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1494 p28.442.0 @0>
 positions:
  p28.442.0
  p.42
 uses:
  call, operand 42
  custom-call.57.0, operand 1
 from instruction: %p28.442.0 = bf16[6144,1024]{1,0} parameter(28), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1495 p27.440.0 @0>
 positions:
  p27.440.0
  p.43
 uses:
  call, operand 43
  custom-call.58.0, operand 1
 from instruction: %p27.440.0 = bf16[1024,3072]{1,0} parameter(27), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1496 p33.516.0 @0>
 positions:
  p33.516.0
  p.44
 uses:
  call, operand 44
  fusion.285, operand 1
 from instruction: %p33.516.0 = bf16[1024]{0} parameter(33), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1497 p32.514.0 @0>
 positions:
  p32.514.0
  p.45
 uses:
  call, operand 45
  custom-call.59.0, operand 1
 from instruction: %p32.514.0 = bf16[6144,1024]{1,0} parameter(32), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1498 p31.512.0 @0>
 positions:
  p31.512.0
  p.46
 uses:
  call, operand 46
  custom-call.60.0, operand 1
 from instruction: %p31.512.0 = bf16[1024,3072]{1,0} parameter(31), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1499 p37.588.0 @0>
 positions:
  p37.588.0
  p.47
 uses:
  call, operand 47
  fusion.284, operand 3
 from instruction: %p37.588.0 = bf16[1024]{0} parameter(37), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1500 p36.586.0 @0>
 positions:
  p36.586.0
  p.48
 uses:
  call, operand 48
  custom-call.61.0, operand 1
 from instruction: %p36.586.0 = bf16[6144,1024]{1,0} parameter(36), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1501 p35.584.0 @0>
 positions:
  p35.584.0
  p.49
 uses:
  call, operand 49
  custom-call.62.0, operand 1
 from instruction: %p35.584.0 = bf16[1024,3072]{1,0} parameter(35), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1502 p41.660.0 @0>
 positions:
  p41.660.0
  p.50
 uses:
  call, operand 50
  fusion.283, operand 2
 from instruction: %p41.660.0 = bf16[1024]{0} parameter(41), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1503 p40.658.0 @0>
 positions:
  p40.658.0
  p.51
 uses:
  call, operand 51
  custom-call.63.0, operand 1
 from instruction: %p40.658.0 = bf16[6144,1024]{1,0} parameter(40), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1504 p39.656.0 @0>
 positions:
  p39.656.0
  p.52
 uses:
  call, operand 52
  custom-call.64.0, operand 1
 from instruction: %p39.656.0 = bf16[1024,3072]{1,0} parameter(39), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1505 p45.732.0 @0>
 positions:
  p45.732.0
  p.53
 uses:
  call, operand 53
  fusion.282, operand 4
 from instruction: %p45.732.0 = bf16[1024]{0} parameter(45), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1506 p44.730.0 @0>
 positions:
  p44.730.0
  p.54
 uses:
  call, operand 54
  custom-call.65.0, operand 1
 from instruction: %p44.730.0 = bf16[6144,1024]{1,0} parameter(44), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1507 p43.728.0 @0>
 positions:
  p43.728.0
  p.55
 uses:
  call, operand 55
  custom-call.66.0, operand 1
 from instruction: %p43.728.0 = bf16[1024,3072]{1,0} parameter(43), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1508 p49.804.0 @0>
 positions:
  p49.804.0
  p.56
 uses:
  call, operand 56
  fusion.281, operand 1
 from instruction: %p49.804.0 = bf16[1024]{0} parameter(49), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1509 p48.802.0 @0>
 positions:
  p48.802.0
  p.57
 uses:
  call, operand 57
  custom-call.67.0, operand 1
 from instruction: %p48.802.0 = bf16[6144,1024]{1,0} parameter(48), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1510 p47.800.0 @0>
 positions:
  p47.800.0
  p.58
 uses:
  call, operand 58
  custom-call.68.0, operand 1
 from instruction: %p47.800.0 = bf16[1024,3072]{1,0} parameter(47), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1511 p53.876.0 @0>
 positions:
  p53.876.0
  p.59
 uses:
  call, operand 59
  fusion.280, operand 3
 from instruction: %p53.876.0 = bf16[1024]{0} parameter(53), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1512 p52.874.0 @0>
 positions:
  p52.874.0
  p.60
 uses:
  call, operand 60
  custom-call.69.0, operand 1
 from instruction: %p52.874.0 = bf16[6144,1024]{1,0} parameter(52), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1513 p51.872.0 @0>
 positions:
  p51.872.0
  p.61
 uses:
  call, operand 61
  custom-call.70.0, operand 1
 from instruction: %p51.872.0 = bf16[1024,3072]{1,0} parameter(51), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1514 p57.948.0 @0>
 positions:
  p57.948.0
  p.62
 uses:
  call, operand 62
  fusion.279, operand 2
 from instruction: %p57.948.0 = bf16[1024]{0} parameter(57), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1515 p56.946.0 @0>
 positions:
  p56.946.0
  p.63
 uses:
  call, operand 63
  custom-call.71.0, operand 1
 from instruction: %p56.946.0 = bf16[6144,1024]{1,0} parameter(56), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1516 p55.944.0 @0>
 positions:
  p55.944.0
  p.64
 uses:
  call, operand 64
  custom-call.72.0, operand 1
 from instruction: %p55.944.0 = bf16[1024,3072]{1,0} parameter(55), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1517 p61.1020.0 @0>
 positions:
  p61.1020.0
  p.65
 uses:
  call, operand 65
  fusion.278, operand 4
 from instruction: %p61.1020.0 = bf16[1024]{0} parameter(61), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1518 p60.1018.0 @0>
 positions:
  p60.1018.0
  p.66
 uses:
  call, operand 66
  custom-call.73.0, operand 1
 from instruction: %p60.1018.0 = bf16[6144,1024]{1,0} parameter(60), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1519 p59.1016.0 @0>
 positions:
  p59.1016.0
  p.67
 uses:
  call, operand 67
  custom-call.74.0, operand 1
 from instruction: %p59.1016.0 = bf16[1024,3072]{1,0} parameter(59), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1520 p65.1092.0 @0>
 positions:
  p65.1092.0
  p.68
 uses:
  call, operand 68
  fusion.277, operand 1
 from instruction: %p65.1092.0 = bf16[1024]{0} parameter(65), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1521 p64.1090.0 @0>
 positions:
  p64.1090.0
  p.69
 uses:
  call, operand 69
  custom-call.75.0, operand 1
 from instruction: %p64.1090.0 = bf16[6144,1024]{1,0} parameter(64), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1522 p63.1088.0 @0>
 positions:
  p63.1088.0
  p.70
 uses:
  call, operand 70
  custom-call.76.0, operand 1
 from instruction: %p63.1088.0 = bf16[1024,3072]{1,0} parameter(63), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1523 p69.1164.0 @0>
 positions:
  p69.1164.0
  p.71
 uses:
  call, operand 71
  fusion.276, operand 3
 from instruction: %p69.1164.0 = bf16[1024]{0} parameter(69), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1524 p68.1162.0 @0>
 positions:
  p68.1162.0
  p.72
 uses:
  call, operand 72
  custom-call.77.0, operand 1
 from instruction: %p68.1162.0 = bf16[6144,1024]{1,0} parameter(68), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1525 p67.1160.0 @0>
 positions:
  p67.1160.0
  p.73
 uses:
  call, operand 73
  custom-call.78.0, operand 1
 from instruction: %p67.1160.0 = bf16[1024,3072]{1,0} parameter(67), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1526 p73.1236.0 @0>
 positions:
  p73.1236.0
  p.74
 uses:
  call, operand 74
  fusion.275, operand 2
 from instruction: %p73.1236.0 = bf16[1024]{0} parameter(73), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1527 p72.1234.0 @0>
 positions:
  p72.1234.0
  p.75
 uses:
  call, operand 75
  custom-call.79.0, operand 1
 from instruction: %p72.1234.0 = bf16[6144,1024]{1,0} parameter(72), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1528 p71.1232.0 @0>
 positions:
  p71.1232.0
  p.76
 uses:
  call, operand 76
  custom-call.80.0, operand 1
 from instruction: %p71.1232.0 = bf16[1024,3072]{1,0} parameter(71), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1529 p77.1308.0 @0>
 positions:
  p77.1308.0
  p.77
 uses:
  call, operand 77
  fusion.274, operand 4
 from instruction: %p77.1308.0 = bf16[1024]{0} parameter(77), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1530 p76.1306.0 @0>
 positions:
  p76.1306.0
  p.78
 uses:
  call, operand 78
  custom-call.81.0, operand 1
 from instruction: %p76.1306.0 = bf16[6144,1024]{1,0} parameter(76), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1531 p75.1304.0 @0>
 positions:
  p75.1304.0
  p.79
 uses:
  call, operand 79
  custom-call.82.0, operand 1
 from instruction: %p75.1304.0 = bf16[1024,3072]{1,0} parameter(75), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1532 p81.1380.0 @0>
 positions:
  p81.1380.0
  p.80
 uses:
  call, operand 80
  fusion.273, operand 1
 from instruction: %p81.1380.0 = bf16[1024]{0} parameter(81), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1533 p80.1378.0 @0>
 positions:
  p80.1378.0
  p.81
 uses:
  call, operand 81
  custom-call.83.0, operand 1
 from instruction: %p80.1378.0 = bf16[6144,1024]{1,0} parameter(80), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1534 p79.1376.0 @0>
 positions:
  p79.1376.0
  p.82
 uses:
  call, operand 82
  custom-call.84.0, operand 1
 from instruction: %p79.1376.0 = bf16[1024,3072]{1,0} parameter(79), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1535 p85.1452.0 @0>
 positions:
  p85.1452.0
  p.83
 uses:
  call, operand 83
  fusion.272, operand 3
 from instruction: %p85.1452.0 = bf16[1024]{0} parameter(85), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1536 p84.1450.0 @0>
 positions:
  p84.1450.0
  p.84
 uses:
  call, operand 84
  custom-call.85.0, operand 1
 from instruction: %p84.1450.0 = bf16[6144,1024]{1,0} parameter(84), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1537 p83.1448.0 @0>
 positions:
  p83.1448.0
  p.85
 uses:
  call, operand 85
  custom-call.86.0, operand 1
 from instruction: %p83.1448.0 = bf16[1024,3072]{1,0} parameter(83), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1538 p89.1524.0 @0>
 positions:
  p89.1524.0
  p.86
 uses:
  call, operand 86
  fusion.271, operand 2
 from instruction: %p89.1524.0 = bf16[1024]{0} parameter(89), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1539 p88.1522.0 @0>
 positions:
  p88.1522.0
  p.87
 uses:
  call, operand 87
  custom-call.87.0, operand 1
 from instruction: %p88.1522.0 = bf16[6144,1024]{1,0} parameter(88), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1540 p87.1520.0 @0>
 positions:
  p87.1520.0
  p.88
 uses:
  call, operand 88
  custom-call.88.0, operand 1
 from instruction: %p87.1520.0 = bf16[1024,3072]{1,0} parameter(87), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1541 p93.1596.0 @0>
 positions:
  p93.1596.0
  p.89
 uses:
  call, operand 89
  fusion.270, operand 4
 from instruction: %p93.1596.0 = bf16[1024]{0} parameter(93), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1542 p92.1594.0 @0>
 positions:
  p92.1594.0
  p.90
 uses:
  call, operand 90
  custom-call.89.0, operand 1
 from instruction: %p92.1594.0 = bf16[6144,1024]{1,0} parameter(92), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1543 p91.1592.0 @0>
 positions:
  p91.1592.0
  p.91
 uses:
  call, operand 91
  custom-call.90.0, operand 1
 from instruction: %p91.1592.0 = bf16[1024,3072]{1,0} parameter(91), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1544 p97.1668.0 @0>
 positions:
  p97.1668.0
  p.92
 uses:
  call, operand 92
  fusion.269, operand 1
 from instruction: %p97.1668.0 = bf16[1024]{0} parameter(97), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1545 p96.1666.0 @0>
 positions:
  p96.1666.0
  p.93
 uses:
  call, operand 93
  custom-call.91.0, operand 1
 from instruction: %p96.1666.0 = bf16[6144,1024]{1,0} parameter(96), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1546 p95.1664.0 @0>
 positions:
  p95.1664.0
  p.94
 uses:
  call, operand 94
  custom-call.92.0, operand 1
 from instruction: %p95.1664.0 = bf16[1024,3072]{1,0} parameter(95), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1547 p3.8.0 @0>
 positions:
  p3.8.0
  p.95
 uses:
  call, operand 95
  fusion.267, operand 2
 from instruction: %p3.8.0 = bf16[1024]{0} parameter(3), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1548 p2.6.0 @0>
 positions:
  p2.6.0
  p.96
 uses:
  call, operand 96
  custom-call.93.0, operand 1
 from instruction: %p2.6.0 = bf16[4096,1024]{1,0} parameter(2), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1549 p0.1.0 @0>
 positions:
  p0.1.0
  p.97
 uses:
  call, operand 97
  input_concatenate_fusion, operand 1
 from instruction: %p0.1.0 = bf16[128]{0} parameter(0), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1550 p99.1793.0 @0>
 positions:
  p99.1793.0
  p.98
 uses:
  call, operand 98
  input_concatenate_fusion, operand 2
 from instruction: %p99.1793.0 = bf16[40960,128]{1,0} parameter(99), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1551 p98.1792.0 @0>
 positions:
  p98.1792.0
  p.99
 uses:
  call, operand 99
  input_concatenate_fusion, operand 3
 from instruction: %p98.1792.0 = s32[64]{0} parameter(98), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1552 p100.1837.0 @0>
 positions:
  p100.1837.0
  p.100
 uses:
  call, operand 100
  loop_slice_fusion, operand 0
  wrapped_slice.1, operand 0
 from instruction: %p100.1837.0 = bf16[2,4233,16,8,128]{4,3,2,1,0} parameter(100), metadata={op_type="xla__device_data" op_name="xla__device_data" source_file="/usr/local/lib/python3.10/site-packages/torch_xla/_dynamo/dynamo_bridge.py" source_line=355}
<1553 tuple.1846.0{} @0>
 positions:
  tuple.1846.0 {}
 uses:
 from instruction: %tuple.1846.0 = (bf16[64,8,128]{2,1,0}, bf16[64,8,128]{2,1,0}, bf16[4233,16,8,128]{3,2,1,0}, bf16[4233,16,8,128]{3,2,1,0}) tuple(bf16[64,8,128]{2,1,0} %get-tuple-element.49, bf16[64,8,128]{2,1,0} %get-tuple-element.50, bf16[4233,16,8,128]{3,2,1,0} %bitcast.4676.0, bf16[4233,16,8,128]{3,2,1,0} %get-tuple-element.51)


HloLiveRange (max 366):
  InstructionSequence:
    0:p1.4.0
    1:p100.1837.0
    2:p99.1793.0
    3:p98.1792.0
    4:p97.1668.0
    5:p96.1666.0
    6:p95.1664.0
    7:p94.1648.0
    8:p93.1596.0
    9:p92.1594.0
    10:p91.1592.0
    11:p90.1576.0
    12:p89.1524.0
    13:p88.1522.0
    14:p87.1520.0
    15:p86.1504.0
    16:p85.1452.0
    17:p84.1450.0
    18:p83.1448.0
    19:p82.1432.0
    20:p81.1380.0
    21:p80.1378.0
    22:p79.1376.0
    23:p78.1360.0
    24:p77.1308.0
    25:p76.1306.0
    26:p75.1304.0
    27:p74.1288.0
    28:p73.1236.0
    29:p72.1234.0
    30:p71.1232.0
    31:p70.1216.0
    32:p69.1164.0
    33:p68.1162.0
    34:p67.1160.0
    35:p66.1144.0
    36:p65.1092.0
    37:p64.1090.0
    38:p63.1088.0
    39:p62.1072.0
    40:p61.1020.0
    41:p60.1018.0
    42:p59.1016.0
    43:p58.1000.0
    44:p57.948.0
    45:p56.946.0
    46:p55.944.0
    47:p54.928.0
    48:p53.876.0
    49:p52.874.0
    50:p51.872.0
    51:p50.856.0
    52:p49.804.0
    53:p48.802.0
    54:p47.800.0
    55:p46.784.0
    56:p45.732.0
    57:p44.730.0
    58:p43.728.0
    59:p42.712.0
    60:p41.660.0
    61:p40.658.0
    62:p39.656.0
    63:p38.640.0
    64:p37.588.0
    65:p36.586.0
    66:p35.584.0
    67:p34.568.0
    68:p33.516.0
    69:p32.514.0
    70:p31.512.0
    71:p30.496.0
    72:p29.444.0
    73:p28.442.0
    74:p27.440.0
    75:p26.424.0
    76:p25.372.0
    77:p24.370.0
    78:p23.368.0
    79:p22.352.0
    80:p21.300.0
    81:p20.298.0
    82:p19.296.0
    83:p18.280.0
    84:p17.228.0
    85:p16.226.0
    86:p15.224.0
    87:p14.208.0
    88:p13.156.0
    89:p12.154.0
    90:p11.152.0
    91:p10.136.0
    92:p9.84.0
    93:p8.82.0
    94:p7.80.0
    95:p6.64.0
    96:p5.60.0
    97:p4.58.0
    98:p3.8.0
    99:p2.6.0
    100:p0.1.0
    101:p
    102:p.1
    103:p.2
    104:p.3
    105:p.4
    106:p.5
    107:p.6
    108:p.7
    109:p.8
    110:p.9
    111:p.10
    112:p.11
    113:p.12
    114:p.13
    115:p.14
    116:p.15
    117:p.16
    118:p.17
    119:p.18
    120:p.19
    121:p.20
    122:p.21
    123:p.22
    124:p.23
    125:p.24
    126:p.25
    127:p.26
    128:p.27
    129:p.28
    130:p.29
    131:p.30
    132:p.31
    133:p.32
    134:p.33
    135:p.34
    136:p.35
    137:p.36
    138:p.37
    139:p.38
    140:p.39
    141:p.40
    142:p.41
    143:p.42
    144:p.43
    145:p.44
    146:p.45
    147:p.46
    148:p.47
    149:p.48
    150:p.49
    151:p.50
    152:p.51
    153:p.52
    154:p.53
    155:p.54
    156:p.55
    157:p.56
    158:p.57
    159:p.58
    160:p.59
    161:p.60
    162:p.61
    163:p.62
    164:p.63
    165:p.64
    166:p.65
    167:p.66
    168:p.67
    169:p.68
    170:p.69
    171:p.70
    172:p.71
    173:p.72
    174:p.73
    175:p.74
    176:p.75
    177:p.76
    178:p.77
    179:p.78
    180:p.79
    181:p.80
    182:p.81
    183:p.82
    184:p.83
    185:p.84
    186:p.85
    187:p.86
    188:p.87
    189:p.88
    190:p.89
    191:p.90
    192:p.91
    193:p.92
    194:p.93
    195:p.94
    196:p.95
    197:p.96
    198:p.97
    199:p.98
    200:p.99
    201:p.100
    202:loop_gather_fusion
    203:wrapped_concatenate
    204:gemm_fusion_dot.92.0
    205:fusion.291
    206:custom-call.47.0
    207:get-tuple-element.47
    208:loop_convert_fusion.22
    209:custom-call.48.0
    210:get-tuple-element.1.0
    211:fusion.290
    212:custom-call.49.0
    213:get-tuple-element.2.0
    214:loop_convert_fusion.21
    215:custom-call.50.0
    216:get-tuple-element.3.0
    217:fusion.289
    218:custom-call.51.0
    219:get-tuple-element.4.0
    220:loop_convert_fusion.20
    221:custom-call.52.0
    222:get-tuple-element.5.0
    223:fusion.288
    224:custom-call.53.0
    225:get-tuple-element.6.0
    226:loop_convert_fusion.19
    227:custom-call.54.0
    228:get-tuple-element.7.0
    229:loop_add_fusion
    230:fusion.287
    231:custom-call.55.0
    232:get-tuple-element.8.0
    233:loop_convert_fusion.18
    234:custom-call.56.0
    235:get-tuple-element.9.0
    236:fusion.286
    237:custom-call.57.0
    238:get-tuple-element.10.0
    239:loop_convert_fusion.17
    240:custom-call.58.0
    241:get-tuple-element.11.0
    242:fusion.285
    243:custom-call.59.0
    244:get-tuple-element.12.0
    245:loop_convert_fusion.16
    246:custom-call.60.0
    247:get-tuple-element.13.0
    248:fusion.284
    249:custom-call.61.0
    250:get-tuple-element.14.0
    251:loop_convert_fusion.15
    252:custom-call.62.0
    253:get-tuple-element.15.0
    254:loop_add_fusion.1
    255:fusion.283
    256:custom-call.63.0
    257:get-tuple-element.16.0
    258:loop_convert_fusion.14
    259:custom-call.64.0
    260:get-tuple-element.17.0
    261:fusion.282
    262:custom-call.65.0
    263:get-tuple-element.18.0
    264:loop_convert_fusion.13
    265:custom-call.66.0
    266:get-tuple-element.19.0
    267:fusion.281
    268:custom-call.67.0
    269:get-tuple-element.20.0
    270:loop_convert_fusion.12
    271:custom-call.68.0
    272:get-tuple-element.21.0
    273:fusion.280
    274:custom-call.69.0
    275:get-tuple-element.22.0
    276:loop_convert_fusion.11
    277:custom-call.70.0
    278:get-tuple-element.23.0
    279:loop_add_fusion.2
    280:fusion.279
    281:custom-call.71.0
    282:get-tuple-element.24.0
    283:loop_convert_fusion.10
    284:custom-call.72.0
    285:get-tuple-element.25.0
    286:fusion.278
    287:custom-call.73.0
    288:get-tuple-element.26.0
    289:loop_convert_fusion.9
    290:custom-call.74.0
    291:get-tuple-element.27.0
    292:fusion.277
    293:custom-call.75.0
    294:get-tuple-element.28.0
    295:loop_convert_fusion.8
    296:custom-call.76.0
    297:get-tuple-element.29.0
    298:fusion.276
    299:custom-call.77.0
    300:get-tuple-element.30.0
    301:loop_convert_fusion.7
    302:custom-call.78.0
    303:get-tuple-element.31.0
    304:loop_add_fusion.3
    305:fusion.275
    306:custom-call.79.0
    307:get-tuple-element.32.0
    308:loop_convert_fusion.6
    309:custom-call.80.0
    310:get-tuple-element.33.0
    311:fusion.274
    312:custom-call.81.0
    313:get-tuple-element.34.0
    314:loop_convert_fusion.5
    315:custom-call.82.0
    316:get-tuple-element.35.0
    317:fusion.273
    318:custom-call.83.0
    319:get-tuple-element.36.0
    320:loop_convert_fusion.4
    321:custom-call.84.0
    322:get-tuple-element.37.0
    323:fusion.272
    324:custom-call.85.0
    325:get-tuple-element.38.0
    326:loop_convert_fusion.3
    327:custom-call.86.0
    328:get-tuple-element.39.0
    329:loop_add_fusion.4
    330:fusion.271
    331:custom-call.87.0
    332:get-tuple-element.40.0
    333:loop_convert_fusion.2
    334:custom-call.88.0
    335:get-tuple-element.41.0
    336:fusion.270
    337:custom-call.89.0
    338:get-tuple-element.42.0
    339:loop_convert_fusion.1
    340:custom-call.90.0
    341:get-tuple-element.43.0
    342:fusion.269
    343:custom-call.91.0
    344:get-tuple-element.44.0
    345:loop_convert_fusion
    346:custom-call.92.0
    347:get-tuple-element.45.0
    348:fusion.267
    349:custom-call.93.0
    350:get-tuple-element.46.0
    351:wrapped_slice
    352:triton_softmax.25.0
    353:input_concatenate_fusion
    354:bitcast.4671.0
    355:loop_slice_fusion
    356:bitcast.4683.0
    357:wrapped_slice.1
    358:tuple
    359:call
    360:get-tuple-element.49
    361:get-tuple-element.50
    362:get-tuple-element.51
    363:get-tuple-element.52
    364:bitcast.4676.0
    365:tuple.1846.0
  BufferLiveRange:
    wrapped_concatenate{}:203-204
    gemm_fusion_dot.92.0{}:204-348
    loop_gather_fusion{}:202-229
    fusion.291{}:205-206
    custom-call.47.0{}:206-207
    custom-call.47.0{0}:206-208
    custom-call.47.0{1}:206-206
    loop_convert_fusion.22{}:208-209
    custom-call.48.0{}:209-210
    custom-call.48.0{0}:209-229
    custom-call.48.0{1}:209-209
    fusion.290{}:211-212
    custom-call.49.0{}:212-213
    custom-call.49.0{0}:212-214
    custom-call.49.0{1}:212-212
    loop_convert_fusion.21{}:214-215
    custom-call.50.0{}:215-216
    custom-call.50.0{0}:215-229
    custom-call.50.0{1}:215-215
    fusion.289{}:217-218
    custom-call.51.0{}:218-219
    custom-call.51.0{0}:218-220
    custom-call.51.0{1}:218-218
    loop_convert_fusion.20{}:220-221
    custom-call.52.0{}:221-222
    custom-call.52.0{0}:221-229
    custom-call.52.0{1}:221-221
    fusion.288{}:223-224
    custom-call.53.0{}:224-225
    custom-call.53.0{0}:224-226
    custom-call.53.0{1}:224-224
    loop_convert_fusion.19{}:226-227
    custom-call.54.0{}:227-228
    custom-call.54.0{0}:227-229
    custom-call.54.0{1}:227-227
    loop_add_fusion{}:229-254
    fusion.287{}:230-231
    custom-call.55.0{}:231-232
    custom-call.55.0{0}:231-233
    custom-call.55.0{1}:231-231
    loop_convert_fusion.18{}:233-234
    custom-call.56.0{}:234-235
    custom-call.56.0{0}:234-254
    custom-call.56.0{1}:234-234
    fusion.286{}:236-237
    custom-call.57.0{}:237-238
    custom-call.57.0{0}:237-239
    custom-call.57.0{1}:237-237
    loop_convert_fusion.17{}:239-240
    custom-call.58.0{}:240-241
    custom-call.58.0{0}:240-254
    custom-call.58.0{1}:240-240
    fusion.285{}:242-243
    custom-call.59.0{}:243-244
    custom-call.59.0{0}:243-245
    custom-call.59.0{1}:243-243
    loop_convert_fusion.16{}:245-246
    custom-call.60.0{}:246-247
    custom-call.60.0{0}:246-254
    custom-call.60.0{1}:246-246
    fusion.284{}:248-249
    custom-call.61.0{}:249-250
    custom-call.61.0{0}:249-251
    custom-call.61.0{1}:249-249
    loop_convert_fusion.15{}:251-252
    custom-call.62.0{}:252-253
    custom-call.62.0{0}:252-254
    custom-call.62.0{1}:252-252
    loop_add_fusion.1{}:254-279
    fusion.283{}:255-256
    custom-call.63.0{}:256-257
    custom-call.63.0{0}:256-258
    custom-call.63.0{1}:256-256
    loop_convert_fusion.14{}:258-259
    custom-call.64.0{}:259-260
    custom-call.64.0{0}:259-279
    custom-call.64.0{1}:259-259
    fusion.282{}:261-262
    custom-call.65.0{}:262-263
    custom-call.65.0{0}:262-264
    custom-call.65.0{1}:262-262
    loop_convert_fusion.13{}:264-265
    custom-call.66.0{}:265-266
    custom-call.66.0{0}:265-279
    custom-call.66.0{1}:265-265
    fusion.281{}:267-268
    custom-call.67.0{}:268-269
    custom-call.67.0{0}:268-270
    custom-call.67.0{1}:268-268
    loop_convert_fusion.12{}:270-271
    custom-call.68.0{}:271-272
    custom-call.68.0{0}:271-279
    custom-call.68.0{1}:271-271
    fusion.280{}:273-274
    custom-call.69.0{}:274-275
    custom-call.69.0{0}:274-276
    custom-call.69.0{1}:274-274
    loop_convert_fusion.11{}:276-277
    custom-call.70.0{}:277-278
    custom-call.70.0{0}:277-279
    custom-call.70.0{1}:277-277
    loop_add_fusion.2{}:279-304
    fusion.279{}:280-281
    custom-call.71.0{}:281-282
    custom-call.71.0{0}:281-283
    custom-call.71.0{1}:281-281
    loop_convert_fusion.10{}:283-284
    custom-call.72.0{}:284-285
    custom-call.72.0{0}:284-304
    custom-call.72.0{1}:284-284
    fusion.278{}:286-287
    custom-call.73.0{}:287-288
    custom-call.73.0{0}:287-289
    custom-call.73.0{1}:287-287
    loop_convert_fusion.9{}:289-290
    custom-call.74.0{}:290-291
    custom-call.74.0{0}:290-304
    custom-call.74.0{1}:290-290
    fusion.277{}:292-293
    custom-call.75.0{}:293-294
    custom-call.75.0{0}:293-295
    custom-call.75.0{1}:293-293
    loop_convert_fusion.8{}:295-296
    custom-call.76.0{}:296-297
    custom-call.76.0{0}:296-304
    custom-call.76.0{1}:296-296
    fusion.276{}:298-299
    custom-call.77.0{}:299-300
    custom-call.77.0{0}:299-301
    custom-call.77.0{1}:299-299
    loop_convert_fusion.7{}:301-302
    custom-call.78.0{}:302-303
    custom-call.78.0{0}:302-304
    custom-call.78.0{1}:302-302
    loop_add_fusion.3{}:304-329
    fusion.275{}:305-306
    custom-call.79.0{}:306-307
    custom-call.79.0{0}:306-308
    custom-call.79.0{1}:306-306
    loop_convert_fusion.6{}:308-309
    custom-call.80.0{}:309-310
    custom-call.80.0{0}:309-329
    custom-call.80.0{1}:309-309
    fusion.274{}:311-312
    custom-call.81.0{}:312-313
    custom-call.81.0{0}:312-314
    custom-call.81.0{1}:312-312
    loop_convert_fusion.5{}:314-315
    custom-call.82.0{}:315-316
    custom-call.82.0{0}:315-329
    custom-call.82.0{1}:315-315
    fusion.273{}:317-318
    custom-call.83.0{}:318-319
    custom-call.83.0{0}:318-320
    custom-call.83.0{1}:318-318
    loop_convert_fusion.4{}:320-321
    custom-call.84.0{}:321-322
    custom-call.84.0{0}:321-329
    custom-call.84.0{1}:321-321
    fusion.272{}:323-324
    custom-call.85.0{}:324-325
    custom-call.85.0{0}:324-326
    custom-call.85.0{1}:324-324
    loop_convert_fusion.3{}:326-327
    custom-call.86.0{}:327-328
    custom-call.86.0{0}:327-329
    custom-call.86.0{1}:327-327
    loop_add_fusion.4{}:329-348
    fusion.271{}:330-331
    custom-call.87.0{}:331-332
    custom-call.87.0{0}:331-333
    custom-call.87.0{1}:331-331
    loop_convert_fusion.2{}:333-334
    custom-call.88.0{}:334-335
    custom-call.88.0{0}:334-348
    custom-call.88.0{1}:334-334
    fusion.270{}:336-337
    custom-call.89.0{}:337-338
    custom-call.89.0{0}:337-339
    custom-call.89.0{1}:337-337
    loop_convert_fusion.1{}:339-340
    custom-call.90.0{}:340-341
    custom-call.90.0{0}:340-348
    custom-call.90.0{1}:340-340
    fusion.269{}:342-343
    custom-call.91.0{}:343-344
    custom-call.91.0{0}:343-345
    custom-call.91.0{1}:343-343
    loop_convert_fusion{}:345-346
    custom-call.92.0{}:346-347
    custom-call.92.0{0}:346-348
    custom-call.92.0{1}:346-346
    fusion.267{}:348-349
    custom-call.93.0{}:349-350
    custom-call.93.0{0}:349-352
    custom-call.93.0{1}:349-349
    triton_softmax.25.0{}:352-353
    input_concatenate_fusion{}:353-366
    wrapped_slice{}:351-366
    loop_slice_fusion{}:355-366
    wrapped_slice.1{}:357-366
    tuple{}:358-363
    p5.60.0{}:0-366
    p4.58.0{}:0-366
    p94.1648.0{}:0-366
    p90.1576.0{}:0-366
    p86.1504.0{}:0-366
    p82.1432.0{}:0-366
    p78.1360.0{}:0-366
    p74.1288.0{}:0-366
    p70.1216.0{}:0-366
    p66.1144.0{}:0-366
    p62.1072.0{}:0-366
    p58.1000.0{}:0-366
    p54.928.0{}:0-366
    p50.856.0{}:0-366
    p46.784.0{}:0-366
    p42.712.0{}:0-366
    p38.640.0{}:0-366
    p34.568.0{}:0-366
    p30.496.0{}:0-366
    p26.424.0{}:0-366
    p22.352.0{}:0-366
    p18.280.0{}:0-366
    p14.208.0{}:0-366
    p10.136.0{}:0-366
    p6.64.0{}:0-366
    p1.4.0{}:0-366
    p9.84.0{}:0-366
    p8.82.0{}:0-366
    p7.80.0{}:0-366
    p13.156.0{}:0-366
    p12.154.0{}:0-366
    p11.152.0{}:0-366
    p17.228.0{}:0-366
    p16.226.0{}:0-366
    p15.224.0{}:0-366
    p21.300.0{}:0-366
    p20.298.0{}:0-366
    p19.296.0{}:0-366
    p25.372.0{}:0-366
    p24.370.0{}:0-366
    p23.368.0{}:0-366
    p29.444.0{}:0-366
    p28.442.0{}:0-366
    p27.440.0{}:0-366
    p33.516.0{}:0-366
    p32.514.0{}:0-366
    p31.512.0{}:0-366
    p37.588.0{}:0-366
    p36.586.0{}:0-366
    p35.584.0{}:0-366
    p41.660.0{}:0-366
    p40.658.0{}:0-366
    p39.656.0{}:0-366
    p45.732.0{}:0-366
    p44.730.0{}:0-366
    p43.728.0{}:0-366
    p49.804.0{}:0-366
    p48.802.0{}:0-366
    p47.800.0{}:0-366
    p53.876.0{}:0-366
    p52.874.0{}:0-366
    p51.872.0{}:0-366
    p57.948.0{}:0-366
    p56.946.0{}:0-366
    p55.944.0{}:0-366
    p61.1020.0{}:0-366
    p60.1018.0{}:0-366
    p59.1016.0{}:0-366
    p65.1092.0{}:0-366
    p64.1090.0{}:0-366
    p63.1088.0{}:0-366
    p69.1164.0{}:0-366
    p68.1162.0{}:0-366
    p67.1160.0{}:0-366
    p73.1236.0{}:0-366
    p72.1234.0{}:0-366
    p71.1232.0{}:0-366
    p77.1308.0{}:0-366
    p76.1306.0{}:0-366
    p75.1304.0{}:0-366
    p81.1380.0{}:0-366
    p80.1378.0{}:0-366
    p79.1376.0{}:0-366
    p85.1452.0{}:0-366
    p84.1450.0{}:0-366
    p83.1448.0{}:0-366
    p89.1524.0{}:0-366
    p88.1522.0{}:0-366
    p87.1520.0{}:0-366
    p93.1596.0{}:0-366
    p92.1594.0{}:0-366
    p91.1592.0{}:0-366
    p97.1668.0{}:0-366
    p96.1666.0{}:0-366
    p95.1664.0{}:0-366
    p3.8.0{}:0-366
    p2.6.0{}:0-366
    p0.1.0{}:0-366
    p99.1793.0{}:0-366
    p98.1792.0{}:0-366
    p100.1837.0{}:0-366
    tuple.1846.0{}:365-366
  Live ranges at 358 (peak):
    input_concatenate_fusion: 131072 bytes
    wrapped_slice: 131072 bytes
    loop_slice_fusion: 138706944 bytes
    wrapped_slice.1: 138706944 bytes
    tuple: 32 bytes
    p5.60.0: 311164928 bytes
    p4.58.0: 256 bytes
    p94.1648.0: 4194304 bytes
    p90.1576.0: 4194304 bytes
    p86.1504.0: 4194304 bytes
    p82.1432.0: 4194304 bytes
    p78.1360.0: 4194304 bytes
    p74.1288.0: 4194304 bytes
    p70.1216.0: 4194304 bytes
    p66.1144.0: 4194304 bytes
    p62.1072.0: 4194304 bytes
    p58.1000.0: 4194304 bytes
    p54.928.0: 4194304 bytes
    p50.856.0: 4194304 bytes
    p46.784.0: 4194304 bytes
    p42.712.0: 4194304 bytes
    p38.640.0: 4194304 bytes
    p34.568.0: 4194304 bytes
    p30.496.0: 4194304 bytes
    p26.424.0: 4194304 bytes
    p22.352.0: 4194304 bytes
    p18.280.0: 4194304 bytes
    p14.208.0: 4194304 bytes
    p10.136.0: 4194304 bytes
    p6.64.0: 4194304 bytes
    p1.4.0: 4 bytes
    p9.84.0: 2048 bytes
    p8.82.0: 12582912 bytes
    p7.80.0: 6291456 bytes
    p13.156.0: 2048 bytes
    p12.154.0: 12582912 bytes
    p11.152.0: 6291456 bytes
    p17.228.0: 2048 bytes
    p16.226.0: 12582912 bytes
    p15.224.0: 6291456 bytes
    p21.300.0: 2048 bytes
    p20.298.0: 12582912 bytes
    p19.296.0: 6291456 bytes
    p25.372.0: 2048 bytes
    p24.370.0: 12582912 bytes
    p23.368.0: 6291456 bytes
    p29.444.0: 2048 bytes
    p28.442.0: 12582912 bytes
    p27.440.0: 6291456 bytes
    p33.516.0: 2048 bytes
    p32.514.0: 12582912 bytes
    p31.512.0: 6291456 bytes
    p37.588.0: 2048 bytes
    p36.586.0: 12582912 bytes
    p35.584.0: 6291456 bytes
    p41.660.0: 2048 bytes
    p40.658.0: 12582912 bytes
    p39.656.0: 6291456 bytes
    p45.732.0: 2048 bytes
    p44.730.0: 12582912 bytes
    p43.728.0: 6291456 bytes
    p49.804.0: 2048 bytes
    p48.802.0: 12582912 bytes
    p47.800.0: 6291456 bytes
    p53.876.0: 2048 bytes
    p52.874.0: 12582912 bytes
    p51.872.0: 6291456 bytes
    p57.948.0: 2048 bytes
    p56.946.0: 12582912 bytes
    p55.944.0: 6291456 bytes
    p61.1020.0: 2048 bytes
    p60.1018.0: 12582912 bytes
    p59.1016.0: 6291456 bytes
    p65.1092.0: 2048 bytes
    p64.1090.0: 12582912 bytes
    p63.1088.0: 6291456 bytes
    p69.1164.0: 2048 bytes
    p68.1162.0: 12582912 bytes
    p67.1160.0: 6291456 bytes
    p73.1236.0: 2048 bytes
    p72.1234.0: 12582912 bytes
    p71.1232.0: 6291456 bytes
    p77.1308.0: 2048 bytes
    p76.1306.0: 12582912 bytes
    p75.1304.0: 6291456 bytes
    p81.1380.0: 2048 bytes
    p80.1378.0: 12582912 bytes
    p79.1376.0: 6291456 bytes
    p85.1452.0: 2048 bytes
    p84.1450.0: 12582912 bytes
    p83.1448.0: 6291456 bytes
    p89.1524.0: 2048 bytes
    p88.1522.0: 12582912 bytes
    p87.1520.0: 6291456 bytes
    p93.1596.0: 2048 bytes
    p92.1594.0: 12582912 bytes
    p91.1592.0: 6291456 bytes
    p97.1668.0: 2048 bytes
    p96.1666.0: 12582912 bytes
    p95.1664.0: 6291456 bytes
    p3.8.0: 2048 bytes
    p2.6.0: 8388608 bytes
    p0.1.0: 256 bytes
    p99.1793.0: 10485760 bytes
    p98.1792.0: 256 bytes
    p100.1837.0: 277413888 bytes
