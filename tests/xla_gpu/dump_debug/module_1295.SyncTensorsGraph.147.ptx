//
// Generated by LLVM NVPTX Back-End
//

.version 8.5
.target sm_80
.address_size 64

	// .globl	loop_gather_fusion
.extern .shared .align 16 .b8 global_smem[];

.visible .entry loop_gather_fusion(
	.param .u64 .ptr .align 16 loop_gather_fusion_param_0,
	.param .u64 .ptr .align 16 loop_gather_fusion_param_1,
	.param .u64 .ptr .align 128 loop_gather_fusion_param_2
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<12>;
	.reg .b64 	%rd<13>;

	ld.param.u64 	%rd1, [loop_gather_fusion_param_0];
	ld.param.u64 	%rd2, [loop_gather_fusion_param_2];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [loop_gather_fusion_param_1];
	cvta.to.global.u64 	%rd5, %rd4;
	cvta.to.global.u64 	%rd6, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shr.u32 	%r3, %r2, 3;
	mul.wide.u32 	%rd7, %r3, 4;
	add.s64 	%rd8, %rd5, %rd7;
	ld.global.nc.u32 	%r4, [%rd8];
	min.u32 	%r5, %r4, 151935;
	shl.b32 	%r6, %r2, 7;
	and.b32  	%r7, %r6, 896;
	shl.b32 	%r8, %r5, 10;
	or.b32  	%r9, %r8, %r7;
	or.b32  	%r10, %r9, %r1;
	mul.wide.u32 	%rd9, %r10, 2;
	add.s64 	%rd10, %rd6, %rd9;
	ld.global.nc.u16 	%rs1, [%rd10];
	or.b32  	%r11, %r6, %r1;
	mul.wide.u32 	%rd11, %r11, 2;
	add.s64 	%rd12, %rd3, %rd11;
	st.global.b16 	[%rd12], %rs1;
	ret;

}
	// .globl	fusion_11
.visible .entry fusion_11(
	.param .u64 .ptr .align 16 fusion_11_param_0,
	.param .u64 .ptr .align 128 fusion_11_param_1,
	.param .u64 .ptr .align 16 fusion_11_param_2,
	.param .u64 .ptr .align 128 fusion_11_param_3
)
.reqntid 64, 1, 1
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<29>;
	.reg .b32 	%r<74>;
	.reg .f32 	%f<82>;
	.reg .b64 	%rd<29>;

	ld.param.u64 	%rd7, [fusion_11_param_0];
	ld.param.u64 	%rd8, [fusion_11_param_3];
	cvta.to.global.u64 	%rd9, %rd8;
	ld.param.u64 	%rd10, [fusion_11_param_1];
	ld.param.u64 	%rd11, [fusion_11_param_2];
	cvta.to.global.u64 	%rd12, %rd11;
	cvta.to.global.u64 	%rd13, %rd10;
	cvta.to.global.u64 	%rd4, %rd7;
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	mul.wide.s32 	%rd14, %r1, 256;
	shl.b64 	%rd15, %rd14, 1;
	add.s64 	%rd16, %rd13, %rd15;
	mov.u32 	%r51, %tid.x;
	and.b32  	%r52, %r51, 31;
	shl.b32 	%r53, %r51, 2;
	and.b32  	%r54, %r53, 124;
	setp.lt.u32 	%p4, %r51, 32;
	selp.b32 	%r55, 0, 128, %p4;
	or.b32  	%r56, %r54, %r55;
	mul.wide.u32 	%rd17, %r56, 2;
	add.s64 	%rd1, %rd16, %rd17;
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	ld.global.v2.b32 { %r2, %r3 }, [ %rd1 + 0 ];
	// end inline asm
	mov.b32 	{%rs1, %rs2}, %r2;
	mov.b32 	{%rs3, %rs4}, %r3;
	// begin inline asm
	cvt.f32.bf16 %r4, %rs1;
	// end inline asm
	mov.b32 	%f1, %r4;
	// begin inline asm
	cvt.f32.bf16 %r5, %rs2;
	// end inline asm
	mov.b32 	%f2, %r5;
	// begin inline asm
	cvt.f32.bf16 %r6, %rs3;
	// end inline asm
	mov.b32 	%f3, %r6;
	// begin inline asm
	cvt.f32.bf16 %r7, %rs4;
	// end inline asm
	mov.b32 	%f4, %r7;
	and.b64  	%rd18, %rd15, -2048;
	add.s64 	%rd19, %rd13, %rd18;
	shl.b32 	%r57, %r51, 3;
	and.b32  	%r58, %r57, 248;
	selp.b32 	%r59, 0, 256, %p4;
	or.b32  	%r60, %r58, %r59;
	mul.wide.u32 	%rd20, %r60, 2;
	add.s64 	%rd2, %rd19, %rd20;
	add.s64 	%rd3, %rd2, 1024;
	// begin inline asm
	mov.u32 %r8, 0x0;
	mov.u32 %r9, 0x0;
	mov.u32 %r10, 0x0;
	mov.u32 %r11, 0x0;
	ld.global.v4.b32 { %r8, %r9, %r10, %r11 }, [ %rd2 + 0 ];
	// end inline asm
	mov.b32 	{%rs5, %rs6}, %r8;
	mov.b32 	{%rs7, %rs8}, %r9;
	mov.b32 	{%rs9, %rs10}, %r10;
	mov.b32 	{%rs11, %rs12}, %r11;
	// begin inline asm
	mov.u32 %r12, 0x0;
	mov.u32 %r13, 0x0;
	mov.u32 %r14, 0x0;
	mov.u32 %r15, 0x0;
	ld.global.v4.b32 { %r12, %r13, %r14, %r15 }, [ %rd3 + 0 ];
	// end inline asm
	mov.b32 	{%rs13, %rs14}, %r12;
	mov.b32 	{%rs15, %rs16}, %r13;
	mov.b32 	{%rs17, %rs18}, %r14;
	mov.b32 	{%rs19, %rs20}, %r15;
	// begin inline asm
	cvt.f32.bf16 %r16, %rs5;
	// end inline asm
	mov.b32 	%f5, %r16;
	// begin inline asm
	cvt.f32.bf16 %r17, %rs6;
	// end inline asm
	mov.b32 	%f6, %r17;
	// begin inline asm
	cvt.f32.bf16 %r18, %rs7;
	// end inline asm
	mov.b32 	%f7, %r18;
	// begin inline asm
	cvt.f32.bf16 %r19, %rs8;
	// end inline asm
	mov.b32 	%f8, %r19;
	// begin inline asm
	cvt.f32.bf16 %r20, %rs9;
	// end inline asm
	mov.b32 	%f9, %r20;
	// begin inline asm
	cvt.f32.bf16 %r21, %rs10;
	// end inline asm
	mov.b32 	%f10, %r21;
	// begin inline asm
	cvt.f32.bf16 %r22, %rs11;
	// end inline asm
	mov.b32 	%f11, %r22;
	// begin inline asm
	cvt.f32.bf16 %r23, %rs12;
	// end inline asm
	mov.b32 	%f12, %r23;
	// begin inline asm
	cvt.f32.bf16 %r24, %rs13;
	// end inline asm
	mov.b32 	%f13, %r24;
	// begin inline asm
	cvt.f32.bf16 %r25, %rs14;
	// end inline asm
	mov.b32 	%f14, %r25;
	// begin inline asm
	cvt.f32.bf16 %r26, %rs15;
	// end inline asm
	mov.b32 	%f15, %r26;
	// begin inline asm
	cvt.f32.bf16 %r27, %rs16;
	// end inline asm
	mov.b32 	%f16, %r27;
	// begin inline asm
	cvt.f32.bf16 %r28, %rs17;
	// end inline asm
	mov.b32 	%f17, %r28;
	// begin inline asm
	cvt.f32.bf16 %r29, %rs18;
	// end inline asm
	mov.b32 	%f18, %r29;
	// begin inline asm
	cvt.f32.bf16 %r30, %rs19;
	// end inline asm
	mov.b32 	%f19, %r30;
	// begin inline asm
	cvt.f32.bf16 %r31, %rs20;
	// end inline asm
	mov.b32 	%f20, %r31;
	mul.rn.f32 	%f21, %f5, %f5;
	mul.rn.f32 	%f22, %f6, %f6;
	mul.rn.f32 	%f23, %f7, %f7;
	mul.rn.f32 	%f24, %f8, %f8;
	mul.rn.f32 	%f25, %f9, %f9;
	mul.rn.f32 	%f26, %f10, %f10;
	mul.rn.f32 	%f27, %f11, %f11;
	mul.rn.f32 	%f28, %f12, %f12;
	mul.rn.f32 	%f29, %f13, %f13;
	mul.rn.f32 	%f30, %f14, %f14;
	mul.rn.f32 	%f31, %f15, %f15;
	mul.rn.f32 	%f32, %f16, %f16;
	mul.rn.f32 	%f33, %f17, %f17;
	mul.rn.f32 	%f34, %f18, %f18;
	mul.rn.f32 	%f35, %f19, %f19;
	mul.rn.f32 	%f36, %f20, %f20;
	add.rn.f32 	%f37, %f21, %f22;
	add.rn.f32 	%f38, %f37, %f23;
	add.rn.f32 	%f39, %f38, %f24;
	add.rn.f32 	%f40, %f39, %f25;
	add.rn.f32 	%f41, %f40, %f26;
	add.rn.f32 	%f42, %f41, %f27;
	add.rn.f32 	%f43, %f42, %f28;
	add.rn.f32 	%f44, %f43, %f29;
	add.rn.f32 	%f45, %f44, %f30;
	add.rn.f32 	%f46, %f45, %f31;
	add.rn.f32 	%f47, %f46, %f32;
	add.rn.f32 	%f48, %f47, %f33;
	add.rn.f32 	%f49, %f48, %f34;
	add.rn.f32 	%f50, %f49, %f35;
	add.rn.f32 	%f51, %f50, %f36;
	mov.b32 	%r61, %f51;
	shfl.sync.bfly.b32	%r62, %r61, 16, 31, -1;
	mov.b32 	%f52, %r62;
	add.rn.f32 	%f53, %f51, %f52;
	mov.b32 	%r63, %f53;
	shfl.sync.bfly.b32	%r64, %r63, 8, 31, -1;
	mov.b32 	%f54, %r64;
	add.rn.f32 	%f55, %f53, %f54;
	mov.b32 	%r65, %f55;
	shfl.sync.bfly.b32	%r66, %r65, 4, 31, -1;
	mov.b32 	%f56, %r66;
	add.rn.f32 	%f57, %f55, %f56;
	mov.b32 	%r67, %f57;
	shfl.sync.bfly.b32	%r68, %r67, 2, 31, -1;
	mov.b32 	%f58, %r68;
	add.rn.f32 	%f59, %f57, %f58;
	mov.b32 	%r69, %f59;
	shfl.sync.bfly.b32	%r70, %r69, 1, 31, -1;
	mov.b32 	%f60, %r70;
	add.rn.f32 	%f61, %f59, %f60;
	setp.eq.s32 	%p1, %r52, 0;
	shr.u32 	%r71, %r51, 3;
	and.b32  	%r72, %r71, 4;
	cvt.u64.u32 	%rd21, %r72;
	mov.u64 	%rd22, global_smem;
	add.s64 	%rd23, %rd22, %rd21;
	mov.b32 	%r33, %f61;
	cvt.u32.u64 	%r32, %rd23;
	// begin inline asm
	@%p1 st.shared.b32 [ %r32 + 0 ], %r33;
	// end inline asm
	bar.sync 	0;
	setp.lt.u32 	%p2, %r51, 2;
	cvt.u64.u32 	%rd24, %r53;
	add.s64 	%rd25, %rd22, %rd24;
	cvt.u32.u64 	%r35, %rd25;
	// begin inline asm
	@%p2 ld.shared.b32 %r34, [ %r35 + 0 ];
	// end inline asm
	mov.b32 	%f62, %r34;
	shfl.sync.bfly.b32	%r73, %r34, 1, 31, -1;
	mov.b32 	%f63, %r73;
	add.rn.f32 	%f64, %f62, %f63;
	setp.eq.s32 	%p3, %r51, 0;
	mov.b32 	%r37, %f64;
	// begin inline asm
	@%p3 st.shared.b32 [ %r35 + 0 ], %r37;
	// end inline asm
	bar.sync 	0;
	ld.shared.f32 	%f65, [global_smem];
	mul.rn.f32 	%f66, %f65, 0f3A800000;
	// begin inline asm
	mov.u32 %r38, 0x0;
	ld.global.b32 { %r38 }, [ %rd4 + 0 ];
	// end inline asm
	mov.b32 	%f67, %r38;
	add.rn.f32 	%f68, %f66, %f67;
	rsqrt.approx.f32 	%f69, %f68;
	mul.rn.f32 	%f70, %f1, %f69;
	mul.rn.f32 	%f71, %f2, %f69;
	mul.rn.f32 	%f72, %f3, %f69;
	mul.rn.f32 	%f73, %f4, %f69;
	and.b64  	%rd26, %rd15, 1536;
	add.s64 	%rd27, %rd12, %rd26;
	add.s64 	%rd5, %rd27, %rd17;
	// begin inline asm
	mov.u32 %r39, 0x0;
	mov.u32 %r40, 0x0;
	ld.global.v2.b32 { %r39, %r40 }, [ %rd5 + 0 ];
	// end inline asm
	mov.b32 	{%rs21, %rs22}, %r39;
	mov.b32 	{%rs23, %rs24}, %r40;
	// begin inline asm
	cvt.f32.bf16 %r41, %rs21;
	// end inline asm
	mov.b32 	%f74, %r41;
	// begin inline asm
	cvt.f32.bf16 %r42, %rs22;
	// end inline asm
	mov.b32 	%f75, %r42;
	// begin inline asm
	cvt.f32.bf16 %r43, %rs23;
	// end inline asm
	mov.b32 	%f76, %r43;
	// begin inline asm
	cvt.f32.bf16 %r44, %rs24;
	// end inline asm
	mov.b32 	%f77, %r44;
	mul.rn.f32 	%f78, %f70, %f74;
	mul.rn.f32 	%f79, %f71, %f75;
	mul.rn.f32 	%f80, %f72, %f76;
	mul.rn.f32 	%f81, %f73, %f77;
	mov.b32 	%r45, %f78;
	// begin inline asm
	cvt.rn.bf16.f32 %rs25, %r45;
	// end inline asm
	mov.b32 	%r46, %f79;
	// begin inline asm
	cvt.rn.bf16.f32 %rs26, %r46;
	// end inline asm
	mov.b32 	%r47, %f80;
	// begin inline asm
	cvt.rn.bf16.f32 %rs27, %r47;
	// end inline asm
	mov.b32 	%r48, %f81;
	// begin inline asm
	cvt.rn.bf16.f32 %rs28, %r48;
	// end inline asm
	add.s64 	%rd28, %rd9, %rd15;
	add.s64 	%rd6, %rd28, %rd17;
	mov.b32 	%r49, {%rs25, %rs26};
	mov.b32 	%r50, {%rs27, %rs28};
	// begin inline asm
	st.global.v2.b32 [ %rd6 + 0 ], { %r49, %r50 };
	// end inline asm
	ret;

}
	// .globl	wrapped_slice
.visible .entry wrapped_slice(
	.param .u64 .ptr .align 128 wrapped_slice_param_0,
	.param .u64 .ptr .align 128 wrapped_slice_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;

	ld.param.u64 	%rd1, [wrapped_slice_param_0];
	ld.param.u64 	%rd2, [wrapped_slice_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r2, 7;
	and.b32  	%r4, %r3, 896;
	shl.b32 	%r5, %r2, 9;
	and.b32  	%r6, %r5, 126976;
	or.b32  	%r7, %r6, %r4;
	or.b32  	%r8, %r7, %r1;
	mul.wide.u32 	%rd5, %r8, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.u16 	%rs1, [%rd6+6144];
	or.b32  	%r9, %r3, %r1;
	mul.wide.u32 	%rd7, %r9, 2;
	add.s64 	%rd8, %rd3, %rd7;
	st.global.b16 	[%rd8], %rs1;
	ret;

}
	// .globl	fusion_9
.visible .entry fusion_9(
	.param .u64 .ptr .align 16 fusion_9_param_0,
	.param .u64 .ptr .align 128 fusion_9_param_1,
	.param .u64 .ptr .align 16 fusion_9_param_2,
	.param .u64 .ptr .align 128 fusion_9_param_3
)
.reqntid 64, 1, 1
{
	.reg .pred 	%p<5>;
	.reg .b16 	%rs<19>;
	.reg .b32 	%r<78>;
	.reg .f32 	%f<60>;
	.reg .b64 	%rd<43>;

	ld.param.u64 	%rd6, [fusion_9_param_0];
	ld.param.u64 	%rd7, [fusion_9_param_3];
	cvta.to.global.u64 	%rd8, %rd7;
	ld.param.u64 	%rd9, [fusion_9_param_1];
	ld.param.u64 	%rd10, [fusion_9_param_2];
	cvta.to.global.u64 	%rd11, %rd10;
	cvta.to.global.u64 	%rd12, %rd9;
	cvta.to.global.u64 	%rd2, %rd6;
	// begin inline asm
	mov.u32 %r1, %ctaid.x;
	// end inline asm
	shl.b32 	%r43, %r1, 9;
	cvt.u64.u32 	%rd13, %r43;
	and.b64  	%rd14, %rd13, 512;
	mul.wide.s32 	%rd15, %r1, 2048;
	or.b64  	%rd16, %rd15, %rd14;
	shl.b64 	%rd17, %rd16, 1;
	or.b64  	%rd18, %rd17, 4096;
	add.s64 	%rd19, %rd12, %rd18;
	mov.u32 	%r44, %tid.x;
	shl.b32 	%r45, %r44, 3;
	and.b32  	%r46, %r45, 120;
	and.b32  	%r47, %r45, 248;
	setp.gt.u32 	%p4, %r44, 31;
	selp.b32 	%r48, 256, 0, %p4;
	or.b32  	%r49, %r47, %r48;
	mul.wide.u32 	%rd20, %r49, 2;
	add.s64 	%rd1, %rd19, %rd20;
	// begin inline asm
	mov.u32 %r2, 0x0;
	mov.u32 %r3, 0x0;
	mov.u32 %r4, 0x0;
	mov.u32 %r5, 0x0;
	ld.global.v4.b32 { %r2, %r3, %r4, %r5 }, [ %rd1 + 0 ];
	// end inline asm
	mov.b32 	{%rs1, %rs2}, %r2;
	mov.b32 	{%rs3, %rs4}, %r3;
	mov.b32 	{%rs5, %rs6}, %r4;
	mov.b32 	{%rs7, %rs8}, %r5;
	// begin inline asm
	cvt.f32.bf16 %r6, %rs1;
	// end inline asm
	mov.b32 	%f1, %r6;
	// begin inline asm
	cvt.f32.bf16 %r7, %rs2;
	// end inline asm
	mov.b32 	%f2, %r7;
	// begin inline asm
	cvt.f32.bf16 %r8, %rs3;
	// end inline asm
	mov.b32 	%f3, %r8;
	// begin inline asm
	cvt.f32.bf16 %r9, %rs4;
	// end inline asm
	mov.b32 	%f4, %r9;
	// begin inline asm
	cvt.f32.bf16 %r10, %rs5;
	// end inline asm
	mov.b32 	%f5, %r10;
	// begin inline asm
	cvt.f32.bf16 %r11, %rs6;
	// end inline asm
	mov.b32 	%f6, %r11;
	// begin inline asm
	cvt.f32.bf16 %r12, %rs7;
	// end inline asm
	mov.b32 	%f7, %r12;
	// begin inline asm
	cvt.f32.bf16 %r13, %rs8;
	// end inline asm
	mov.b32 	%f8, %r13;
	mul.rn.f32 	%f9, %f1, %f1;
	mul.rn.f32 	%f10, %f2, %f2;
	mul.rn.f32 	%f11, %f3, %f3;
	mul.rn.f32 	%f12, %f4, %f4;
	mul.rn.f32 	%f13, %f5, %f5;
	mul.rn.f32 	%f14, %f6, %f6;
	mul.rn.f32 	%f15, %f7, %f7;
	mul.rn.f32 	%f16, %f8, %f8;
	add.rn.f32 	%f17, %f9, %f10;
	add.rn.f32 	%f18, %f17, %f11;
	add.rn.f32 	%f19, %f18, %f12;
	add.rn.f32 	%f20, %f19, %f13;
	add.rn.f32 	%f21, %f20, %f14;
	add.rn.f32 	%f22, %f21, %f15;
	add.rn.f32 	%f23, %f22, %f16;
	mov.b32 	%r50, %f23;
	shfl.sync.bfly.b32	%r51, %r50, 8, 31, -1;
	mov.b32 	%f24, %r51;
	add.rn.f32 	%f25, %f23, %f24;
	mov.b32 	%r52, %f25;
	shfl.sync.bfly.b32	%r53, %r52, 4, 31, -1;
	mov.b32 	%f26, %r53;
	add.rn.f32 	%f27, %f25, %f26;
	mov.b32 	%r54, %f27;
	shfl.sync.bfly.b32	%r55, %r54, 2, 31, -1;
	mov.b32 	%f28, %r55;
	add.rn.f32 	%f29, %f27, %f28;
	mov.b32 	%r56, %f29;
	shfl.sync.bfly.b32	%r57, %r56, 1, 31, -1;
	mov.b32 	%f30, %r57;
	add.rn.f32 	%f31, %f29, %f30;
	mul.rn.f32 	%f32, %f31, 0f3C000000;
	// begin inline asm
	mov.u32 %r14, 0x0;
	ld.global.b32 { %r14 }, [ %rd2 + 0 ];
	// end inline asm
	mov.b32 	%f33, %r14;
	add.rn.f32 	%f34, %f32, %f33;
	rsqrt.approx.f32 	%f35, %f34;
	mul.rn.f32 	%f36, %f1, %f35;
	mul.rn.f32 	%f37, %f2, %f35;
	mul.rn.f32 	%f38, %f3, %f35;
	mul.rn.f32 	%f39, %f4, %f35;
	mul.rn.f32 	%f40, %f5, %f35;
	mul.rn.f32 	%f41, %f6, %f35;
	mul.rn.f32 	%f42, %f7, %f35;
	mul.rn.f32 	%f43, %f8, %f35;
	shl.b32 	%r58, %r44, 1;
	and.b32  	%r59, %r58, 62;
	selp.b32 	%r60, 64, 0, %p4;
	or.b32  	%r61, %r59, %r60;
	shl.b32 	%r62, %r44, 2;
	and.b32  	%r63, %r62, 124;
	cvt.u64.u32 	%rd21, %r63;
	mul.wide.u32 	%rd22, %r61, 2;
	add.s64 	%rd3, %rd11, %rd22;
	// begin inline asm
	mov.u32 %r15, 0x0;
	ld.global.b32 { %r15 }, [ %rd3 + 0 ];
	// end inline asm
	{ .reg .b16 tmp; mov.b32 {tmp, %rs10}, %r15; }
	mov.u64 	%rd23, global_smem;
	add.s64 	%rd24, %rd23, %rd22;
	cvt.u16.u32 	%rs9, %r15;
	cvt.u32.u64 	%r16, %rd24;
	mov.pred 	%p1, -1;
	// begin inline asm
	@%p1 st.shared.v2.b16 [ %r16 + 0 ], { %rs9, %rs10 };
	// end inline asm
	bar.sync 	0;
	mul.wide.u32 	%rd25, %r46, 2;
	add.s64 	%rd26, %rd23, %rd25;
	ld.shared.v4.b32 	{%r64, %r65, %r66, %r67}, [%rd26];
	mov.b32 	{%rs17, %rs18}, %r67;
	mov.b32 	{%rs15, %rs16}, %r66;
	mov.b32 	{%rs13, %rs14}, %r65;
	mov.b32 	{%rs11, %rs12}, %r64;
	// begin inline asm
	cvt.f32.bf16 %r17, %rs11;
	// end inline asm
	mov.b32 	%f44, %r17;
	// begin inline asm
	cvt.f32.bf16 %r18, %rs12;
	// end inline asm
	mov.b32 	%f45, %r18;
	// begin inline asm
	cvt.f32.bf16 %r19, %rs13;
	// end inline asm
	mov.b32 	%f46, %r19;
	// begin inline asm
	cvt.f32.bf16 %r20, %rs14;
	// end inline asm
	mov.b32 	%f47, %r20;
	// begin inline asm
	cvt.f32.bf16 %r21, %rs15;
	// end inline asm
	mov.b32 	%f48, %r21;
	// begin inline asm
	cvt.f32.bf16 %r22, %rs16;
	// end inline asm
	mov.b32 	%f49, %r22;
	// begin inline asm
	cvt.f32.bf16 %r23, %rs17;
	// end inline asm
	mov.b32 	%f50, %r23;
	// begin inline asm
	cvt.f32.bf16 %r24, %rs18;
	// end inline asm
	mov.b32 	%f51, %r24;
	mul.rn.f32 	%f52, %f36, %f44;
	mul.rn.f32 	%f53, %f37, %f45;
	mul.rn.f32 	%f54, %f38, %f46;
	mul.rn.f32 	%f55, %f39, %f47;
	mul.rn.f32 	%f56, %f40, %f48;
	mul.rn.f32 	%f57, %f41, %f49;
	mul.rn.f32 	%f58, %f42, %f50;
	mul.rn.f32 	%f59, %f43, %f51;
	add.s64 	%rd27, %rd8, %rd15;
	selp.b64 	%rd28, 128, 0, %p4;
	selp.b64 	%rd29, 384, 256, %p4;
	or.b64  	%rd30, %rd28, %rd21;
	or.b64  	%rd31, %rd29, %rd21;
	shl.b64 	%rd32, %rd30, 2;
	add.s64 	%rd4, %rd27, %rd32;
	shl.b64 	%rd33, %rd31, 2;
	add.s64 	%rd5, %rd27, %rd33;
	bar.sync 	0;
	selp.b32 	%r68, 128, 0, %p4;
	or.b32  	%r69, %r63, %r68;
	shr.u32 	%r70, %r49, 4;
	and.b32  	%r71, %r70, 24;
	add.s32 	%r72, %r71, %r49;
	shl.b32 	%r73, %r72, 2;
	cvt.u64.u32 	%rd34, %r73;
	add.s64 	%rd35, %rd23, %rd34;
	mov.b32 	%r26, %f52;
	mov.b32 	%r27, %f53;
	mov.b32 	%r28, %f54;
	mov.b32 	%r29, %f55;
	mov.b32 	%r31, %f56;
	mov.b32 	%r32, %f57;
	mov.b32 	%r33, %f58;
	mov.b32 	%r34, %f59;
	cvt.u32.u64 	%r25, %rd35;
	// begin inline asm
	@%p1 st.shared.v4.b32 [ %r25 + 0 ], { %r26, %r27, %r28, %r29 };
	// end inline asm
	add.s32 	%r30, %r25, 16;
	// begin inline asm
	@%p1 st.shared.v4.b32 [ %r30 + 0 ], { %r31, %r32, %r33, %r34 };
	// end inline asm
	bar.sync 	0;
	shr.u32 	%r74, %r68, 4;
	add.s32 	%r75, %r74, %r69;
	mul.wide.u32 	%rd36, %r75, 4;
	add.s64 	%rd37, %rd23, %rd36;
	or.b32  	%r76, %r68, 256;
	shr.u32 	%r77, %r76, 4;
	cvt.u64.u32 	%rd38, %r77;
	cvt.u64.u32 	%rd39, %r69;
	add.s64 	%rd40, %rd38, %rd39;
	shl.b64 	%rd41, %rd40, 2;
	add.s64 	%rd42, %rd23, %rd41;
	ld.shared.v4.u32 	{%r39, %r40, %r41, %r42}, [%rd42+1024];
	ld.shared.v4.u32 	{%r35, %r36, %r37, %r38}, [%rd37];
	// begin inline asm
	st.global.v4.b32 [ %rd4 + 0 ], { %r35, %r36, %r37, %r38 };
	// end inline asm
	// begin inline asm
	st.global.v4.b32 [ %rd5 + 0 ], { %r39, %r40, %r41, %r42 };
	// end inline asm
	ret;

}
	// .globl	input_concatenate_fusion
.visible .entry input_concatenate_fusion(
	.param .u64 .ptr .align 128 input_concatenate_fusion_param_0,
	.param .u64 .ptr .align 16 input_concatenate_fusion_param_1,
	.param .u64 .ptr .align 16 input_concatenate_fusion_param_2,
	.param .u64 .ptr .align 128 input_concatenate_fusion_param_3
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<17>;
	.reg .f32 	%f<11>;
	.reg .b64 	%rd<19>;

	ld.param.u64 	%rd1, [input_concatenate_fusion_param_0];
	ld.param.u64 	%rd2, [input_concatenate_fusion_param_3];
	cvta.to.global.u64 	%rd3, %rd2;
	ld.param.u64 	%rd4, [input_concatenate_fusion_param_1];
	ld.param.u64 	%rd5, [input_concatenate_fusion_param_2];
	cvta.to.global.u64 	%rd6, %rd5;
	cvta.to.global.u64 	%rd7, %rd4;
	cvta.to.global.u64 	%rd8, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	and.b32  	%r3, %r1, 63;
	shl.b32 	%r4, %r1, 1;
	and.b32  	%r5, %r4, 128;
	shl.b32 	%r6, %r2, 8;
	or.b32  	%r7, %r5, %r6;
	or.b32  	%r8, %r7, %r3;
	mul.wide.u32 	%rd9, %r8, 4;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.nc.f32 	%f1, [%rd10];
	and.b32  	%r9, %r2, -4;
	cvt.u64.u32 	%rd11, %r9;
	add.s64 	%rd12, %rd6, %rd11;
	ld.global.nc.u32 	%r10, [%rd12];
	min.s32 	%r11, %r10, 40959;
	max.s32 	%r12, %r11, 0;
	shl.b32 	%r13, %r12, 7;
	or.b32  	%r14, %r13, %r3;
	mul.wide.u32 	%rd13, %r14, 2;
	add.s64 	%rd14, %rd7, %rd13;
	ld.global.nc.u16 	%rs1, [%rd14];
	cvt.f32.bf16 	%f2, %rs1;
	mul.rn.f32 	%f3, %f1, %f2;
	ld.global.nc.f32 	%f4, [%rd10+256];
	or.b32  	%r15, %r1, %r13;
	or.b32  	%r16, %r15, 64;
	mul.wide.u32 	%rd15, %r16, 2;
	add.s64 	%rd16, %rd7, %rd15;
	ld.global.nc.u16 	%rs2, [%rd16];
	cvt.f32.bf16 	%f5, %rs2;
	mul.rn.f32 	%f6, %f4, %f5;
	sub.rn.f32 	%f7, %f3, %f6;
	cvt.rn.bf16.f32 	%rs3, %f7;
	mul.wide.u32 	%rd17, %r8, 2;
	add.s64 	%rd18, %rd3, %rd17;
	st.global.b16 	[%rd18], %rs3;
	mul.rn.f32 	%f8, %f4, %f2;
	mul.rn.f32 	%f9, %f1, %f5;
	add.rn.f32 	%f10, %f8, %f9;
	cvt.rn.bf16.f32 	%rs4, %f10;
	st.global.b16 	[%rd18+128], %rs4;
	ret;

}
	// .globl	loop_slice_fusion
.visible .entry loop_slice_fusion(
	.param .u64 .ptr .align 16 loop_slice_fusion_param_0,
	.param .u64 .ptr .align 128 loop_slice_fusion_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	ld.param.u64 	%rd1, [loop_slice_fusion_param_0];
	ld.param.u64 	%rd2, [loop_slice_fusion_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r1, 2;
	shl.b32 	%r4, %r2, 9;
	or.b32  	%r5, %r3, %r4;
	mul.wide.u32 	%rd5, %r5, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd6+138706944];
	add.s64 	%rd7, %rd3, %rd5;
	st.global.v4.b16 	[%rd7], {%rs1, %rs2, %rs3, %rs4};
	ret;

}
	// .globl	wrapped_slice_1
.visible .entry wrapped_slice_1(
	.param .u64 .ptr .align 16 wrapped_slice_1_param_0,
	.param .u64 .ptr .align 128 wrapped_slice_1_param_1
)
.reqntid 128, 1, 1
{
	.reg .b16 	%rs<5>;
	.reg .b32 	%r<6>;
	.reg .b64 	%rd<8>;

	ld.param.u64 	%rd1, [wrapped_slice_1_param_0];
	ld.param.u64 	%rd2, [wrapped_slice_1_param_1];
	cvta.to.global.u64 	%rd3, %rd2;
	cvta.to.global.u64 	%rd4, %rd1;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r2, %ctaid.x;
	shl.b32 	%r3, %r1, 2;
	shl.b32 	%r4, %r2, 9;
	or.b32  	%r5, %r3, %r4;
	mul.wide.u32 	%rd5, %r5, 2;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.nc.v4.u16 	{%rs1, %rs2, %rs3, %rs4}, [%rd6];
	add.s64 	%rd7, %rd3, %rd5;
	st.global.v4.b16 	[%rd7], {%rs1, %rs2, %rs3, %rs4};
	ret;

}
